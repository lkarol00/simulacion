{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de muestras:  12330\n",
      "Número de características:  18\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "filename = 'online_shoppers_intention.csv'\n",
    "data = pandas.read_csv(filename, header=0)\n",
    "\n",
    "print(\"Número de muestras: \", data.shape[0])\n",
    "print(\"Número de características: \", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>154.216667</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "5               0                      0.0              0   \n",
       "6               0                      0.0              0   \n",
       "7               1                      0.0              0   \n",
       "8               0                      0.0              0   \n",
       "9               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "5                     0.0              19               154.216667   \n",
       "6                     0.0               1                 0.000000   \n",
       "7                     0.0               0                 0.000000   \n",
       "8                     0.0               2                37.000000   \n",
       "9                     0.0               3               738.000000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0     0.200000   0.200000         0.0         0.0   Feb                 1   \n",
       "1     0.000000   0.100000         0.0         0.0   Feb                 2   \n",
       "2     0.200000   0.200000         0.0         0.0   Feb                 4   \n",
       "3     0.050000   0.140000         0.0         0.0   Feb                 3   \n",
       "4     0.020000   0.050000         0.0         0.0   Feb                 3   \n",
       "5     0.015789   0.024561         0.0         0.0   Feb                 2   \n",
       "6     0.200000   0.200000         0.0         0.4   Feb                 2   \n",
       "7     0.200000   0.200000         0.0         0.0   Feb                 1   \n",
       "8     0.000000   0.100000         0.0         0.8   Feb                 2   \n",
       "9     0.000000   0.022222         0.0         0.4   Feb                 2   \n",
       "\n",
       "   Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0        1       1            1  Returning_Visitor    False    False  \n",
       "1        2       1            2  Returning_Visitor    False    False  \n",
       "2        1       9            3  Returning_Visitor    False    False  \n",
       "3        2       2            4  Returning_Visitor    False    False  \n",
       "4        3       1            4  Returning_Visitor     True    False  \n",
       "5        2       1            3  Returning_Visitor    False    False  \n",
       "6        4       3            3  Returning_Visitor    False    False  \n",
       "7        2       1            5  Returning_Visitor     True    False  \n",
       "8        2       2            3  Returning_Visitor    False    False  \n",
       "9        4       1            2  Returning_Visitor    False    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative               int64\n",
       "Administrative_Duration    float64\n",
       "Informational                int64\n",
       "Informational_Duration     float64\n",
       "ProductRelated               int64\n",
       "ProductRelated_Duration    float64\n",
       "BounceRates                float64\n",
       "ExitRates                  float64\n",
       "PageValues                 float64\n",
       "SpecialDay                 float64\n",
       "Month                       object\n",
       "OperatingSystems             int64\n",
       "Browser                      int64\n",
       "Region                       int64\n",
       "TrafficType                  int64\n",
       "VisitorType                 object\n",
       "Weekend                       bool\n",
       "Revenue                       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    9462\n",
       "True     2868\n",
       "Name: Weekend, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Weekend'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    10422\n",
       "True      1908\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "May     3364\n",
       "Nov     2998\n",
       "Mar     1907\n",
       "Dec     1727\n",
       "Oct      549\n",
       "Sep      448\n",
       "Aug      433\n",
       "Jul      432\n",
       "June     288\n",
       "Feb      184\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Returning_Visitor    10551\n",
       "New_Visitor           1694\n",
       "Other                   85\n",
       "Name: VisitorType, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['VisitorType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative             0\n",
       "Administrative_Duration    0\n",
       "Informational              0\n",
       "Informational_Duration     0\n",
       "ProductRelated             0\n",
       "ProductRelated_Duration    0\n",
       "BounceRates                0\n",
       "ExitRates                  0\n",
       "PageValues                 0\n",
       "SpecialDay                 0\n",
       "Month                      0\n",
       "OperatingSystems           0\n",
       "Browser                    0\n",
       "Region                     0\n",
       "TrafficType                0\n",
       "VisitorType                0\n",
       "Weekend                    0\n",
       "Revenue                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Weekend\"] = data[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9462\n",
       "1    2868\n",
       "Name: Weekend, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Weekend'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Revenue'] = data[\"Revenue\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10422\n",
       "1     1908\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = data['Month'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = np.where(data['Month']==\"Feb\", 0, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Mar\", 1, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"May\", 2, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"June\", 3, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Jul\", 4, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Aug\", 5, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Sep\", 6, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Oct\", 7, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Nov\", 8, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Dec\", 9, data['Month'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3364\n",
       "8    2998\n",
       "1    1907\n",
       "9    1727\n",
       "7     549\n",
       "6     448\n",
       "5     433\n",
       "4     432\n",
       "3     288\n",
       "0     184\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VisitorType'] = np.where(data['VisitorType']==\"Returning_Visitor\", 0, data['VisitorType'])\n",
    "data['VisitorType'] = np.where(data['VisitorType']==\"New_Visitor\", 1, data['VisitorType'])\n",
    "data['VisitorType'] = np.where(data['VisitorType']==\"Other\", 2, data['VisitorType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10551\n",
       "1     1694\n",
       "2       85\n",
       "Name: VisitorType, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['VisitorType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>154.216667</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "5               0                      0.0              0   \n",
       "6               0                      0.0              0   \n",
       "7               1                      0.0              0   \n",
       "8               0                      0.0              0   \n",
       "9               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "5                     0.0              19               154.216667   \n",
       "6                     0.0               1                 0.000000   \n",
       "7                     0.0               0                 0.000000   \n",
       "8                     0.0               2                37.000000   \n",
       "9                     0.0               3               738.000000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0     0.200000   0.200000         0.0         0.0     0                 1   \n",
       "1     0.000000   0.100000         0.0         0.0     0                 2   \n",
       "2     0.200000   0.200000         0.0         0.0     0                 4   \n",
       "3     0.050000   0.140000         0.0         0.0     0                 3   \n",
       "4     0.020000   0.050000         0.0         0.0     0                 3   \n",
       "5     0.015789   0.024561         0.0         0.0     0                 2   \n",
       "6     0.200000   0.200000         0.0         0.4     0                 2   \n",
       "7     0.200000   0.200000         0.0         0.0     0                 1   \n",
       "8     0.000000   0.100000         0.0         0.8     0                 2   \n",
       "9     0.000000   0.022222         0.0         0.4     0                 2   \n",
       "\n",
       "   Browser  Region  TrafficType VisitorType  Weekend  Revenue  \n",
       "0        1       1            1           0        0        0  \n",
       "1        2       1            2           0        0        0  \n",
       "2        1       9            3           0        0        0  \n",
       "3        2       2            4           0        0        0  \n",
       "4        3       1            4           0        1        0  \n",
       "5        2       1            3           0        0        0  \n",
       "6        4       3            3           0        0        0  \n",
       "7        2       1            5           0        1        0  \n",
       "8        2       2            3           0        0        0  \n",
       "9        4       1            2           0        0        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = data['Month'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VisitorType'] = data['VisitorType'].astype('str').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative               int64\n",
       "Administrative_Duration    float64\n",
       "Informational                int64\n",
       "Informational_Duration     float64\n",
       "ProductRelated               int64\n",
       "ProductRelated_Duration    float64\n",
       "BounceRates                float64\n",
       "ExitRates                  float64\n",
       "PageValues                 float64\n",
       "SpecialDay                 float64\n",
       "Month                        int32\n",
       "OperatingSystems             int64\n",
       "Browser                      int64\n",
       "Region                       int64\n",
       "TrafficType                  int64\n",
       "VisitorType                  int32\n",
       "Weekend                      int32\n",
       "Revenue                      int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12330, 17) (12330,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('Revenue', axis=1)\n",
    "Y = data[\"Revenue\"]\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(0,np.sum(Y==0)/Y.shape[0])\n",
    "plt.bar(1,np.sum(Y==1)/Y.shape[0])\n",
    "plt.title('Distribución de clases original')\n",
    "for i in range(2):\n",
    "    plt.text(i, np.sum(Y==i)/Y.shape[0], str(round(np.sum(Y==i)/Y.shape[0],3)), color='black', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(data):\n",
    "    df = pandas.DataFrame()\n",
    "    df = df.assign(mean_fit_time = data[\"mean_fit_time\"])\n",
    "\n",
    "    df = df.assign(params = data[\"params\"])\n",
    "\n",
    "    df = df.assign(mean_test_AUC = data[\"mean_test_AUC\"])\n",
    "    df = df.assign(mean_test_F_score = data[\"mean_test_F-score\"])\n",
    "    df = df.assign(mean_test_Sensitivity = data[\"mean_test_Sensitivity\"])\n",
    "    df = df.assign(rank_test_AUC = data[\"rank_test_AUC\"])\n",
    "    df = df.assign(rank_test_F_score = data[\"rank_test_F-score\"])\n",
    "    df = df.assign(rank_test_Sensitivity = data[\"rank_test_Sensitivity\"])\n",
    "\n",
    "    df = df.assign(mean_train_AUC = data[\"mean_train_AUC\"])\n",
    "    df = df.assign(mean_train_F_score = data[\"mean_train_F-score\"])\n",
    "    df = df.assign(mean_train_Sensitivity = data[\"mean_train_Sensitivity\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  48 | elapsed: 16.9min remaining:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 17.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('svc',\n",
       "                                        SVC(C=1.0, cache_size=200,\n",
       "                                            class_weight=None, coef0=0.0,\n",
       "                                            decision_function_shape='ovr',\n",
       "                                            degree=3, gamma='auto_deprecated',\n",
       "                                            kernel=...\n",
       "                                            probability=True, random_state=None,\n",
       "                                            shrinking=True, tol=0.001,\n",
       "                                            verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'svc__C': (0.001, 0.01, 1, 10),\n",
       "                         'svc__gamma': ['scale'],\n",
       "                         'svc__kernel': ('linear', 'poly', 'sigmoid')},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'svc__kernel':('linear', 'poly', 'sigmoid'),\n",
    "              'svc__gamma':['scale'],\n",
    "              'svc__C':(0.001, 0.01, 1, 10)}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), SVC(probability = True))\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_svc__C</th>\n",
       "      <th>param_svc__gamma</th>\n",
       "      <th>param_svc__kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.011196</td>\n",
       "      <td>1.434184</td>\n",
       "      <td>1.213589</td>\n",
       "      <td>0.198066</td>\n",
       "      <td>0.001</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.970697</td>\n",
       "      <td>0.839589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253669</td>\n",
       "      <td>0.268344</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>8</td>\n",
       "      <td>0.208945</td>\n",
       "      <td>0.274633</td>\n",
       "      <td>0.334032</td>\n",
       "      <td>0.315164</td>\n",
       "      <td>0.283194</td>\n",
       "      <td>0.047940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.010588</td>\n",
       "      <td>0.406227</td>\n",
       "      <td>1.262536</td>\n",
       "      <td>0.079116</td>\n",
       "      <td>0.001</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.903156</td>\n",
       "      <td>0.851529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029350</td>\n",
       "      <td>0.022011</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>11</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.020615</td>\n",
       "      <td>0.002495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.948824</td>\n",
       "      <td>1.096042</td>\n",
       "      <td>2.343567</td>\n",
       "      <td>0.225114</td>\n",
       "      <td>0.001</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.952383</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.745382</td>\n",
       "      <td>2.080913</td>\n",
       "      <td>1.057702</td>\n",
       "      <td>0.241812</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.966250</td>\n",
       "      <td>0.835507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327044</td>\n",
       "      <td>0.364260</td>\n",
       "      <td>0.027589</td>\n",
       "      <td>6</td>\n",
       "      <td>0.308176</td>\n",
       "      <td>0.375961</td>\n",
       "      <td>0.433263</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.387841</td>\n",
       "      <td>0.051667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.130707</td>\n",
       "      <td>1.595418</td>\n",
       "      <td>1.520653</td>\n",
       "      <td>0.163336</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.885291</td>\n",
       "      <td>0.848628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115304</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>10</td>\n",
       "      <td>0.061495</td>\n",
       "      <td>0.077568</td>\n",
       "      <td>0.097834</td>\n",
       "      <td>0.088749</td>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.013555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.219475</td>\n",
       "      <td>0.815531</td>\n",
       "      <td>2.683069</td>\n",
       "      <td>0.207297</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.931649</td>\n",
       "      <td>0.847148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218029</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>9</td>\n",
       "      <td>0.160028</td>\n",
       "      <td>0.214535</td>\n",
       "      <td>0.255066</td>\n",
       "      <td>0.244584</td>\n",
       "      <td>0.218553</td>\n",
       "      <td>0.036919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.269707</td>\n",
       "      <td>22.889794</td>\n",
       "      <td>1.068824</td>\n",
       "      <td>0.191393</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.965817</td>\n",
       "      <td>0.835809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>4</td>\n",
       "      <td>0.322851</td>\n",
       "      <td>0.394829</td>\n",
       "      <td>0.464710</td>\n",
       "      <td>0.452131</td>\n",
       "      <td>0.408630</td>\n",
       "      <td>0.056093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51.326718</td>\n",
       "      <td>2.224108</td>\n",
       "      <td>1.194457</td>\n",
       "      <td>0.209341</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.852108</td>\n",
       "      <td>0.843723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425577</td>\n",
       "      <td>0.346431</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>7</td>\n",
       "      <td>0.330538</td>\n",
       "      <td>0.410203</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.415793</td>\n",
       "      <td>0.055879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.178688</td>\n",
       "      <td>2.981318</td>\n",
       "      <td>1.260144</td>\n",
       "      <td>0.039451</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.625320</td>\n",
       "      <td>0.683358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448637</td>\n",
       "      <td>0.434485</td>\n",
       "      <td>0.021038</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>0.450734</td>\n",
       "      <td>0.426974</td>\n",
       "      <td>0.024468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>389.273173</td>\n",
       "      <td>124.027194</td>\n",
       "      <td>1.003884</td>\n",
       "      <td>0.213332</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.965871</td>\n",
       "      <td>0.836233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>4</td>\n",
       "      <td>0.322851</td>\n",
       "      <td>0.394829</td>\n",
       "      <td>0.465409</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.408980</td>\n",
       "      <td>0.056404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.258530</td>\n",
       "      <td>11.832461</td>\n",
       "      <td>1.031252</td>\n",
       "      <td>0.064425</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.796524</td>\n",
       "      <td>0.818780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486373</td>\n",
       "      <td>0.410371</td>\n",
       "      <td>0.048453</td>\n",
       "      <td>3</td>\n",
       "      <td>0.445143</td>\n",
       "      <td>0.535290</td>\n",
       "      <td>0.563242</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.532495</td>\n",
       "      <td>0.053570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.129594</td>\n",
       "      <td>2.312611</td>\n",
       "      <td>1.246092</td>\n",
       "      <td>0.111782</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.825978</td>\n",
       "      <td>0.679914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.431863</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>2</td>\n",
       "      <td>0.376660</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.459818</td>\n",
       "      <td>0.452131</td>\n",
       "      <td>0.428022</td>\n",
       "      <td>0.032600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
       "0       40.011196      1.434184         1.213589        0.198066        0.001   \n",
       "1       46.010588      0.406227         1.262536        0.079116        0.001   \n",
       "2       48.948824      1.096042         2.343567        0.225114        0.001   \n",
       "3       39.745382      2.080913         1.057702        0.241812         0.01   \n",
       "4       50.130707      1.595418         1.520653        0.163336         0.01   \n",
       "5       51.219475      0.815531         2.683069        0.207297         0.01   \n",
       "6       96.269707     22.889794         1.068824        0.191393            1   \n",
       "7       51.326718      2.224108         1.194457        0.209341            1   \n",
       "8       24.178688      2.981318         1.260144        0.039451            1   \n",
       "9      389.273173    124.027194         1.003884        0.213332           10   \n",
       "10      86.258530     11.832461         1.031252        0.064425           10   \n",
       "11      16.129594      2.312611         1.246092        0.111782           10   \n",
       "\n",
       "   param_svc__gamma param_svc__kernel  \\\n",
       "0             scale            linear   \n",
       "1             scale              poly   \n",
       "2             scale           sigmoid   \n",
       "3             scale            linear   \n",
       "4             scale              poly   \n",
       "5             scale           sigmoid   \n",
       "6             scale            linear   \n",
       "7             scale              poly   \n",
       "8             scale           sigmoid   \n",
       "9             scale            linear   \n",
       "10            scale              poly   \n",
       "11            scale           sigmoid   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...         0.970697   \n",
       "1   {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...         0.903156   \n",
       "2   {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...         0.952383   \n",
       "3   {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...         0.966250   \n",
       "4   {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...         0.885291   \n",
       "5   {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...         0.931649   \n",
       "6   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...         0.965817   \n",
       "7   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...         0.852108   \n",
       "8   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...         0.625320   \n",
       "9   {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...         0.965871   \n",
       "10  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...         0.796524   \n",
       "11  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...         0.825978   \n",
       "\n",
       "    split1_test_AUC  ...  split3_test_Sensitivity  mean_test_Sensitivity  \\\n",
       "0          0.839589  ...                 0.253669               0.268344   \n",
       "1          0.851529  ...                 0.029350               0.022011   \n",
       "2          0.853107  ...                 0.000000               0.000000   \n",
       "3          0.835507  ...                 0.327044               0.364260   \n",
       "4          0.848628  ...                 0.115304               0.074420   \n",
       "5          0.847148  ...                 0.218029               0.219601   \n",
       "6          0.835809  ...                 0.339623               0.391514   \n",
       "7          0.843723  ...                 0.425577               0.346431   \n",
       "8          0.683358  ...                 0.448637               0.434485   \n",
       "9          0.836233  ...                 0.339623               0.391514   \n",
       "10         0.818780  ...                 0.486373               0.410371   \n",
       "11         0.679914  ...                 0.452830               0.431863   \n",
       "\n",
       "    std_test_Sensitivity  rank_test_Sensitivity  split0_train_Sensitivity  \\\n",
       "0               0.009720                      8                  0.208945   \n",
       "1               0.007338                     11                  0.017470   \n",
       "2               0.000000                     12                  0.000000   \n",
       "3               0.027589                      6                  0.308176   \n",
       "4               0.025525                     10                  0.061495   \n",
       "5               0.007756                      9                  0.160028   \n",
       "6               0.033126                      4                  0.322851   \n",
       "7               0.047721                      7                  0.330538   \n",
       "8               0.021038                      1                  0.390636   \n",
       "9               0.033126                      4                  0.322851   \n",
       "10              0.048453                      3                  0.445143   \n",
       "11              0.032917                      2                  0.376660   \n",
       "\n",
       "    split1_train_Sensitivity  split2_train_Sensitivity  \\\n",
       "0                   0.274633                  0.334032   \n",
       "1                   0.020266                  0.024458   \n",
       "2                   0.000000                  0.000000   \n",
       "3                   0.375961                  0.433263   \n",
       "4                   0.077568                  0.097834   \n",
       "5                   0.214535                  0.255066   \n",
       "6                   0.394829                  0.464710   \n",
       "7                   0.410203                  0.438155   \n",
       "8                   0.418588                  0.447939   \n",
       "9                   0.394829                  0.465409   \n",
       "10                  0.535290                  0.563242   \n",
       "11                  0.423480                  0.459818   \n",
       "\n",
       "    split3_train_Sensitivity  mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                   0.315164                0.283194               0.047940  \n",
       "1                   0.020266                0.020615               0.002495  \n",
       "2                   0.000000                0.000000               0.000000  \n",
       "3                   0.433962                0.387841               0.051667  \n",
       "4                   0.088749                0.081412               0.013555  \n",
       "5                   0.244584                0.218553               0.036919  \n",
       "6                   0.452131                0.408630               0.056093  \n",
       "7                   0.484277                0.415793               0.055879  \n",
       "8                   0.450734                0.426974               0.024468  \n",
       "9                   0.452830                0.408980               0.056404  \n",
       "10                  0.586303                0.532495               0.053570  \n",
       "11                  0.452131                0.428022               0.032600  \n",
       "\n",
       "[12 rows x 47 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_svc = gs.cv_results_\n",
    "data = pandas.DataFrame(results_svc)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.011196</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.849134</td>\n",
       "      <td>0.399287</td>\n",
       "      <td>0.268344</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.873948</td>\n",
       "      <td>0.414885</td>\n",
       "      <td>0.283194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.010588</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.839221</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>0.022011</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.040283</td>\n",
       "      <td>0.020615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.948824</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.842535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.853601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.745382</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.844399</td>\n",
       "      <td>0.487738</td>\n",
       "      <td>0.364260</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.868839</td>\n",
       "      <td>0.510337</td>\n",
       "      <td>0.387841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.130707</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>0.135121</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.884842</td>\n",
       "      <td>0.149338</td>\n",
       "      <td>0.081412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.219475</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.831313</td>\n",
       "      <td>0.342604</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.839334</td>\n",
       "      <td>0.339956</td>\n",
       "      <td>0.218553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.269707</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.844004</td>\n",
       "      <td>0.510040</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.867881</td>\n",
       "      <td>0.525761</td>\n",
       "      <td>0.408630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51.326718</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.812497</td>\n",
       "      <td>0.459162</td>\n",
       "      <td>0.346431</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.906362</td>\n",
       "      <td>0.559482</td>\n",
       "      <td>0.415793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.178688</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.644190</td>\n",
       "      <td>0.448139</td>\n",
       "      <td>0.434485</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.642793</td>\n",
       "      <td>0.453644</td>\n",
       "      <td>0.426974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>389.273173</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.843949</td>\n",
       "      <td>0.510040</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.867898</td>\n",
       "      <td>0.526133</td>\n",
       "      <td>0.408980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.258530</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.782387</td>\n",
       "      <td>0.496160</td>\n",
       "      <td>0.410371</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.917496</td>\n",
       "      <td>0.658020</td>\n",
       "      <td>0.532495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.129594</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.688934</td>\n",
       "      <td>0.426242</td>\n",
       "      <td>0.431863</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653024</td>\n",
       "      <td>0.430286</td>\n",
       "      <td>0.428022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0       40.011196  {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...   \n",
       "1       46.010588  {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...   \n",
       "2       48.948824  {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...   \n",
       "3       39.745382  {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...   \n",
       "4       50.130707  {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...   \n",
       "5       51.219475  {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...   \n",
       "6       96.269707  {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...   \n",
       "7       51.326718  {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...   \n",
       "8       24.178688  {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...   \n",
       "9      389.273173  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "10      86.258530  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "11      16.129594  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.849134           0.399287               0.268344              1   \n",
       "1        0.839221           0.042729               0.022011              6   \n",
       "2        0.842535           0.000000               0.000000              5   \n",
       "3        0.844399           0.487738               0.364260              2   \n",
       "4        0.832056           0.135121               0.074420              7   \n",
       "5        0.831313           0.342604               0.219601              8   \n",
       "6        0.844004           0.510040               0.391514              3   \n",
       "7        0.812497           0.459162               0.346431              9   \n",
       "8        0.644190           0.448139               0.434485             12   \n",
       "9        0.843949           0.510040               0.391514              4   \n",
       "10       0.782387           0.496160               0.410371             10   \n",
       "11       0.688934           0.426242               0.431863             11   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   8                      8        0.873948   \n",
       "1                  11                     11        0.883681   \n",
       "2                  12                     12        0.853601   \n",
       "3                   4                      6        0.868839   \n",
       "4                  10                     10        0.884842   \n",
       "5                   9                      9        0.839334   \n",
       "6                   1                      4        0.867881   \n",
       "7                   5                      7        0.906362   \n",
       "8                   6                      1        0.642793   \n",
       "9                   1                      4        0.867898   \n",
       "10                  3                      3        0.917496   \n",
       "11                  7                      2        0.653024   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.414885                0.283194  \n",
       "1             0.040283                0.020615  \n",
       "2             0.000000                0.000000  \n",
       "3             0.510337                0.387841  \n",
       "4             0.149338                0.081412  \n",
       "5             0.339956                0.218553  \n",
       "6             0.525761                0.408630  \n",
       "7             0.559482                0.415793  \n",
       "8             0.453644                0.426974  \n",
       "9             0.526133                0.408980  \n",
       "10            0.658020                0.532495  \n",
       "11            0.430286                0.428022  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_svc = make_table(data)\n",
    "table_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 1, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 32 candidates, totalling 128 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 128 out of 128 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('gradientboostingclassifier',\n",
       "                                        GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                                   init=None,\n",
       "                                                                   learning_rate=0.1,\n",
       "                                                                   loss='deviance',\n",
       "                                                                   max_...\n",
       "             param_grid={'gradientboostingclassifier__learning_rate': [0.3, 0.2,\n",
       "                                                                       0.1,\n",
       "                                                                       0.01],\n",
       "                         'gradientboostingclassifier__loss': ('deviance',\n",
       "                                                              'exponential'),\n",
       "                         'gradientboostingclassifier__n_estimators': [50, 100,\n",
       "                                                                      200,\n",
       "                                                                      300]},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'gradientboostingclassifier__loss':('deviance', 'exponential'), \n",
    "              'gradientboostingclassifier__learning_rate':([0.3, 0.2, 0.1, 0.01]), \n",
    "              'gradientboostingclassifier__n_estimators':([50, 100, 200, 300]),\n",
    "             }\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), GradientBoostingClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gradientboostingclassifier__learning_rate</th>\n",
       "      <th>param_gradientboostingclassifier__loss</th>\n",
       "      <th>param_gradientboostingclassifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018934</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.051197</td>\n",
       "      <td>7.218690e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980061</td>\n",
       "      <td>0.893875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626834</td>\n",
       "      <td>0.522001</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>16</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.711391</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.735150</td>\n",
       "      <td>0.704577</td>\n",
       "      <td>0.044747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.908934</td>\n",
       "      <td>0.034056</td>\n",
       "      <td>0.058595</td>\n",
       "      <td>6.763725e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978085</td>\n",
       "      <td>0.888735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.517809</td>\n",
       "      <td>0.071062</td>\n",
       "      <td>22</td>\n",
       "      <td>0.697414</td>\n",
       "      <td>0.780573</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.763627</td>\n",
       "      <td>0.038407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.236166</td>\n",
       "      <td>0.119706</td>\n",
       "      <td>0.074007</td>\n",
       "      <td>1.589358e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.975819</td>\n",
       "      <td>0.890047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.514666</td>\n",
       "      <td>0.063907</td>\n",
       "      <td>23</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.835779</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.834906</td>\n",
       "      <td>0.037428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.784479</td>\n",
       "      <td>0.107923</td>\n",
       "      <td>0.109123</td>\n",
       "      <td>1.981880e-02</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.972247</td>\n",
       "      <td>0.890363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.511520</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>25</td>\n",
       "      <td>0.835779</td>\n",
       "      <td>0.891684</td>\n",
       "      <td>0.916841</td>\n",
       "      <td>0.907757</td>\n",
       "      <td>0.888015</td>\n",
       "      <td>0.031475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.942293</td>\n",
       "      <td>0.037418</td>\n",
       "      <td>0.046877</td>\n",
       "      <td>6.344746e-06</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981410</td>\n",
       "      <td>0.920846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.537726</td>\n",
       "      <td>0.063562</td>\n",
       "      <td>3</td>\n",
       "      <td>0.606569</td>\n",
       "      <td>0.685535</td>\n",
       "      <td>0.714885</td>\n",
       "      <td>0.689727</td>\n",
       "      <td>0.674179</td>\n",
       "      <td>0.040616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.797441</td>\n",
       "      <td>0.053532</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>2.030439e-02</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981878</td>\n",
       "      <td>0.919125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597484</td>\n",
       "      <td>0.527771</td>\n",
       "      <td>0.047975</td>\n",
       "      <td>12</td>\n",
       "      <td>0.635919</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.732355</td>\n",
       "      <td>0.712788</td>\n",
       "      <td>0.046351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.030110</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>0.062490</td>\n",
       "      <td>1.030919e-05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.913181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593291</td>\n",
       "      <td>0.520433</td>\n",
       "      <td>0.048657</td>\n",
       "      <td>19</td>\n",
       "      <td>0.696716</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.819008</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>0.773410</td>\n",
       "      <td>0.046801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.160142</td>\n",
       "      <td>0.111802</td>\n",
       "      <td>0.093746</td>\n",
       "      <td>2.787433e-06</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>0.902096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.510474</td>\n",
       "      <td>0.055337</td>\n",
       "      <td>26</td>\n",
       "      <td>0.747030</td>\n",
       "      <td>0.822502</td>\n",
       "      <td>0.865129</td>\n",
       "      <td>0.850454</td>\n",
       "      <td>0.821279</td>\n",
       "      <td>0.045520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.949221</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.054682</td>\n",
       "      <td>1.354142e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.979664</td>\n",
       "      <td>0.895185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.528290</td>\n",
       "      <td>0.072939</td>\n",
       "      <td>11</td>\n",
       "      <td>0.607966</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.725367</td>\n",
       "      <td>0.701607</td>\n",
       "      <td>0.680294</td>\n",
       "      <td>0.044024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.831514</td>\n",
       "      <td>0.033565</td>\n",
       "      <td>0.058592</td>\n",
       "      <td>2.029400e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.979098</td>\n",
       "      <td>0.896087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>0.063265</td>\n",
       "      <td>17</td>\n",
       "      <td>0.655486</td>\n",
       "      <td>0.730259</td>\n",
       "      <td>0.767994</td>\n",
       "      <td>0.741440</td>\n",
       "      <td>0.723795</td>\n",
       "      <td>0.041752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.472657</td>\n",
       "      <td>0.043318</td>\n",
       "      <td>0.066403</td>\n",
       "      <td>6.763003e-03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978551</td>\n",
       "      <td>0.896703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.523576</td>\n",
       "      <td>0.065315</td>\n",
       "      <td>14</td>\n",
       "      <td>0.716282</td>\n",
       "      <td>0.796646</td>\n",
       "      <td>0.823201</td>\n",
       "      <td>0.809224</td>\n",
       "      <td>0.786338</td>\n",
       "      <td>0.041523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.641785</td>\n",
       "      <td>0.118673</td>\n",
       "      <td>0.089843</td>\n",
       "      <td>6.767442e-03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978074</td>\n",
       "      <td>0.894272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.519907</td>\n",
       "      <td>0.064565</td>\n",
       "      <td>21</td>\n",
       "      <td>0.766597</td>\n",
       "      <td>0.832285</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.854647</td>\n",
       "      <td>0.828966</td>\n",
       "      <td>0.037663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.964845</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.046870</td>\n",
       "      <td>1.104779e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980877</td>\n",
       "      <td>0.919508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.533008</td>\n",
       "      <td>0.071561</td>\n",
       "      <td>7</td>\n",
       "      <td>0.591894</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.682041</td>\n",
       "      <td>0.662648</td>\n",
       "      <td>0.041784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.789047</td>\n",
       "      <td>0.056867</td>\n",
       "      <td>0.054681</td>\n",
       "      <td>7.812859e-03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981155</td>\n",
       "      <td>0.918554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633124</td>\n",
       "      <td>0.536677</td>\n",
       "      <td>0.068431</td>\n",
       "      <td>4</td>\n",
       "      <td>0.614256</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.721873</td>\n",
       "      <td>0.704403</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.590663</td>\n",
       "      <td>0.104142</td>\n",
       "      <td>0.089025</td>\n",
       "      <td>3.004021e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.909749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.525148</td>\n",
       "      <td>0.065251</td>\n",
       "      <td>13</td>\n",
       "      <td>0.662474</td>\n",
       "      <td>0.747030</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.738994</td>\n",
       "      <td>0.046466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.865595</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.093749</td>\n",
       "      <td>3.576279e-07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981014</td>\n",
       "      <td>0.905370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614256</td>\n",
       "      <td>0.522527</td>\n",
       "      <td>0.064724</td>\n",
       "      <td>15</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.824598</td>\n",
       "      <td>0.803634</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.047091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.010746</td>\n",
       "      <td>0.017026</td>\n",
       "      <td>0.046872</td>\n",
       "      <td>8.066691e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980584</td>\n",
       "      <td>0.898009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654088</td>\n",
       "      <td>0.539296</td>\n",
       "      <td>0.080232</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.662474</td>\n",
       "      <td>0.696017</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.650594</td>\n",
       "      <td>0.045002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.466631</td>\n",
       "      <td>0.041158</td>\n",
       "      <td>0.064034</td>\n",
       "      <td>1.625492e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980691</td>\n",
       "      <td>0.901120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631027</td>\n",
       "      <td>0.534579</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>5</td>\n",
       "      <td>0.607966</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.710692</td>\n",
       "      <td>0.693920</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.039944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.073749</td>\n",
       "      <td>0.107715</td>\n",
       "      <td>0.066402</td>\n",
       "      <td>6.753197e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980072</td>\n",
       "      <td>0.888999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>0.060334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.756813</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.042985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.876342</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>0.104995</td>\n",
       "      <td>2.080029e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980126</td>\n",
       "      <td>0.890811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.520432</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>20</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.756115</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.768693</td>\n",
       "      <td>0.749301</td>\n",
       "      <td>0.042583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.144039</td>\n",
       "      <td>0.017098</td>\n",
       "      <td>0.042977</td>\n",
       "      <td>6.761799e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981005</td>\n",
       "      <td>0.922730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.534055</td>\n",
       "      <td>0.079531</td>\n",
       "      <td>6</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.654088</td>\n",
       "      <td>0.680643</td>\n",
       "      <td>0.657582</td>\n",
       "      <td>0.642383</td>\n",
       "      <td>0.038981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.836223</td>\n",
       "      <td>0.027213</td>\n",
       "      <td>0.058596</td>\n",
       "      <td>6.764688e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981278</td>\n",
       "      <td>0.919517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>0.076013</td>\n",
       "      <td>2</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.701607</td>\n",
       "      <td>0.679944</td>\n",
       "      <td>0.662648</td>\n",
       "      <td>0.041579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.563814</td>\n",
       "      <td>0.087699</td>\n",
       "      <td>0.078122</td>\n",
       "      <td>2.016012e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981661</td>\n",
       "      <td>0.916248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.529864</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>9</td>\n",
       "      <td>0.615653</td>\n",
       "      <td>0.697414</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.707897</td>\n",
       "      <td>0.688854</td>\n",
       "      <td>0.044366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.525891</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.109373</td>\n",
       "      <td>1.913932e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981686</td>\n",
       "      <td>0.913524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631027</td>\n",
       "      <td>0.530913</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>8</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.719776</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.733753</td>\n",
       "      <td>0.713312</td>\n",
       "      <td>0.044613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.140621</td>\n",
       "      <td>0.067201</td>\n",
       "      <td>0.050788</td>\n",
       "      <td>6.766239e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978728</td>\n",
       "      <td>0.876318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.957033</td>\n",
       "      <td>0.072777</td>\n",
       "      <td>0.062491</td>\n",
       "      <td>1.105006e-02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980022</td>\n",
       "      <td>0.883821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>0.288263</td>\n",
       "      <td>0.037811</td>\n",
       "      <td>29</td>\n",
       "      <td>0.252271</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.387841</td>\n",
       "      <td>0.352376</td>\n",
       "      <td>0.068272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.874999</td>\n",
       "      <td>0.094398</td>\n",
       "      <td>0.078121</td>\n",
       "      <td>2.227013e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.906263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607966</td>\n",
       "      <td>0.479030</td>\n",
       "      <td>0.076458</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439553</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.596785</td>\n",
       "      <td>0.623340</td>\n",
       "      <td>0.551712</td>\n",
       "      <td>0.070290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.025265</td>\n",
       "      <td>0.139256</td>\n",
       "      <td>0.109373</td>\n",
       "      <td>3.573794e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981053</td>\n",
       "      <td>0.913476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637317</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>0.072187</td>\n",
       "      <td>18</td>\n",
       "      <td>0.539483</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.664570</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.624389</td>\n",
       "      <td>0.051014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.214842</td>\n",
       "      <td>0.090005</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>6.762728e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978594</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.228589</td>\n",
       "      <td>0.120521</td>\n",
       "      <td>0.056160</td>\n",
       "      <td>5.716732e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.979367</td>\n",
       "      <td>0.920586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.077044</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>30</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>0.069881</td>\n",
       "      <td>0.154437</td>\n",
       "      <td>0.101328</td>\n",
       "      <td>0.097834</td>\n",
       "      <td>0.035464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.855266</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.070322</td>\n",
       "      <td>7.806734e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980562</td>\n",
       "      <td>0.928645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.448633</td>\n",
       "      <td>0.066407</td>\n",
       "      <td>28</td>\n",
       "      <td>0.386443</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.506639</td>\n",
       "      <td>0.079327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.734383</td>\n",
       "      <td>0.147604</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>6.766168e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981024</td>\n",
       "      <td>0.926110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.512571</td>\n",
       "      <td>0.076128</td>\n",
       "      <td>24</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.609364</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.599057</td>\n",
       "      <td>0.053299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        1.018934      0.034350         0.051197    7.218690e-03   \n",
       "1        1.908934      0.034056         0.058595    6.763725e-03   \n",
       "2        4.236166      0.119706         0.074007    1.589358e-03   \n",
       "3        6.784479      0.107923         0.109123    1.981880e-02   \n",
       "4        0.942293      0.037418         0.046877    6.344746e-06   \n",
       "5        1.797441      0.053532         0.058597    2.030439e-02   \n",
       "6        4.030110      0.064890         0.062490    1.030919e-05   \n",
       "7        5.160142      0.111802         0.093746    2.787433e-06   \n",
       "8        0.949221      0.020303         0.054682    1.354142e-02   \n",
       "9        1.831514      0.033565         0.058592    2.029400e-02   \n",
       "10       3.472657      0.043318         0.066403    6.763003e-03   \n",
       "11       5.641785      0.118673         0.089843    6.767442e-03   \n",
       "12       0.964845      0.023105         0.046870    1.104779e-02   \n",
       "13       1.789047      0.056867         0.054681    7.812859e-03   \n",
       "14       3.590663      0.104142         0.089025    3.004021e-02   \n",
       "15       5.865595      0.084518         0.093749    3.576279e-07   \n",
       "16       1.010746      0.017026         0.046872    8.066691e-06   \n",
       "17       2.466631      0.041158         0.064034    1.625492e-02   \n",
       "18       4.073749      0.107715         0.066402    6.753197e-03   \n",
       "19       6.876342      0.161335         0.104995    2.080029e-02   \n",
       "20       1.144039      0.017098         0.042977    6.761799e-03   \n",
       "21       1.836223      0.027213         0.058596    6.764688e-03   \n",
       "22       3.563814      0.087699         0.078122    2.016012e-06   \n",
       "23       5.525891      0.050021         0.109373    1.913932e-02   \n",
       "24       1.140621      0.067201         0.050788    6.766239e-03   \n",
       "25       1.957033      0.072777         0.062491    1.105006e-02   \n",
       "26       3.874999      0.094398         0.078121    2.227013e-06   \n",
       "27       6.025265      0.139256         0.109373    3.573794e-06   \n",
       "28       1.214842      0.090005         0.058594    6.762728e-03   \n",
       "29       2.228589      0.120521         0.056160    5.716732e-03   \n",
       "30       3.855266      0.109932         0.070322    7.806734e-03   \n",
       "31       5.734383      0.147604         0.105469    6.766168e-03   \n",
       "\n",
       "   param_gradientboostingclassifier__learning_rate  \\\n",
       "0                                              0.3   \n",
       "1                                              0.3   \n",
       "2                                              0.3   \n",
       "3                                              0.3   \n",
       "4                                              0.3   \n",
       "5                                              0.3   \n",
       "6                                              0.3   \n",
       "7                                              0.3   \n",
       "8                                              0.2   \n",
       "9                                              0.2   \n",
       "10                                             0.2   \n",
       "11                                             0.2   \n",
       "12                                             0.2   \n",
       "13                                             0.2   \n",
       "14                                             0.2   \n",
       "15                                             0.2   \n",
       "16                                             0.1   \n",
       "17                                             0.1   \n",
       "18                                             0.1   \n",
       "19                                             0.1   \n",
       "20                                             0.1   \n",
       "21                                             0.1   \n",
       "22                                             0.1   \n",
       "23                                             0.1   \n",
       "24                                            0.01   \n",
       "25                                            0.01   \n",
       "26                                            0.01   \n",
       "27                                            0.01   \n",
       "28                                            0.01   \n",
       "29                                            0.01   \n",
       "30                                            0.01   \n",
       "31                                            0.01   \n",
       "\n",
       "   param_gradientboostingclassifier__loss  \\\n",
       "0                                deviance   \n",
       "1                                deviance   \n",
       "2                                deviance   \n",
       "3                                deviance   \n",
       "4                             exponential   \n",
       "5                             exponential   \n",
       "6                             exponential   \n",
       "7                             exponential   \n",
       "8                                deviance   \n",
       "9                                deviance   \n",
       "10                               deviance   \n",
       "11                               deviance   \n",
       "12                            exponential   \n",
       "13                            exponential   \n",
       "14                            exponential   \n",
       "15                            exponential   \n",
       "16                               deviance   \n",
       "17                               deviance   \n",
       "18                               deviance   \n",
       "19                               deviance   \n",
       "20                            exponential   \n",
       "21                            exponential   \n",
       "22                            exponential   \n",
       "23                            exponential   \n",
       "24                               deviance   \n",
       "25                               deviance   \n",
       "26                               deviance   \n",
       "27                               deviance   \n",
       "28                            exponential   \n",
       "29                            exponential   \n",
       "30                            exponential   \n",
       "31                            exponential   \n",
       "\n",
       "   param_gradientboostingclassifier__n_estimators  \\\n",
       "0                                              50   \n",
       "1                                             100   \n",
       "2                                             200   \n",
       "3                                             300   \n",
       "4                                              50   \n",
       "5                                             100   \n",
       "6                                             200   \n",
       "7                                             300   \n",
       "8                                              50   \n",
       "9                                             100   \n",
       "10                                            200   \n",
       "11                                            300   \n",
       "12                                             50   \n",
       "13                                            100   \n",
       "14                                            200   \n",
       "15                                            300   \n",
       "16                                             50   \n",
       "17                                            100   \n",
       "18                                            200   \n",
       "19                                            300   \n",
       "20                                             50   \n",
       "21                                            100   \n",
       "22                                            200   \n",
       "23                                            300   \n",
       "24                                             50   \n",
       "25                                            100   \n",
       "26                                            200   \n",
       "27                                            300   \n",
       "28                                             50   \n",
       "29                                            100   \n",
       "30                                            200   \n",
       "31                                            300   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'gradientboostingclassifier__learning_rate': ...         0.980061   \n",
       "1   {'gradientboostingclassifier__learning_rate': ...         0.978085   \n",
       "2   {'gradientboostingclassifier__learning_rate': ...         0.975819   \n",
       "3   {'gradientboostingclassifier__learning_rate': ...         0.972247   \n",
       "4   {'gradientboostingclassifier__learning_rate': ...         0.981410   \n",
       "5   {'gradientboostingclassifier__learning_rate': ...         0.981878   \n",
       "6   {'gradientboostingclassifier__learning_rate': ...         0.981495   \n",
       "7   {'gradientboostingclassifier__learning_rate': ...         0.980849   \n",
       "8   {'gradientboostingclassifier__learning_rate': ...         0.979664   \n",
       "9   {'gradientboostingclassifier__learning_rate': ...         0.979098   \n",
       "10  {'gradientboostingclassifier__learning_rate': ...         0.978551   \n",
       "11  {'gradientboostingclassifier__learning_rate': ...         0.978074   \n",
       "12  {'gradientboostingclassifier__learning_rate': ...         0.980877   \n",
       "13  {'gradientboostingclassifier__learning_rate': ...         0.981155   \n",
       "14  {'gradientboostingclassifier__learning_rate': ...         0.981408   \n",
       "15  {'gradientboostingclassifier__learning_rate': ...         0.981014   \n",
       "16  {'gradientboostingclassifier__learning_rate': ...         0.980584   \n",
       "17  {'gradientboostingclassifier__learning_rate': ...         0.980691   \n",
       "18  {'gradientboostingclassifier__learning_rate': ...         0.980072   \n",
       "19  {'gradientboostingclassifier__learning_rate': ...         0.980126   \n",
       "20  {'gradientboostingclassifier__learning_rate': ...         0.981005   \n",
       "21  {'gradientboostingclassifier__learning_rate': ...         0.981278   \n",
       "22  {'gradientboostingclassifier__learning_rate': ...         0.981661   \n",
       "23  {'gradientboostingclassifier__learning_rate': ...         0.981686   \n",
       "24  {'gradientboostingclassifier__learning_rate': ...         0.978728   \n",
       "25  {'gradientboostingclassifier__learning_rate': ...         0.980022   \n",
       "26  {'gradientboostingclassifier__learning_rate': ...         0.980583   \n",
       "27  {'gradientboostingclassifier__learning_rate': ...         0.981053   \n",
       "28  {'gradientboostingclassifier__learning_rate': ...         0.978594   \n",
       "29  {'gradientboostingclassifier__learning_rate': ...         0.979367   \n",
       "30  {'gradientboostingclassifier__learning_rate': ...         0.980562   \n",
       "31  {'gradientboostingclassifier__learning_rate': ...         0.981024   \n",
       "\n",
       "    split1_test_AUC  ...  split3_test_Sensitivity  mean_test_Sensitivity  \\\n",
       "0          0.893875  ...                 0.626834               0.522001   \n",
       "1          0.888735  ...                 0.612159               0.517809   \n",
       "2          0.890047  ...                 0.603774               0.514666   \n",
       "3          0.890363  ...                 0.612159               0.511520   \n",
       "4          0.920846  ...                 0.624738               0.537726   \n",
       "5          0.919125  ...                 0.597484               0.527771   \n",
       "6          0.913181  ...                 0.593291               0.520433   \n",
       "7          0.902096  ...                 0.587002               0.510474   \n",
       "8          0.895185  ...                 0.624738               0.528290   \n",
       "9          0.896087  ...                 0.612159               0.521479   \n",
       "10         0.896703  ...                 0.622642               0.523576   \n",
       "11         0.894272  ...                 0.616352               0.519907   \n",
       "12         0.919508  ...                 0.624738               0.533008   \n",
       "13         0.918554  ...                 0.633124               0.536677   \n",
       "14         0.909749  ...                 0.616352               0.525148   \n",
       "15         0.905370  ...                 0.614256               0.522527   \n",
       "16         0.898009  ...                 0.654088               0.539296   \n",
       "17         0.901120  ...                 0.631027               0.534579   \n",
       "18         0.888999  ...                 0.612159               0.529341   \n",
       "19         0.890811  ...                 0.605870               0.520432   \n",
       "20         0.922730  ...                 0.645702               0.534055   \n",
       "21         0.919517  ...                 0.641509               0.538248   \n",
       "22         0.916248  ...                 0.622642               0.529864   \n",
       "23         0.913524  ...                 0.631027               0.530913   \n",
       "24         0.876318  ...                 0.000000               0.000000   \n",
       "25         0.883821  ...                 0.266247               0.288263   \n",
       "26         0.906263  ...                 0.607966               0.479030   \n",
       "27         0.913476  ...                 0.637317               0.521479   \n",
       "28         0.875114  ...                 0.000000               0.000000   \n",
       "29         0.920586  ...                 0.064990               0.077044   \n",
       "30         0.928645  ...                 0.555556               0.448633   \n",
       "31         0.926110  ...                 0.643606               0.512571   \n",
       "\n",
       "    std_test_Sensitivity  rank_test_Sensitivity  split0_train_Sensitivity  \\\n",
       "0               0.080801                     16                  0.629630   \n",
       "1               0.071062                     22                  0.697414   \n",
       "2               0.063907                     23                  0.773585   \n",
       "3               0.071401                     25                  0.835779   \n",
       "4               0.063562                      3                  0.606569   \n",
       "5               0.047975                     12                  0.635919   \n",
       "6               0.048657                     19                  0.696716   \n",
       "7               0.055337                     26                  0.747030   \n",
       "8               0.072939                     11                  0.607966   \n",
       "9               0.063265                     17                  0.655486   \n",
       "10              0.065315                     14                  0.716282   \n",
       "11              0.064565                     21                  0.766597   \n",
       "12              0.071561                      7                  0.591894   \n",
       "13              0.068431                      4                  0.614256   \n",
       "14              0.065251                     13                  0.662474   \n",
       "15              0.064724                     15                  0.700210   \n",
       "16              0.080232                      1                  0.575821   \n",
       "17              0.076037                      5                  0.607966   \n",
       "18              0.060334                     10                  0.642907   \n",
       "19              0.057093                     20                  0.679245   \n",
       "20              0.079531                      6                  0.577219   \n",
       "21              0.076013                      2                  0.592593   \n",
       "22              0.065332                      9                  0.615653   \n",
       "23              0.065332                      8                  0.640112   \n",
       "24              0.000000                     31                  0.000000   \n",
       "25              0.037811                     29                  0.252271   \n",
       "26              0.076458                     27                  0.439553   \n",
       "27              0.072187                     18                  0.539483   \n",
       "28              0.000000                     31                  0.000000   \n",
       "29              0.009182                     30                  0.065688   \n",
       "30              0.066407                     28                  0.386443   \n",
       "31              0.076128                     24                  0.509434   \n",
       "\n",
       "    split1_train_Sensitivity  split2_train_Sensitivity  \\\n",
       "0                   0.711391                  0.742138   \n",
       "1                   0.780573                  0.791055   \n",
       "2                   0.835779                  0.867925   \n",
       "3                   0.891684                  0.916841   \n",
       "4                   0.685535                  0.714885   \n",
       "5                   0.723270                  0.759609   \n",
       "6                   0.776380                  0.819008   \n",
       "7                   0.822502                  0.865129   \n",
       "8                   0.686233                  0.725367   \n",
       "9                   0.730259                  0.767994   \n",
       "10                  0.796646                  0.823201   \n",
       "11                  0.832285                  0.862334   \n",
       "12                  0.676450                  0.700210   \n",
       "13                  0.700210                  0.721873   \n",
       "14                  0.747030                  0.786862   \n",
       "15                  0.777079                  0.824598   \n",
       "16                  0.662474                  0.696017   \n",
       "17                  0.691125                  0.710692   \n",
       "18                  0.723270                  0.756813   \n",
       "19                  0.756115                  0.793152   \n",
       "20                  0.654088                  0.680643   \n",
       "21                  0.676450                  0.701607   \n",
       "22                  0.697414                  0.734451   \n",
       "23                  0.719776                  0.759609   \n",
       "24                  0.000000                  0.000000   \n",
       "25                  0.333333                  0.436059   \n",
       "26                  0.547170                  0.596785   \n",
       "27                  0.629630                  0.664570   \n",
       "28                  0.000000                  0.000000   \n",
       "29                  0.069881                  0.154437   \n",
       "30                  0.484277                  0.572327   \n",
       "31                  0.609364                  0.632425   \n",
       "\n",
       "    split3_train_Sensitivity  mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                   0.735150                0.704577               0.044747  \n",
       "1                   0.785465                0.763627               0.038407  \n",
       "2                   0.862334                0.834906               0.037428  \n",
       "3                   0.907757                0.888015               0.031475  \n",
       "4                   0.689727                0.674179               0.040616  \n",
       "5                   0.732355                0.712788               0.046351  \n",
       "6                   0.801537                0.773410               0.046801  \n",
       "7                   0.850454                0.821279               0.045520  \n",
       "8                   0.701607                0.680294               0.044024  \n",
       "9                   0.741440                0.723795               0.041752  \n",
       "10                  0.809224                0.786338               0.041523  \n",
       "11                  0.854647                0.828966               0.037663  \n",
       "12                  0.682041                0.662648               0.041784  \n",
       "13                  0.704403                0.685185               0.041749  \n",
       "14                  0.759609                0.738994               0.046466  \n",
       "15                  0.803634                0.776380               0.047091  \n",
       "16                  0.668064                0.650594               0.045002  \n",
       "17                  0.693920                0.675926               0.039944  \n",
       "18                  0.734451                0.714361               0.042985  \n",
       "19                  0.768693                0.749301               0.042583  \n",
       "20                  0.657582                0.642383               0.038981  \n",
       "21                  0.679944                0.662648               0.041579  \n",
       "22                  0.707897                0.688854               0.044366  \n",
       "23                  0.733753                0.713312               0.044613  \n",
       "24                  0.000000                0.000000               0.000000  \n",
       "25                  0.387841                0.352376               0.068272  \n",
       "26                  0.623340                0.551712               0.070290  \n",
       "27                  0.663871                0.624389               0.051014  \n",
       "28                  0.000000                0.000000               0.000000  \n",
       "29                  0.101328                0.097834               0.035464  \n",
       "30                  0.583508                0.506639               0.079327  \n",
       "31                  0.645003                0.599057               0.053299  \n",
       "\n",
       "[32 rows x 47 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gbt = gs.cv_results_\n",
    "data = pandas.DataFrame(results_gbt)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018934</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.872946</td>\n",
       "      <td>0.544303</td>\n",
       "      <td>0.522001</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>0.957320</td>\n",
       "      <td>0.758080</td>\n",
       "      <td>0.704577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.908934</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.870756</td>\n",
       "      <td>0.540353</td>\n",
       "      <td>0.517809</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969131</td>\n",
       "      <td>0.815212</td>\n",
       "      <td>0.763627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.236166</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.868728</td>\n",
       "      <td>0.534473</td>\n",
       "      <td>0.514666</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>0.983424</td>\n",
       "      <td>0.880818</td>\n",
       "      <td>0.834906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.784479</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.865349</td>\n",
       "      <td>0.528994</td>\n",
       "      <td>0.511520</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>0.990853</td>\n",
       "      <td>0.926336</td>\n",
       "      <td>0.888015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.942293</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888167</td>\n",
       "      <td>0.564358</td>\n",
       "      <td>0.537726</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.957226</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.674179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.797441</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.884061</td>\n",
       "      <td>0.556206</td>\n",
       "      <td>0.527771</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.968761</td>\n",
       "      <td>0.769530</td>\n",
       "      <td>0.712788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.030110</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878462</td>\n",
       "      <td>0.546526</td>\n",
       "      <td>0.520433</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>0.982480</td>\n",
       "      <td>0.827539</td>\n",
       "      <td>0.773410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.160142</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.873521</td>\n",
       "      <td>0.537298</td>\n",
       "      <td>0.510474</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>0.990113</td>\n",
       "      <td>0.872008</td>\n",
       "      <td>0.821279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.949221</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.876137</td>\n",
       "      <td>0.554978</td>\n",
       "      <td>0.528290</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.952377</td>\n",
       "      <td>0.734608</td>\n",
       "      <td>0.680294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.831514</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.875376</td>\n",
       "      <td>0.548371</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>0.962328</td>\n",
       "      <td>0.778154</td>\n",
       "      <td>0.723795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.472657</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.873858</td>\n",
       "      <td>0.549199</td>\n",
       "      <td>0.523576</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>0.975941</td>\n",
       "      <td>0.837674</td>\n",
       "      <td>0.786338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.641785</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.543803</td>\n",
       "      <td>0.519907</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>0.984011</td>\n",
       "      <td>0.876723</td>\n",
       "      <td>0.828966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.964845</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888308</td>\n",
       "      <td>0.560044</td>\n",
       "      <td>0.533008</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.952037</td>\n",
       "      <td>0.716416</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.789047</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888604</td>\n",
       "      <td>0.564260</td>\n",
       "      <td>0.536677</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.961848</td>\n",
       "      <td>0.742433</td>\n",
       "      <td>0.685185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.590663</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.882644</td>\n",
       "      <td>0.553299</td>\n",
       "      <td>0.525148</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0.974691</td>\n",
       "      <td>0.794916</td>\n",
       "      <td>0.738994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.865595</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878666</td>\n",
       "      <td>0.548506</td>\n",
       "      <td>0.522527</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0.982918</td>\n",
       "      <td>0.832414</td>\n",
       "      <td>0.776380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.010746</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.879321</td>\n",
       "      <td>0.563918</td>\n",
       "      <td>0.539296</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945533</td>\n",
       "      <td>0.706109</td>\n",
       "      <td>0.650594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.466631</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878823</td>\n",
       "      <td>0.558759</td>\n",
       "      <td>0.534579</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.952546</td>\n",
       "      <td>0.731256</td>\n",
       "      <td>0.675926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.073749</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.874902</td>\n",
       "      <td>0.554668</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962220</td>\n",
       "      <td>0.771219</td>\n",
       "      <td>0.714361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.876342</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.874559</td>\n",
       "      <td>0.547532</td>\n",
       "      <td>0.520432</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969629</td>\n",
       "      <td>0.805357</td>\n",
       "      <td>0.749301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.144039</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.887300</td>\n",
       "      <td>0.564191</td>\n",
       "      <td>0.534055</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.945660</td>\n",
       "      <td>0.700368</td>\n",
       "      <td>0.642383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.836223</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888507</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.952666</td>\n",
       "      <td>0.715934</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.563814</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.887299</td>\n",
       "      <td>0.558405</td>\n",
       "      <td>0.529864</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.688854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.525891</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.884334</td>\n",
       "      <td>0.555391</td>\n",
       "      <td>0.530913</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.771530</td>\n",
       "      <td>0.713312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.140621</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.860386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0.925091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.957033</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.865128</td>\n",
       "      <td>0.423044</td>\n",
       "      <td>0.288263</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.931197</td>\n",
       "      <td>0.499943</td>\n",
       "      <td>0.352376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.874999</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878733</td>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.479030</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>0.650173</td>\n",
       "      <td>0.551712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.025265</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.883566</td>\n",
       "      <td>0.558328</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.691046</td>\n",
       "      <td>0.624389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.214842</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.863905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0.928807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.228589</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.876798</td>\n",
       "      <td>0.141746</td>\n",
       "      <td>0.077044</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.932455</td>\n",
       "      <td>0.175823</td>\n",
       "      <td>0.097834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.855266</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.884999</td>\n",
       "      <td>0.533070</td>\n",
       "      <td>0.448633</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>0.937284</td>\n",
       "      <td>0.621062</td>\n",
       "      <td>0.506639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.734383</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.886453</td>\n",
       "      <td>0.561638</td>\n",
       "      <td>0.512571</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>0.940853</td>\n",
       "      <td>0.676696</td>\n",
       "      <td>0.599057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        1.018934  {'gradientboostingclassifier__learning_rate': ...   \n",
       "1        1.908934  {'gradientboostingclassifier__learning_rate': ...   \n",
       "2        4.236166  {'gradientboostingclassifier__learning_rate': ...   \n",
       "3        6.784479  {'gradientboostingclassifier__learning_rate': ...   \n",
       "4        0.942293  {'gradientboostingclassifier__learning_rate': ...   \n",
       "5        1.797441  {'gradientboostingclassifier__learning_rate': ...   \n",
       "6        4.030110  {'gradientboostingclassifier__learning_rate': ...   \n",
       "7        5.160142  {'gradientboostingclassifier__learning_rate': ...   \n",
       "8        0.949221  {'gradientboostingclassifier__learning_rate': ...   \n",
       "9        1.831514  {'gradientboostingclassifier__learning_rate': ...   \n",
       "10       3.472657  {'gradientboostingclassifier__learning_rate': ...   \n",
       "11       5.641785  {'gradientboostingclassifier__learning_rate': ...   \n",
       "12       0.964845  {'gradientboostingclassifier__learning_rate': ...   \n",
       "13       1.789047  {'gradientboostingclassifier__learning_rate': ...   \n",
       "14       3.590663  {'gradientboostingclassifier__learning_rate': ...   \n",
       "15       5.865595  {'gradientboostingclassifier__learning_rate': ...   \n",
       "16       1.010746  {'gradientboostingclassifier__learning_rate': ...   \n",
       "17       2.466631  {'gradientboostingclassifier__learning_rate': ...   \n",
       "18       4.073749  {'gradientboostingclassifier__learning_rate': ...   \n",
       "19       6.876342  {'gradientboostingclassifier__learning_rate': ...   \n",
       "20       1.144039  {'gradientboostingclassifier__learning_rate': ...   \n",
       "21       1.836223  {'gradientboostingclassifier__learning_rate': ...   \n",
       "22       3.563814  {'gradientboostingclassifier__learning_rate': ...   \n",
       "23       5.525891  {'gradientboostingclassifier__learning_rate': ...   \n",
       "24       1.140621  {'gradientboostingclassifier__learning_rate': ...   \n",
       "25       1.957033  {'gradientboostingclassifier__learning_rate': ...   \n",
       "26       3.874999  {'gradientboostingclassifier__learning_rate': ...   \n",
       "27       6.025265  {'gradientboostingclassifier__learning_rate': ...   \n",
       "28       1.214842  {'gradientboostingclassifier__learning_rate': ...   \n",
       "29       2.228589  {'gradientboostingclassifier__learning_rate': ...   \n",
       "30       3.855266  {'gradientboostingclassifier__learning_rate': ...   \n",
       "31       5.734383  {'gradientboostingclassifier__learning_rate': ...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.872946           0.544303               0.522001             25   \n",
       "1        0.870756           0.540353               0.517809             27   \n",
       "2        0.868728           0.534473               0.514666             28   \n",
       "3        0.865349           0.528994               0.511520             29   \n",
       "4        0.888167           0.564358               0.537726              4   \n",
       "5        0.884061           0.556206               0.527771             10   \n",
       "6        0.878462           0.546526               0.520433             17   \n",
       "7        0.873521           0.537298               0.510474             24   \n",
       "8        0.876137           0.554978               0.528290             19   \n",
       "9        0.875376           0.548371               0.521479             20   \n",
       "10       0.873858           0.549199               0.523576             23   \n",
       "11       0.871421           0.543803               0.519907             26   \n",
       "12       0.888308           0.560044               0.533008              3   \n",
       "13       0.888604           0.564260               0.536677              1   \n",
       "14       0.882644           0.553299               0.525148             12   \n",
       "15       0.878666           0.548506               0.522527             16   \n",
       "16       0.879321           0.563918               0.539296             13   \n",
       "17       0.878823           0.558759               0.534579             14   \n",
       "18       0.874902           0.554668               0.529341             21   \n",
       "19       0.874559           0.547532               0.520432             22   \n",
       "20       0.887300           0.564191               0.534055              5   \n",
       "21       0.888507           0.564609               0.538248              2   \n",
       "22       0.887299           0.558405               0.529864              6   \n",
       "23       0.884334           0.555391               0.530913              9   \n",
       "24       0.860386           0.000000               0.000000             32   \n",
       "25       0.865128           0.423044               0.288263             30   \n",
       "26       0.878733           0.542255               0.479030             15   \n",
       "27       0.883566           0.558328               0.521479             11   \n",
       "28       0.863905           0.000000               0.000000             31   \n",
       "29       0.876798           0.141746               0.077044             18   \n",
       "30       0.884999           0.533070               0.448633              8   \n",
       "31       0.886453           0.561638               0.512571              7   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  21                     16        0.957320   \n",
       "1                  24                     22        0.969131   \n",
       "2                  26                     23        0.983424   \n",
       "3                  28                     25        0.990853   \n",
       "4                   2                      3        0.957226   \n",
       "5                  11                     12        0.968761   \n",
       "6                  20                     19        0.982480   \n",
       "7                  25                     26        0.990113   \n",
       "8                  13                     11        0.952377   \n",
       "9                  18                     17        0.962328   \n",
       "10                 16                     14        0.975941   \n",
       "11                 22                     21        0.984011   \n",
       "12                  7                      7        0.952037   \n",
       "13                  3                      4        0.961848   \n",
       "14                 15                     13        0.974691   \n",
       "15                 17                     15        0.982918   \n",
       "16                  5                      1        0.945533   \n",
       "17                  8                      5        0.952546   \n",
       "18                 14                     10        0.962220   \n",
       "19                 19                     20        0.969629   \n",
       "20                  4                      6        0.945660   \n",
       "21                  1                      2        0.952666   \n",
       "22                  9                      9        0.962348   \n",
       "23                 12                      8        0.969423   \n",
       "24                 31                     31        0.925091   \n",
       "25                 29                     29        0.931197   \n",
       "26                 23                     27        0.936753   \n",
       "27                 10                     18        0.939973   \n",
       "28                 31                     31        0.928807   \n",
       "29                 30                     30        0.932455   \n",
       "30                 27                     28        0.937284   \n",
       "31                  6                     24        0.940853   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.758080                0.704577  \n",
       "1             0.815212                0.763627  \n",
       "2             0.880818                0.834906  \n",
       "3             0.926336                0.888015  \n",
       "4             0.726937                0.674179  \n",
       "5             0.769530                0.712788  \n",
       "6             0.827539                0.773410  \n",
       "7             0.872008                0.821279  \n",
       "8             0.734608                0.680294  \n",
       "9             0.778154                0.723795  \n",
       "10            0.837674                0.786338  \n",
       "11            0.876723                0.828966  \n",
       "12            0.716416                0.662648  \n",
       "13            0.742433                0.685185  \n",
       "14            0.794916                0.738994  \n",
       "15            0.832414                0.776380  \n",
       "16            0.706109                0.650594  \n",
       "17            0.731256                0.675926  \n",
       "18            0.771219                0.714361  \n",
       "19            0.805357                0.749301  \n",
       "20            0.700368                0.642383  \n",
       "21            0.715934                0.662648  \n",
       "22            0.745064                0.688854  \n",
       "23            0.771530                0.713312  \n",
       "24            0.000000                0.000000  \n",
       "25            0.499943                0.352376  \n",
       "26            0.650173                0.551712  \n",
       "27            0.691046                0.624389  \n",
       "28            0.000000                0.000000  \n",
       "29            0.175823                0.097834  \n",
       "30            0.621062                0.506639  \n",
       "31            0.676696                0.599057  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_gbt = make_table(data)\n",
    "table_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier__learning_rate': 0.1,\n",
       " 'gradientboostingclassifier__loss': 'exponential',\n",
       " 'gradientboostingclassifier__n_estimators': 100}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  48 | elapsed:    4.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('quadraticdiscriminantanalysis',\n",
       "                                        QuadraticDiscriminantAnalysis(priors=None,\n",
       "                                                                      reg_param=0.0,\n",
       "                                                                      store_covariance=False,\n",
       "                                                                      tol=0.0001))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'quadraticdiscriminantanalysis__reg_param': [0, 0.001,\n",
       "                                                                      0.01, 0.1,\n",
       "                                                                      0.6, 1],\n",
       "                         'quadraticdiscriminantanalysis__store_covariance': (True,\n",
       "                                                                             False)},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'quadraticdiscriminantanalysis__reg_param':([0, 0.001, 0.01, 0.1, 0.6, 1]), \n",
    "              'quadraticdiscriminantanalysis__store_covariance':(True,False)}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), QuadraticDiscriminantAnalysis()) \n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_quadraticdiscriminantanalysis__reg_param</th>\n",
       "      <th>param_quadraticdiscriminantanalysis__store_covariance</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.094181</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.082813</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848531</td>\n",
       "      <td>0.828068</td>\n",
       "      <td>0.800665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>0.115727</td>\n",
       "      <td>1</td>\n",
       "      <td>0.644305</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.631901</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074236</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.058588</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848531</td>\n",
       "      <td>0.828068</td>\n",
       "      <td>0.800665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>0.115727</td>\n",
       "      <td>1</td>\n",
       "      <td>0.644305</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.631901</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108742</td>\n",
       "      <td>0.015026</td>\n",
       "      <td>0.083789</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>0.829036</td>\n",
       "      <td>0.800740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740042</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>0.115393</td>\n",
       "      <td>3</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.589797</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.650594</td>\n",
       "      <td>0.630503</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117974</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.086623</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>0.829036</td>\n",
       "      <td>0.800740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740042</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>0.115393</td>\n",
       "      <td>3</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.589797</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.650594</td>\n",
       "      <td>0.630503</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158316</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.023579</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.834768</td>\n",
       "      <td>0.801243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>5</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.580014</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.620720</td>\n",
       "      <td>0.023762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.143254</td>\n",
       "      <td>0.022813</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.834768</td>\n",
       "      <td>0.801243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>5</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.580014</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.620720</td>\n",
       "      <td>0.023762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097351</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.054680</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.850243</td>\n",
       "      <td>0.846531</td>\n",
       "      <td>0.801359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.095190</td>\n",
       "      <td>7</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.531796</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.580713</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.021127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092275</td>\n",
       "      <td>0.022673</td>\n",
       "      <td>0.064995</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.850243</td>\n",
       "      <td>0.846531</td>\n",
       "      <td>0.801359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.095190</td>\n",
       "      <td>7</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.531796</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.580713</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.021127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.097658</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.046885</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.873458</td>\n",
       "      <td>0.851085</td>\n",
       "      <td>0.805555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574423</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>0.102468</td>\n",
       "      <td>9</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.461915</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.448812</td>\n",
       "      <td>0.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.098202</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.067987</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.873458</td>\n",
       "      <td>0.851085</td>\n",
       "      <td>0.805555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574423</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>0.102468</td>\n",
       "      <td>9</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.461915</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.448812</td>\n",
       "      <td>0.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.103969</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.077572</td>\n",
       "      <td>0.017845</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>0.131903</td>\n",
       "      <td>11</td>\n",
       "      <td>0.381551</td>\n",
       "      <td>0.403215</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.410727</td>\n",
       "      <td>0.020891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.093759</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>0.131903</td>\n",
       "      <td>11</td>\n",
       "      <td>0.381551</td>\n",
       "      <td>0.403215</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.410727</td>\n",
       "      <td>0.020891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.094181      0.007692         0.082813        0.011657   \n",
       "1        0.074236      0.006767         0.058588        0.006745   \n",
       "2        0.108742      0.015026         0.083789        0.018004   \n",
       "3        0.117974      0.017152         0.086623        0.008798   \n",
       "4        0.158316      0.009123         0.139785        0.023579   \n",
       "5        0.143254      0.022813         0.080048        0.007533   \n",
       "6        0.097351      0.015519         0.054680        0.007826   \n",
       "7        0.092275      0.022673         0.064995        0.010410   \n",
       "8        0.097658      0.006776         0.046885        0.000008   \n",
       "9        0.098202      0.013419         0.067987        0.005269   \n",
       "10       0.103969      0.004126         0.077572        0.017845   \n",
       "11       0.093759      0.000021         0.070200        0.007679   \n",
       "\n",
       "   param_quadraticdiscriminantanalysis__reg_param  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                           0.001   \n",
       "3                                           0.001   \n",
       "4                                            0.01   \n",
       "5                                            0.01   \n",
       "6                                             0.1   \n",
       "7                                             0.1   \n",
       "8                                             0.6   \n",
       "9                                             0.6   \n",
       "10                                              1   \n",
       "11                                              1   \n",
       "\n",
       "   param_quadraticdiscriminantanalysis__store_covariance  \\\n",
       "0                                                True      \n",
       "1                                               False      \n",
       "2                                                True      \n",
       "3                                               False      \n",
       "4                                                True      \n",
       "5                                               False      \n",
       "6                                                True      \n",
       "7                                               False      \n",
       "8                                                True      \n",
       "9                                               False      \n",
       "10                                               True      \n",
       "11                                              False      \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848531   \n",
       "1   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848531   \n",
       "2   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848548   \n",
       "3   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848548   \n",
       "4   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848644   \n",
       "5   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848644   \n",
       "6   {'quadraticdiscriminantanalysis__reg_param': 0...         0.850243   \n",
       "7   {'quadraticdiscriminantanalysis__reg_param': 0...         0.850243   \n",
       "8   {'quadraticdiscriminantanalysis__reg_param': 0...         0.873458   \n",
       "9   {'quadraticdiscriminantanalysis__reg_param': 0...         0.873458   \n",
       "10  {'quadraticdiscriminantanalysis__reg_param': 1...         0.919129   \n",
       "11  {'quadraticdiscriminantanalysis__reg_param': 1...         0.919129   \n",
       "\n",
       "    split1_test_AUC  split2_test_AUC  ...  split3_test_Sensitivity  \\\n",
       "0          0.828068         0.800665  ...                 0.742138   \n",
       "1          0.828068         0.800665  ...                 0.742138   \n",
       "2          0.829036         0.800740  ...                 0.740042   \n",
       "3          0.829036         0.800740  ...                 0.740042   \n",
       "4          0.834768         0.801243  ...                 0.708595   \n",
       "5          0.834768         0.801243  ...                 0.708595   \n",
       "6          0.846531         0.801359  ...                 0.645702   \n",
       "7          0.846531         0.801359  ...                 0.645702   \n",
       "8          0.851085         0.805555  ...                 0.574423   \n",
       "9          0.851085         0.805555  ...                 0.574423   \n",
       "10         0.845140         0.815510  ...                 0.572327   \n",
       "11         0.845140         0.815510  ...                 0.572327   \n",
       "\n",
       "    mean_test_Sensitivity  std_test_Sensitivity  rank_test_Sensitivity  \\\n",
       "0                0.592226              0.115727                      1   \n",
       "1                0.592226              0.115727                      1   \n",
       "2                0.591701              0.115393                      3   \n",
       "3                0.591701              0.115393                      3   \n",
       "4                0.581745              0.106326                      5   \n",
       "5                0.581745              0.106326                      5   \n",
       "6                0.521474              0.095190                      7   \n",
       "7                0.521474              0.095190                      7   \n",
       "8                0.438139              0.102468                      9   \n",
       "9                0.438139              0.102468                      9   \n",
       "10               0.397778              0.131903                     11   \n",
       "11               0.397778              0.131903                     11   \n",
       "\n",
       "    split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                   0.644305                  0.591195   \n",
       "1                   0.644305                  0.591195   \n",
       "2                   0.642907                  0.589797   \n",
       "3                   0.642907                  0.589797   \n",
       "4                   0.635220                  0.580014   \n",
       "5                   0.635220                  0.580014   \n",
       "6                   0.558351                  0.531796   \n",
       "7                   0.558351                  0.531796   \n",
       "8                   0.433962                  0.436059   \n",
       "9                   0.433962                  0.436059   \n",
       "10                  0.381551                  0.403215   \n",
       "11                  0.381551                  0.403215   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.640112                  0.651992   \n",
       "1                   0.640112                  0.651992   \n",
       "2                   0.638714                  0.650594   \n",
       "3                   0.638714                  0.650594   \n",
       "4                   0.628931                  0.638714   \n",
       "5                   0.628931                  0.638714   \n",
       "6                   0.584906                  0.580713   \n",
       "7                   0.584906                  0.580713   \n",
       "8                   0.461915                  0.463312   \n",
       "9                   0.461915                  0.463312   \n",
       "10                  0.438155                  0.419986   \n",
       "11                  0.438155                  0.419986   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.631901               0.023885  \n",
       "1                 0.631901               0.023885  \n",
       "2                 0.630503               0.023885  \n",
       "3                 0.630503               0.023885  \n",
       "4                 0.620720               0.023762  \n",
       "5                 0.620720               0.023762  \n",
       "6                 0.563941               0.021127  \n",
       "7                 0.563941               0.021127  \n",
       "8                 0.448812               0.013830  \n",
       "9                 0.448812               0.013830  \n",
       "10                0.410727               0.020891  \n",
       "11                0.410727               0.020891  \n",
       "\n",
       "[12 rows x 46 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_qda= gs.cv_results_\n",
    "data = pandas.DataFrame(results_qda)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.094181</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820525</td>\n",
       "      <td>0.504882</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845410</td>\n",
       "      <td>0.541456</td>\n",
       "      <td>0.631901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074236</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820525</td>\n",
       "      <td>0.504882</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845410</td>\n",
       "      <td>0.541456</td>\n",
       "      <td>0.631901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108742</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.505032</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.541363</td>\n",
       "      <td>0.630503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117974</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.505032</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.541363</td>\n",
       "      <td>0.630503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158316</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>0.505637</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.542564</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.143254</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>0.505637</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.542564</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097351</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.846881</td>\n",
       "      <td>0.537770</td>\n",
       "      <td>0.563941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092275</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.846881</td>\n",
       "      <td>0.537770</td>\n",
       "      <td>0.563941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.097658</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.834557</td>\n",
       "      <td>0.463247</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.847155</td>\n",
       "      <td>0.488946</td>\n",
       "      <td>0.448812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.098202</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.834557</td>\n",
       "      <td>0.463247</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.847155</td>\n",
       "      <td>0.488946</td>\n",
       "      <td>0.448812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.103969</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.434559</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.470089</td>\n",
       "      <td>0.410727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.093759</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.434559</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.470089</td>\n",
       "      <td>0.410727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        0.094181  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "1        0.074236  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "2        0.108742  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "3        0.117974  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "4        0.158316  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "5        0.143254  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "6        0.097351  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "7        0.092275  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "8        0.097658  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "9        0.098202  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "10       0.103969  {'quadraticdiscriminantanalysis__reg_param': 1...   \n",
       "11       0.093759  {'quadraticdiscriminantanalysis__reg_param': 1...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.820525           0.504882               0.592226             11   \n",
       "1        0.820525           0.504882               0.592226             11   \n",
       "2        0.820785           0.505032               0.591701              9   \n",
       "3        0.820785           0.505032               0.591701              9   \n",
       "4        0.822152           0.505637               0.581745              7   \n",
       "5        0.822152           0.505637               0.581745              7   \n",
       "6        0.824929           0.494459               0.521474              5   \n",
       "7        0.824929           0.494459               0.521474              5   \n",
       "8        0.834557           0.463247               0.438139              3   \n",
       "9        0.834557           0.463247               0.438139              3   \n",
       "10       0.851237           0.434559               0.397778              1   \n",
       "11       0.851237           0.434559               0.397778              1   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   5                      1        0.845410   \n",
       "1                   5                      1        0.845410   \n",
       "2                   3                      3        0.845649   \n",
       "3                   3                      3        0.845649   \n",
       "4                   1                      5        0.846864   \n",
       "5                   1                      5        0.846864   \n",
       "6                   7                      7        0.846881   \n",
       "7                   7                      7        0.846881   \n",
       "8                   9                      9        0.847155   \n",
       "9                   9                      9        0.847155   \n",
       "10                 11                     11        0.855633   \n",
       "11                 11                     11        0.855633   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.541456                0.631901  \n",
       "1             0.541456                0.631901  \n",
       "2             0.541363                0.630503  \n",
       "3             0.541363                0.630503  \n",
       "4             0.542564                0.620720  \n",
       "5             0.542564                0.620720  \n",
       "6             0.537770                0.563941  \n",
       "7             0.537770                0.563941  \n",
       "8             0.488946                0.448812  \n",
       "9             0.488946                0.448812  \n",
       "10            0.470089                0.410727  \n",
       "11            0.470089                0.410727  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_qda = make_table(data)\n",
    "table_qda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quadraticdiscriminantanalysis__reg_param': 0.01,\n",
       " 'quadraticdiscriminantanalysis__store_covariance': True}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Bayesian generative classification based on KDE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bandwidth : float\n",
    "        the kernel bandwidth within each class\n",
    "    kernel : str\n",
    "        the kernel name, passed to KernelDensity\n",
    "    algorithm : str\n",
    "        the algorithm name, passed to KernelDensity\n",
    "    \"\"\"\n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian', algorithm = 'kd_tree'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        training_sets = [X[y == yi] for yi in self.classes_]\n",
    "        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "                                      kernel=self.kernel, \n",
    "                                      algorithm=self.algorithm).fit(Xi)\n",
    "                        for Xi in training_sets]\n",
    "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
    "                           for Xi in training_sets]\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        logprobs = np.array([model.score_samples(X)\n",
    "                             for model in self.models_]).T\n",
    "        result = np.exp(logprobs + self.logpriors_)\n",
    "        return result / result.sum(1, keepdims=True)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.29154967,  1.66810054,  2.15443469,  2.7825594 ,\n",
       "        3.59381366,  4.64158883,  5.9948425 ,  7.74263683, 10.        ])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 10 ** np.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed: 22.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 22.2min finished\n"
     ]
    }
   ],
   "source": [
    "bandwidths = 10 ** np.linspace(0, 1, 10)\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)}\n",
    "parameters = {'kdeclassifier__bandwidth': bandwidths}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), KDEClassifier()) \n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kdeclassifier__bandwidth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>split3_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123743</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>31.727459</td>\n",
       "      <td>0.226179</td>\n",
       "      <td>1</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.0}</td>\n",
       "      <td>0.894678</td>\n",
       "      <td>0.830510</td>\n",
       "      <td>0.790376</td>\n",
       "      <td>0.774951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274633</td>\n",
       "      <td>0.223790</td>\n",
       "      <td>0.031737</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366876</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.441649</td>\n",
       "      <td>0.414046</td>\n",
       "      <td>0.030487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123992</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>34.466054</td>\n",
       "      <td>0.213743</td>\n",
       "      <td>1.29155</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.2915496650148839}</td>\n",
       "      <td>0.891654</td>\n",
       "      <td>0.831096</td>\n",
       "      <td>0.801189</td>\n",
       "      <td>0.795670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174004</td>\n",
       "      <td>0.145176</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>2</td>\n",
       "      <td>0.212439</td>\n",
       "      <td>0.220126</td>\n",
       "      <td>0.259958</td>\n",
       "      <td>0.248777</td>\n",
       "      <td>0.235325</td>\n",
       "      <td>0.019638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131473</td>\n",
       "      <td>0.017763</td>\n",
       "      <td>37.283573</td>\n",
       "      <td>0.258985</td>\n",
       "      <td>1.6681</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.6681005372000588}</td>\n",
       "      <td>0.893810</td>\n",
       "      <td>0.833773</td>\n",
       "      <td>0.801525</td>\n",
       "      <td>0.805612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.075995</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103424</td>\n",
       "      <td>0.109713</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.124389</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.011387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114167</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>32.555808</td>\n",
       "      <td>0.126987</td>\n",
       "      <td>2.15443</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.154434690031884}</td>\n",
       "      <td>0.901172</td>\n",
       "      <td>0.840102</td>\n",
       "      <td>0.805615</td>\n",
       "      <td>0.811767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>0.034590</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040531</td>\n",
       "      <td>0.042628</td>\n",
       "      <td>0.053809</td>\n",
       "      <td>0.049616</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>0.005331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126461</td>\n",
       "      <td>0.019461</td>\n",
       "      <td>31.802794</td>\n",
       "      <td>0.207150</td>\n",
       "      <td>2.78256</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.7825594022071245}</td>\n",
       "      <td>0.907466</td>\n",
       "      <td>0.847083</td>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.818250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.020790</td>\n",
       "      <td>0.002489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.125399</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>32.305125</td>\n",
       "      <td>0.182488</td>\n",
       "      <td>3.59381</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 3.5938136638046276}</td>\n",
       "      <td>0.912042</td>\n",
       "      <td>0.849152</td>\n",
       "      <td>0.813108</td>\n",
       "      <td>0.821159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.121087</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>31.558014</td>\n",
       "      <td>0.112034</td>\n",
       "      <td>4.64159</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 4.641588833612778}</td>\n",
       "      <td>0.914257</td>\n",
       "      <td>0.848604</td>\n",
       "      <td>0.813908</td>\n",
       "      <td>0.821646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.117178</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>31.163359</td>\n",
       "      <td>0.218295</td>\n",
       "      <td>5.99484</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 5.994842503189409}</td>\n",
       "      <td>0.915414</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.814584</td>\n",
       "      <td>0.822821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.109367</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>31.348517</td>\n",
       "      <td>0.172236</td>\n",
       "      <td>7.74264</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 7.742636826811269}</td>\n",
       "      <td>0.916126</td>\n",
       "      <td>0.846189</td>\n",
       "      <td>0.814964</td>\n",
       "      <td>0.823610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.115231</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>31.293514</td>\n",
       "      <td>0.130393</td>\n",
       "      <td>10</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 10.0}</td>\n",
       "      <td>0.916743</td>\n",
       "      <td>0.845432</td>\n",
       "      <td>0.815206</td>\n",
       "      <td>0.824008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.123743      0.008164        31.727459        0.226179   \n",
       "1       0.123992      0.004529        34.466054        0.213743   \n",
       "2       0.131473      0.017763        37.283573        0.258985   \n",
       "3       0.114167      0.012134        32.555808        0.126987   \n",
       "4       0.126461      0.019461        31.802794        0.207150   \n",
       "5       0.125399      0.010837        32.305125        0.182488   \n",
       "6       0.121087      0.012961        31.558014        0.112034   \n",
       "7       0.117178      0.013531        31.163359        0.218295   \n",
       "8       0.109367      0.000006        31.348517        0.172236   \n",
       "9       0.115231      0.004605        31.293514        0.130393   \n",
       "\n",
       "  param_kdeclassifier__bandwidth  \\\n",
       "0                              1   \n",
       "1                        1.29155   \n",
       "2                         1.6681   \n",
       "3                        2.15443   \n",
       "4                        2.78256   \n",
       "5                        3.59381   \n",
       "6                        4.64159   \n",
       "7                        5.99484   \n",
       "8                        7.74264   \n",
       "9                             10   \n",
       "\n",
       "                                             params  split0_test_AUC  \\\n",
       "0                 {'kdeclassifier__bandwidth': 1.0}         0.894678   \n",
       "1  {'kdeclassifier__bandwidth': 1.2915496650148839}         0.891654   \n",
       "2  {'kdeclassifier__bandwidth': 1.6681005372000588}         0.893810   \n",
       "3   {'kdeclassifier__bandwidth': 2.154434690031884}         0.901172   \n",
       "4  {'kdeclassifier__bandwidth': 2.7825594022071245}         0.907466   \n",
       "5  {'kdeclassifier__bandwidth': 3.5938136638046276}         0.912042   \n",
       "6   {'kdeclassifier__bandwidth': 4.641588833612778}         0.914257   \n",
       "7   {'kdeclassifier__bandwidth': 5.994842503189409}         0.915414   \n",
       "8   {'kdeclassifier__bandwidth': 7.742636826811269}         0.916126   \n",
       "9                {'kdeclassifier__bandwidth': 10.0}         0.916743   \n",
       "\n",
       "   split1_test_AUC  split2_test_AUC  split3_test_AUC  ...  \\\n",
       "0         0.830510         0.790376         0.774951  ...   \n",
       "1         0.831096         0.801189         0.795670  ...   \n",
       "2         0.833773         0.801525         0.805612  ...   \n",
       "3         0.840102         0.805615         0.811767  ...   \n",
       "4         0.847083         0.810753         0.818250  ...   \n",
       "5         0.849152         0.813108         0.821159  ...   \n",
       "6         0.848604         0.813908         0.821646  ...   \n",
       "7         0.847477         0.814584         0.822821  ...   \n",
       "8         0.846189         0.814964         0.823610  ...   \n",
       "9         0.845432         0.815206         0.824008  ...   \n",
       "\n",
       "   split3_test_Sensitivity  mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                 0.274633               0.223790              0.031737   \n",
       "1                 0.174004               0.145176              0.017563   \n",
       "2                 0.092243               0.075995              0.009419   \n",
       "3                 0.046122               0.034590              0.008956   \n",
       "4                 0.020964               0.016247              0.004775   \n",
       "5                 0.008386               0.007337              0.003779   \n",
       "6                 0.002096               0.002096              0.002568   \n",
       "7                 0.000000               0.000000              0.000000   \n",
       "8                 0.000000               0.000000              0.000000   \n",
       "9                 0.000000               0.000000              0.000000   \n",
       "\n",
       "   rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                      1                  0.366876                  0.407407   \n",
       "1                      2                  0.212439                  0.220126   \n",
       "2                      3                  0.103424                  0.109713   \n",
       "3                      4                  0.040531                  0.042628   \n",
       "4                      5                  0.020266                  0.020964   \n",
       "5                      6                  0.006289                  0.007687   \n",
       "6                      7                  0.001398                  0.001398   \n",
       "7                      8                  0.000000                  0.000000   \n",
       "8                      8                  0.000000                  0.000000   \n",
       "9                      8                  0.000000                  0.000000   \n",
       "\n",
       "   split2_train_Sensitivity  split3_train_Sensitivity  mean_train_Sensitivity  \\\n",
       "0                  0.440252                  0.441649                0.414046   \n",
       "1                  0.259958                  0.248777                0.235325   \n",
       "2                  0.132075                  0.124389                0.117400   \n",
       "3                  0.053809                  0.049616                0.046646   \n",
       "4                  0.024458                  0.017470                0.020790   \n",
       "5                  0.008386                  0.007687                0.007512   \n",
       "6                  0.000699                  0.001398                0.001223   \n",
       "7                  0.000000                  0.000000                0.000000   \n",
       "8                  0.000000                  0.000000                0.000000   \n",
       "9                  0.000000                  0.000000                0.000000   \n",
       "\n",
       "   std_train_Sensitivity  \n",
       "0               0.030487  \n",
       "1               0.019638  \n",
       "2               0.011387  \n",
       "3               0.005331  \n",
       "4               0.002489  \n",
       "5               0.000762  \n",
       "6               0.000303  \n",
       "7               0.000000  \n",
       "8               0.000000  \n",
       "9               0.000000  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_kd= gs.cv_results_\n",
    "data = pandas.DataFrame(results_kd)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123743</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.0}</td>\n",
       "      <td>0.822635</td>\n",
       "      <td>0.333549</td>\n",
       "      <td>0.223790</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943239</td>\n",
       "      <td>0.578753</td>\n",
       "      <td>0.414046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123992</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.2915496650148839}</td>\n",
       "      <td>0.829907</td>\n",
       "      <td>0.240523</td>\n",
       "      <td>0.145176</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.906338</td>\n",
       "      <td>0.376716</td>\n",
       "      <td>0.235325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131473</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.6681005372000588}</td>\n",
       "      <td>0.833685</td>\n",
       "      <td>0.138044</td>\n",
       "      <td>0.075995</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.878227</td>\n",
       "      <td>0.209060</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114167</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.154434690031884}</td>\n",
       "      <td>0.839669</td>\n",
       "      <td>0.065942</td>\n",
       "      <td>0.034590</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.864297</td>\n",
       "      <td>0.088813</td>\n",
       "      <td>0.046646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126461</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.7825594022071245}</td>\n",
       "      <td>0.845893</td>\n",
       "      <td>0.031702</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859112</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>0.020790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.125399</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 3.5938136638046276}</td>\n",
       "      <td>0.848870</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.856430</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.007512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.121087</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 4.641588833612778}</td>\n",
       "      <td>0.849609</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855848</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.001223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.117178</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 5.994842503189409}</td>\n",
       "      <td>0.850079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.855598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.109367</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 7.742636826811269}</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.855355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.115231</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 10.0}</td>\n",
       "      <td>0.850353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.855183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                            params  \\\n",
       "0       0.123743                 {'kdeclassifier__bandwidth': 1.0}   \n",
       "1       0.123992  {'kdeclassifier__bandwidth': 1.2915496650148839}   \n",
       "2       0.131473  {'kdeclassifier__bandwidth': 1.6681005372000588}   \n",
       "3       0.114167   {'kdeclassifier__bandwidth': 2.154434690031884}   \n",
       "4       0.126461  {'kdeclassifier__bandwidth': 2.7825594022071245}   \n",
       "5       0.125399  {'kdeclassifier__bandwidth': 3.5938136638046276}   \n",
       "6       0.121087   {'kdeclassifier__bandwidth': 4.641588833612778}   \n",
       "7       0.117178   {'kdeclassifier__bandwidth': 5.994842503189409}   \n",
       "8       0.109367   {'kdeclassifier__bandwidth': 7.742636826811269}   \n",
       "9       0.115231                {'kdeclassifier__bandwidth': 10.0}   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0       0.822635           0.333549               0.223790             10   \n",
       "1       0.829907           0.240523               0.145176              9   \n",
       "2       0.833685           0.138044               0.075995              8   \n",
       "3       0.839669           0.065942               0.034590              7   \n",
       "4       0.845893           0.031702               0.016247              6   \n",
       "5       0.848870           0.014461               0.007337              5   \n",
       "6       0.849609           0.004157               0.002096              4   \n",
       "7       0.850079           0.000000               0.000000              3   \n",
       "8       0.850227           0.000000               0.000000              2   \n",
       "9       0.850353           0.000000               0.000000              1   \n",
       "\n",
       "   rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  1                      1        0.943239   \n",
       "1                  2                      2        0.906338   \n",
       "2                  3                      3        0.878227   \n",
       "3                  4                      4        0.864297   \n",
       "4                  5                      5        0.859112   \n",
       "5                  6                      6        0.856430   \n",
       "6                  7                      7        0.855848   \n",
       "7                  8                      8        0.855598   \n",
       "8                  8                      8        0.855355   \n",
       "9                  8                      8        0.855183   \n",
       "\n",
       "   mean_train_F_score  mean_train_Sensitivity  \n",
       "0            0.578753                0.414046  \n",
       "1            0.376716                0.235325  \n",
       "2            0.209060                0.117400  \n",
       "3            0.088813                0.046646  \n",
       "4            0.040661                0.020790  \n",
       "5            0.014896                0.007512  \n",
       "6            0.002443                0.001223  \n",
       "7            0.000000                0.000000  \n",
       "8            0.000000                0.000000  \n",
       "9            0.000000                0.000000  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_kd = make_table(data)\n",
    "table_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-df2c2bd0f67e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed: 20.3min finished\n",
      "C:\\Users\\Karol\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [20, 30, 40],\n",
    "    'mlpclassifier__max_iter': [250],\n",
    "    'mlpclassifier__activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'mlpclassifier__solver': ('sgd', 'adam'),\n",
    "    'mlpclassifier__alpha': (0.001, 0.01, 0.1),\n",
    "    'mlpclassifier__learning_rate': ['adaptive'], \n",
    "    'mlpclassifier__random_state':[0]\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), MLPClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='AUC', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_mlpclassifier__activation</th>\n",
       "      <th>param_mlpclassifier__alpha</th>\n",
       "      <th>param_mlpclassifier__hidden_layer_sizes</th>\n",
       "      <th>param_mlpclassifier__learning_rate</th>\n",
       "      <th>param_mlpclassifier__max_iter</th>\n",
       "      <th>param_mlpclassifier__random_state</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.775499</td>\n",
       "      <td>0.222299</td>\n",
       "      <td>0.047252</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>66</td>\n",
       "      <td>0.310273</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>0.414396</td>\n",
       "      <td>0.376660</td>\n",
       "      <td>0.045452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.694757</td>\n",
       "      <td>0.123489</td>\n",
       "      <td>0.047745</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>59</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.361286</td>\n",
       "      <td>0.430468</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.383124</td>\n",
       "      <td>0.044629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.782754</td>\n",
       "      <td>0.446546</td>\n",
       "      <td>0.064001</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>69</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.415793</td>\n",
       "      <td>0.375087</td>\n",
       "      <td>0.043715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856503</td>\n",
       "      <td>0.087326</td>\n",
       "      <td>0.061756</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>0.052788</td>\n",
       "      <td>54</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.422082</td>\n",
       "      <td>0.377184</td>\n",
       "      <td>0.047741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.251008</td>\n",
       "      <td>0.229054</td>\n",
       "      <td>0.061996</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>62</td>\n",
       "      <td>0.311670</td>\n",
       "      <td>0.360587</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.412998</td>\n",
       "      <td>0.376136</td>\n",
       "      <td>0.043642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.101501</td>\n",
       "      <td>0.385897</td>\n",
       "      <td>0.075764</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>0.049553</td>\n",
       "      <td>57</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.356394</td>\n",
       "      <td>0.426275</td>\n",
       "      <td>0.412299</td>\n",
       "      <td>0.375611</td>\n",
       "      <td>0.047234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.861001</td>\n",
       "      <td>0.176050</td>\n",
       "      <td>0.063767</td>\n",
       "      <td>0.010816</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>66</td>\n",
       "      <td>0.310273</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>0.413697</td>\n",
       "      <td>0.376485</td>\n",
       "      <td>0.045308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.975233</td>\n",
       "      <td>0.134533</td>\n",
       "      <td>0.048255</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.363201</td>\n",
       "      <td>0.046226</td>\n",
       "      <td>60</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.361286</td>\n",
       "      <td>0.430468</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.383124</td>\n",
       "      <td>0.044629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.924519</td>\n",
       "      <td>0.413967</td>\n",
       "      <td>0.071993</td>\n",
       "      <td>0.015847</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>69</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.415793</td>\n",
       "      <td>0.375087</td>\n",
       "      <td>0.043715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.564246</td>\n",
       "      <td>0.133171</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>0.052788</td>\n",
       "      <td>54</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.422082</td>\n",
       "      <td>0.377184</td>\n",
       "      <td>0.047741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.837503</td>\n",
       "      <td>0.084054</td>\n",
       "      <td>0.071249</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>62</td>\n",
       "      <td>0.311670</td>\n",
       "      <td>0.360587</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>0.412998</td>\n",
       "      <td>0.375961</td>\n",
       "      <td>0.043470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.969244</td>\n",
       "      <td>0.273480</td>\n",
       "      <td>0.069498</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>0.049553</td>\n",
       "      <td>57</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.356394</td>\n",
       "      <td>0.426275</td>\n",
       "      <td>0.412299</td>\n",
       "      <td>0.375611</td>\n",
       "      <td>0.047234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.398499</td>\n",
       "      <td>0.184817</td>\n",
       "      <td>0.065744</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.357960</td>\n",
       "      <td>0.047515</td>\n",
       "      <td>68</td>\n",
       "      <td>0.308875</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.412299</td>\n",
       "      <td>0.375087</td>\n",
       "      <td>0.045065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.132252</td>\n",
       "      <td>1.405434</td>\n",
       "      <td>0.064004</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402516</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>0.045968</td>\n",
       "      <td>72</td>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>0.406709</td>\n",
       "      <td>0.375786</td>\n",
       "      <td>0.042428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.191003</td>\n",
       "      <td>0.613985</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.355864</td>\n",
       "      <td>0.046605</td>\n",
       "      <td>71</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.357792</td>\n",
       "      <td>0.414396</td>\n",
       "      <td>0.414396</td>\n",
       "      <td>0.374389</td>\n",
       "      <td>0.043296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.965756</td>\n",
       "      <td>1.215305</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>0.046012</td>\n",
       "      <td>56</td>\n",
       "      <td>0.315863</td>\n",
       "      <td>0.352201</td>\n",
       "      <td>0.443047</td>\n",
       "      <td>0.410203</td>\n",
       "      <td>0.380328</td>\n",
       "      <td>0.049429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.888509</td>\n",
       "      <td>0.429072</td>\n",
       "      <td>0.085996</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>62</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.360587</td>\n",
       "      <td>0.417890</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.375262</td>\n",
       "      <td>0.043261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.239740</td>\n",
       "      <td>1.764236</td>\n",
       "      <td>0.081007</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425577</td>\n",
       "      <td>0.360057</td>\n",
       "      <td>0.047930</td>\n",
       "      <td>65</td>\n",
       "      <td>0.321454</td>\n",
       "      <td>0.354997</td>\n",
       "      <td>0.417191</td>\n",
       "      <td>0.421384</td>\n",
       "      <td>0.378756</td>\n",
       "      <td>0.042256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.984505</td>\n",
       "      <td>0.274071</td>\n",
       "      <td>0.056747</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>0.047167</td>\n",
       "      <td>46</td>\n",
       "      <td>0.340321</td>\n",
       "      <td>0.395528</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>0.456324</td>\n",
       "      <td>0.413173</td>\n",
       "      <td>0.049301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.233255</td>\n",
       "      <td>0.537467</td>\n",
       "      <td>0.054246</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.491612</td>\n",
       "      <td>0.073686</td>\n",
       "      <td>13</td>\n",
       "      <td>0.501048</td>\n",
       "      <td>0.570929</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.579839</td>\n",
       "      <td>0.051885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.456001</td>\n",
       "      <td>0.431638</td>\n",
       "      <td>0.062255</td>\n",
       "      <td>0.011095</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.385213</td>\n",
       "      <td>0.049995</td>\n",
       "      <td>48</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>0.393431</td>\n",
       "      <td>0.453529</td>\n",
       "      <td>0.442348</td>\n",
       "      <td>0.404437</td>\n",
       "      <td>0.049355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.227499</td>\n",
       "      <td>0.372447</td>\n",
       "      <td>0.058501</td>\n",
       "      <td>0.004384</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.084813</td>\n",
       "      <td>6</td>\n",
       "      <td>0.502446</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.621244</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.588924</td>\n",
       "      <td>0.055211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20.316752</td>\n",
       "      <td>1.935356</td>\n",
       "      <td>0.086254</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.369490</td>\n",
       "      <td>0.049620</td>\n",
       "      <td>52</td>\n",
       "      <td>0.316562</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.432565</td>\n",
       "      <td>0.431866</td>\n",
       "      <td>0.389762</td>\n",
       "      <td>0.047697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.478493</td>\n",
       "      <td>1.871172</td>\n",
       "      <td>0.094001</td>\n",
       "      <td>0.034710</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.506809</td>\n",
       "      <td>0.069997</td>\n",
       "      <td>3</td>\n",
       "      <td>0.505940</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.593117</td>\n",
       "      <td>0.055995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.342246</td>\n",
       "      <td>2.135672</td>\n",
       "      <td>0.057502</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.389930</td>\n",
       "      <td>0.046241</td>\n",
       "      <td>47</td>\n",
       "      <td>0.340321</td>\n",
       "      <td>0.393431</td>\n",
       "      <td>0.455625</td>\n",
       "      <td>0.456324</td>\n",
       "      <td>0.411426</td>\n",
       "      <td>0.048345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.477503</td>\n",
       "      <td>0.972069</td>\n",
       "      <td>0.070746</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.487417</td>\n",
       "      <td>0.069858</td>\n",
       "      <td>19</td>\n",
       "      <td>0.493361</td>\n",
       "      <td>0.563242</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.633823</td>\n",
       "      <td>0.573375</td>\n",
       "      <td>0.052538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17.921009</td>\n",
       "      <td>1.186882</td>\n",
       "      <td>0.057752</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.384689</td>\n",
       "      <td>0.049445</td>\n",
       "      <td>49</td>\n",
       "      <td>0.327743</td>\n",
       "      <td>0.392732</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.442348</td>\n",
       "      <td>0.403913</td>\n",
       "      <td>0.049490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.937748</td>\n",
       "      <td>0.400661</td>\n",
       "      <td>0.058747</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.496326</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>10</td>\n",
       "      <td>0.491265</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.610063</td>\n",
       "      <td>0.630328</td>\n",
       "      <td>0.575996</td>\n",
       "      <td>0.053163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.978260</td>\n",
       "      <td>1.704479</td>\n",
       "      <td>0.069743</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>0.049412</td>\n",
       "      <td>53</td>\n",
       "      <td>0.316562</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.431866</td>\n",
       "      <td>0.429071</td>\n",
       "      <td>0.388714</td>\n",
       "      <td>0.046972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.667002</td>\n",
       "      <td>0.400816</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.502616</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>4</td>\n",
       "      <td>0.498952</td>\n",
       "      <td>0.573725</td>\n",
       "      <td>0.625437</td>\n",
       "      <td>0.635919</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.823998</td>\n",
       "      <td>0.305129</td>\n",
       "      <td>0.047753</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>23</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.512928</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.519567</td>\n",
       "      <td>0.052781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>18.064747</td>\n",
       "      <td>0.987442</td>\n",
       "      <td>0.074006</td>\n",
       "      <td>0.025545</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.496325</td>\n",
       "      <td>0.050061</td>\n",
       "      <td>11</td>\n",
       "      <td>0.530398</td>\n",
       "      <td>0.589099</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>0.667365</td>\n",
       "      <td>0.614430</td>\n",
       "      <td>0.058501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20.441744</td>\n",
       "      <td>0.859471</td>\n",
       "      <td>0.058257</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.469598</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>35</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.484976</td>\n",
       "      <td>0.526205</td>\n",
       "      <td>0.545772</td>\n",
       "      <td>0.493012</td>\n",
       "      <td>0.050053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.879757</td>\n",
       "      <td>1.127441</td>\n",
       "      <td>0.063248</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526205</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>0.048503</td>\n",
       "      <td>31</td>\n",
       "      <td>0.560447</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.633997</td>\n",
       "      <td>0.047850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>24.770258</td>\n",
       "      <td>1.138280</td>\n",
       "      <td>0.070745</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.460164</td>\n",
       "      <td>0.041042</td>\n",
       "      <td>41</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.482879</td>\n",
       "      <td>0.524109</td>\n",
       "      <td>0.530398</td>\n",
       "      <td>0.487247</td>\n",
       "      <td>0.047335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>26.123751</td>\n",
       "      <td>2.587408</td>\n",
       "      <td>0.064745</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.491079</td>\n",
       "      <td>0.075942</td>\n",
       "      <td>15</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.613557</td>\n",
       "      <td>0.725367</td>\n",
       "      <td>0.709993</td>\n",
       "      <td>0.651468</td>\n",
       "      <td>0.069384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.667006</td>\n",
       "      <td>0.450582</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.479555</td>\n",
       "      <td>0.039982</td>\n",
       "      <td>28</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.516771</td>\n",
       "      <td>0.054061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15.789512</td>\n",
       "      <td>1.152794</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.487938</td>\n",
       "      <td>0.066087</td>\n",
       "      <td>18</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.636618</td>\n",
       "      <td>0.653389</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.055496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19.630752</td>\n",
       "      <td>0.814912</td>\n",
       "      <td>0.056755</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.461735</td>\n",
       "      <td>0.046794</td>\n",
       "      <td>38</td>\n",
       "      <td>0.403913</td>\n",
       "      <td>0.482879</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.540182</td>\n",
       "      <td>0.487247</td>\n",
       "      <td>0.052379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>19.751008</td>\n",
       "      <td>0.129881</td>\n",
       "      <td>0.063997</td>\n",
       "      <td>0.014076</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532495</td>\n",
       "      <td>0.487942</td>\n",
       "      <td>0.044901</td>\n",
       "      <td>17</td>\n",
       "      <td>0.535290</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.636618</td>\n",
       "      <td>0.599755</td>\n",
       "      <td>0.042737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20.885504</td>\n",
       "      <td>0.414476</td>\n",
       "      <td>0.064501</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.459115</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>42</td>\n",
       "      <td>0.404612</td>\n",
       "      <td>0.480084</td>\n",
       "      <td>0.520615</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>0.482704</td>\n",
       "      <td>0.048411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.926750</td>\n",
       "      <td>0.421111</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.016843</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.498944</td>\n",
       "      <td>0.059279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.524109</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.682739</td>\n",
       "      <td>0.675751</td>\n",
       "      <td>0.614605</td>\n",
       "      <td>0.067221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15.441752</td>\n",
       "      <td>0.355514</td>\n",
       "      <td>0.054748</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>44</td>\n",
       "      <td>0.427673</td>\n",
       "      <td>0.473795</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.494584</td>\n",
       "      <td>0.049793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>19.335506</td>\n",
       "      <td>0.384764</td>\n",
       "      <td>0.052495</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551363</td>\n",
       "      <td>0.482175</td>\n",
       "      <td>0.057986</td>\n",
       "      <td>26</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.669462</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.616702</td>\n",
       "      <td>0.056713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20.795001</td>\n",
       "      <td>0.801262</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.051083</td>\n",
       "      <td>36</td>\n",
       "      <td>0.450035</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.546471</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.520615</td>\n",
       "      <td>0.045752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>21.490246</td>\n",
       "      <td>0.542226</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.493706</td>\n",
       "      <td>0.055677</td>\n",
       "      <td>12</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.617051</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.628232</td>\n",
       "      <td>0.046425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>21.902505</td>\n",
       "      <td>0.395145</td>\n",
       "      <td>0.078490</td>\n",
       "      <td>0.011068</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542977</td>\n",
       "      <td>0.478507</td>\n",
       "      <td>0.058126</td>\n",
       "      <td>29</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.516422</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.529525</td>\n",
       "      <td>0.041810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>24.099248</td>\n",
       "      <td>0.507706</td>\n",
       "      <td>0.071501</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.514668</td>\n",
       "      <td>0.057195</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573026</td>\n",
       "      <td>0.633823</td>\n",
       "      <td>0.737945</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.660028</td>\n",
       "      <td>0.062395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>16.362752</td>\n",
       "      <td>0.324775</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>44</td>\n",
       "      <td>0.429071</td>\n",
       "      <td>0.473096</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.494584</td>\n",
       "      <td>0.049333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>18.254003</td>\n",
       "      <td>0.529828</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551363</td>\n",
       "      <td>0.484795</td>\n",
       "      <td>0.057575</td>\n",
       "      <td>22</td>\n",
       "      <td>0.542278</td>\n",
       "      <td>0.587701</td>\n",
       "      <td>0.668763</td>\n",
       "      <td>0.672956</td>\n",
       "      <td>0.617925</td>\n",
       "      <td>0.055337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>18.654258</td>\n",
       "      <td>0.472881</td>\n",
       "      <td>0.072999</td>\n",
       "      <td>0.020114</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.051083</td>\n",
       "      <td>36</td>\n",
       "      <td>0.450035</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.520790</td>\n",
       "      <td>0.045852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>21.592253</td>\n",
       "      <td>0.304740</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.491085</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>14</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.665269</td>\n",
       "      <td>0.624214</td>\n",
       "      <td>0.050187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>21.988753</td>\n",
       "      <td>0.644842</td>\n",
       "      <td>0.074999</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.477459</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>30</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.557652</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.529175</td>\n",
       "      <td>0.041746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>24.327004</td>\n",
       "      <td>0.705039</td>\n",
       "      <td>0.068753</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.512047</td>\n",
       "      <td>0.061969</td>\n",
       "      <td>2</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.619846</td>\n",
       "      <td>0.730259</td>\n",
       "      <td>0.693920</td>\n",
       "      <td>0.653040</td>\n",
       "      <td>0.063136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>16.787014</td>\n",
       "      <td>0.252831</td>\n",
       "      <td>0.055991</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>0.063748</td>\n",
       "      <td>43</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.492837</td>\n",
       "      <td>0.047927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>15.897499</td>\n",
       "      <td>1.557125</td>\n",
       "      <td>0.049254</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.498945</td>\n",
       "      <td>0.067236</td>\n",
       "      <td>7</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.589797</td>\n",
       "      <td>0.682041</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.620196</td>\n",
       "      <td>0.055452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>19.482251</td>\n",
       "      <td>0.452556</td>\n",
       "      <td>0.060503</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.460689</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>39</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.501048</td>\n",
       "      <td>0.546471</td>\n",
       "      <td>0.570929</td>\n",
       "      <td>0.513103</td>\n",
       "      <td>0.052120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20.434508</td>\n",
       "      <td>1.322099</td>\n",
       "      <td>0.050248</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557652</td>\n",
       "      <td>0.484272</td>\n",
       "      <td>0.060271</td>\n",
       "      <td>24</td>\n",
       "      <td>0.519217</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.656883</td>\n",
       "      <td>0.602725</td>\n",
       "      <td>0.055194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>22.482255</td>\n",
       "      <td>0.316870</td>\n",
       "      <td>0.074746</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>0.058634</td>\n",
       "      <td>33</td>\n",
       "      <td>0.455625</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.557652</td>\n",
       "      <td>0.573026</td>\n",
       "      <td>0.523760</td>\n",
       "      <td>0.045945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16.944497</td>\n",
       "      <td>1.239742</td>\n",
       "      <td>0.058999</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.058070</td>\n",
       "      <td>5</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.596087</td>\n",
       "      <td>0.694619</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.625961</td>\n",
       "      <td>0.049982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        4.775499      0.222299         0.047252        0.001922   \n",
       "1        1.694757      0.123489         0.047745        0.005322   \n",
       "2        6.782754      0.446546         0.064001        0.005999   \n",
       "3        1.856503      0.087326         0.061756        0.002273   \n",
       "4        6.251008      0.229054         0.061996        0.012090   \n",
       "5        2.101501      0.385897         0.075764        0.014529   \n",
       "6        6.861001      0.176050         0.063767        0.010816   \n",
       "7        1.975233      0.134533         0.048255        0.004498   \n",
       "8        5.924519      0.413967         0.071993        0.015847   \n",
       "9        2.564246      0.133171         0.075255        0.017327   \n",
       "10       6.837503      0.084054         0.071249        0.003341   \n",
       "11       1.969244      0.273480         0.069498        0.004026   \n",
       "12       6.398499      0.184817         0.065744        0.008046   \n",
       "13       5.132252      1.405434         0.064004        0.019839   \n",
       "14       6.191003      0.613985         0.076500        0.024681   \n",
       "15       4.965756      1.215305         0.067753        0.011171   \n",
       "16       6.888509      0.429072         0.085996        0.014865   \n",
       "17       6.239740      1.764236         0.081007        0.010891   \n",
       "18      14.984505      0.274071         0.056747        0.003266   \n",
       "19      18.233255      0.537467         0.054246        0.005849   \n",
       "20      17.456001      0.431638         0.062255        0.011095   \n",
       "21      19.227499      0.372447         0.058501        0.004384   \n",
       "22      20.316752      1.935356         0.086254        0.024316   \n",
       "23      32.478493      1.871172         0.094001        0.034710   \n",
       "24      17.342246      2.135672         0.057502        0.018819   \n",
       "25      16.477503      0.972069         0.070746        0.030543   \n",
       "26      17.921009      1.186882         0.057752        0.001297   \n",
       "27      18.937748      0.400661         0.058747        0.005532   \n",
       "28      18.978260      1.704479         0.069743        0.005889   \n",
       "29      21.667002      0.400816         0.077498        0.016587   \n",
       "..            ...           ...              ...             ...   \n",
       "42      14.823998      0.305129         0.047753        0.003342   \n",
       "43      18.064747      0.987442         0.074006        0.025545   \n",
       "44      20.441744      0.859471         0.058257        0.010325   \n",
       "45      26.879757      1.127441         0.063248        0.017687   \n",
       "46      24.770258      1.138280         0.070745        0.006867   \n",
       "47      26.123751      2.587408         0.064745        0.007259   \n",
       "48      14.667006      0.450582         0.045999        0.003534   \n",
       "49      15.789512      1.152794         0.047742        0.003958   \n",
       "50      19.630752      0.814912         0.056755        0.009704   \n",
       "51      19.751008      0.129881         0.063997        0.014076   \n",
       "52      20.885504      0.414476         0.064501        0.002685   \n",
       "53      22.926750      0.421111         0.081250        0.016843   \n",
       "54      15.441752      0.355514         0.054748        0.000826   \n",
       "55      19.335506      0.384764         0.052495        0.004930   \n",
       "56      20.795001      0.801262         0.061502        0.002179   \n",
       "57      21.490246      0.542226         0.061502        0.005680   \n",
       "58      21.902505      0.395145         0.078490        0.011068   \n",
       "59      24.099248      0.507706         0.071501        0.011104   \n",
       "60      16.362752      0.324775         0.057250        0.005492   \n",
       "61      18.254003      0.529828         0.049753        0.006222   \n",
       "62      18.654258      0.472881         0.072999        0.020114   \n",
       "63      21.592253      0.304740         0.057250        0.007728   \n",
       "64      21.988753      0.644842         0.074999        0.009137   \n",
       "65      24.327004      0.705039         0.068753        0.009004   \n",
       "66      16.787014      0.252831         0.055991        0.003317   \n",
       "67      15.897499      1.557125         0.049254        0.006903   \n",
       "68      19.482251      0.452556         0.060503        0.003200   \n",
       "69      20.434508      1.322099         0.050248        0.007462   \n",
       "70      22.482255      0.316870         0.074746        0.015660   \n",
       "71      16.944497      1.239742         0.058999        0.024928   \n",
       "\n",
       "   param_mlpclassifier__activation param_mlpclassifier__alpha  \\\n",
       "0                         identity                      0.001   \n",
       "1                         identity                      0.001   \n",
       "2                         identity                      0.001   \n",
       "3                         identity                      0.001   \n",
       "4                         identity                      0.001   \n",
       "5                         identity                      0.001   \n",
       "6                         identity                       0.01   \n",
       "7                         identity                       0.01   \n",
       "8                         identity                       0.01   \n",
       "9                         identity                       0.01   \n",
       "10                        identity                       0.01   \n",
       "11                        identity                       0.01   \n",
       "12                        identity                        0.1   \n",
       "13                        identity                        0.1   \n",
       "14                        identity                        0.1   \n",
       "15                        identity                        0.1   \n",
       "16                        identity                        0.1   \n",
       "17                        identity                        0.1   \n",
       "18                        logistic                      0.001   \n",
       "19                        logistic                      0.001   \n",
       "20                        logistic                      0.001   \n",
       "21                        logistic                      0.001   \n",
       "22                        logistic                      0.001   \n",
       "23                        logistic                      0.001   \n",
       "24                        logistic                       0.01   \n",
       "25                        logistic                       0.01   \n",
       "26                        logistic                       0.01   \n",
       "27                        logistic                       0.01   \n",
       "28                        logistic                       0.01   \n",
       "29                        logistic                       0.01   \n",
       "..                             ...                        ...   \n",
       "42                            tanh                       0.01   \n",
       "43                            tanh                       0.01   \n",
       "44                            tanh                       0.01   \n",
       "45                            tanh                       0.01   \n",
       "46                            tanh                       0.01   \n",
       "47                            tanh                       0.01   \n",
       "48                            tanh                        0.1   \n",
       "49                            tanh                        0.1   \n",
       "50                            tanh                        0.1   \n",
       "51                            tanh                        0.1   \n",
       "52                            tanh                        0.1   \n",
       "53                            tanh                        0.1   \n",
       "54                            relu                      0.001   \n",
       "55                            relu                      0.001   \n",
       "56                            relu                      0.001   \n",
       "57                            relu                      0.001   \n",
       "58                            relu                      0.001   \n",
       "59                            relu                      0.001   \n",
       "60                            relu                       0.01   \n",
       "61                            relu                       0.01   \n",
       "62                            relu                       0.01   \n",
       "63                            relu                       0.01   \n",
       "64                            relu                       0.01   \n",
       "65                            relu                       0.01   \n",
       "66                            relu                        0.1   \n",
       "67                            relu                        0.1   \n",
       "68                            relu                        0.1   \n",
       "69                            relu                        0.1   \n",
       "70                            relu                        0.1   \n",
       "71                            relu                        0.1   \n",
       "\n",
       "   param_mlpclassifier__hidden_layer_sizes param_mlpclassifier__learning_rate  \\\n",
       "0                                       20                           adaptive   \n",
       "1                                       20                           adaptive   \n",
       "2                                       30                           adaptive   \n",
       "3                                       30                           adaptive   \n",
       "4                                       40                           adaptive   \n",
       "5                                       40                           adaptive   \n",
       "6                                       20                           adaptive   \n",
       "7                                       20                           adaptive   \n",
       "8                                       30                           adaptive   \n",
       "9                                       30                           adaptive   \n",
       "10                                      40                           adaptive   \n",
       "11                                      40                           adaptive   \n",
       "12                                      20                           adaptive   \n",
       "13                                      20                           adaptive   \n",
       "14                                      30                           adaptive   \n",
       "15                                      30                           adaptive   \n",
       "16                                      40                           adaptive   \n",
       "17                                      40                           adaptive   \n",
       "18                                      20                           adaptive   \n",
       "19                                      20                           adaptive   \n",
       "20                                      30                           adaptive   \n",
       "21                                      30                           adaptive   \n",
       "22                                      40                           adaptive   \n",
       "23                                      40                           adaptive   \n",
       "24                                      20                           adaptive   \n",
       "25                                      20                           adaptive   \n",
       "26                                      30                           adaptive   \n",
       "27                                      30                           adaptive   \n",
       "28                                      40                           adaptive   \n",
       "29                                      40                           adaptive   \n",
       "..                                     ...                                ...   \n",
       "42                                      20                           adaptive   \n",
       "43                                      20                           adaptive   \n",
       "44                                      30                           adaptive   \n",
       "45                                      30                           adaptive   \n",
       "46                                      40                           adaptive   \n",
       "47                                      40                           adaptive   \n",
       "48                                      20                           adaptive   \n",
       "49                                      20                           adaptive   \n",
       "50                                      30                           adaptive   \n",
       "51                                      30                           adaptive   \n",
       "52                                      40                           adaptive   \n",
       "53                                      40                           adaptive   \n",
       "54                                      20                           adaptive   \n",
       "55                                      20                           adaptive   \n",
       "56                                      30                           adaptive   \n",
       "57                                      30                           adaptive   \n",
       "58                                      40                           adaptive   \n",
       "59                                      40                           adaptive   \n",
       "60                                      20                           adaptive   \n",
       "61                                      20                           adaptive   \n",
       "62                                      30                           adaptive   \n",
       "63                                      30                           adaptive   \n",
       "64                                      40                           adaptive   \n",
       "65                                      40                           adaptive   \n",
       "66                                      20                           adaptive   \n",
       "67                                      20                           adaptive   \n",
       "68                                      30                           adaptive   \n",
       "69                                      30                           adaptive   \n",
       "70                                      40                           adaptive   \n",
       "71                                      40                           adaptive   \n",
       "\n",
       "   param_mlpclassifier__max_iter param_mlpclassifier__random_state  ...  \\\n",
       "0                            250                                 0  ...   \n",
       "1                            250                                 0  ...   \n",
       "2                            250                                 0  ...   \n",
       "3                            250                                 0  ...   \n",
       "4                            250                                 0  ...   \n",
       "5                            250                                 0  ...   \n",
       "6                            250                                 0  ...   \n",
       "7                            250                                 0  ...   \n",
       "8                            250                                 0  ...   \n",
       "9                            250                                 0  ...   \n",
       "10                           250                                 0  ...   \n",
       "11                           250                                 0  ...   \n",
       "12                           250                                 0  ...   \n",
       "13                           250                                 0  ...   \n",
       "14                           250                                 0  ...   \n",
       "15                           250                                 0  ...   \n",
       "16                           250                                 0  ...   \n",
       "17                           250                                 0  ...   \n",
       "18                           250                                 0  ...   \n",
       "19                           250                                 0  ...   \n",
       "20                           250                                 0  ...   \n",
       "21                           250                                 0  ...   \n",
       "22                           250                                 0  ...   \n",
       "23                           250                                 0  ...   \n",
       "24                           250                                 0  ...   \n",
       "25                           250                                 0  ...   \n",
       "26                           250                                 0  ...   \n",
       "27                           250                                 0  ...   \n",
       "28                           250                                 0  ...   \n",
       "29                           250                                 0  ...   \n",
       "..                           ...                               ...  ...   \n",
       "42                           250                                 0  ...   \n",
       "43                           250                                 0  ...   \n",
       "44                           250                                 0  ...   \n",
       "45                           250                                 0  ...   \n",
       "46                           250                                 0  ...   \n",
       "47                           250                                 0  ...   \n",
       "48                           250                                 0  ...   \n",
       "49                           250                                 0  ...   \n",
       "50                           250                                 0  ...   \n",
       "51                           250                                 0  ...   \n",
       "52                           250                                 0  ...   \n",
       "53                           250                                 0  ...   \n",
       "54                           250                                 0  ...   \n",
       "55                           250                                 0  ...   \n",
       "56                           250                                 0  ...   \n",
       "57                           250                                 0  ...   \n",
       "58                           250                                 0  ...   \n",
       "59                           250                                 0  ...   \n",
       "60                           250                                 0  ...   \n",
       "61                           250                                 0  ...   \n",
       "62                           250                                 0  ...   \n",
       "63                           250                                 0  ...   \n",
       "64                           250                                 0  ...   \n",
       "65                           250                                 0  ...   \n",
       "66                           250                                 0  ...   \n",
       "67                           250                                 0  ...   \n",
       "68                           250                                 0  ...   \n",
       "69                           250                                 0  ...   \n",
       "70                           250                                 0  ...   \n",
       "71                           250                                 0  ...   \n",
       "\n",
       "   split3_test_Sensitivity mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                 0.419287              0.359533              0.046843   \n",
       "1                 0.423480              0.363725              0.046419   \n",
       "2                 0.415094              0.356388              0.046596   \n",
       "3                 0.433962              0.366870              0.052788   \n",
       "4                 0.419287              0.360581              0.048537   \n",
       "5                 0.423480              0.364249              0.049553   \n",
       "6                 0.419287              0.359533              0.046843   \n",
       "7                 0.423480              0.363201              0.046226   \n",
       "8                 0.415094              0.356388              0.046596   \n",
       "9                 0.433962              0.366870              0.052788   \n",
       "10                0.419287              0.360581              0.048537   \n",
       "11                0.423480              0.364249              0.049553   \n",
       "12                0.419287              0.357960              0.047515   \n",
       "13                0.402516              0.355340              0.045968   \n",
       "14                0.415094              0.355864              0.046605   \n",
       "15                0.408805              0.365297              0.046012   \n",
       "16                0.419287              0.360581              0.048537   \n",
       "17                0.425577              0.360057              0.047930   \n",
       "18                0.446541              0.391503              0.047167   \n",
       "19                0.545073              0.491612              0.073686   \n",
       "20                0.438155              0.385213              0.049995   \n",
       "21                0.570231              0.498947              0.084813   \n",
       "22                0.429769              0.369490              0.049620   \n",
       "23                0.566038              0.506809              0.069997   \n",
       "24                0.446541              0.389930              0.046241   \n",
       "25                0.538784              0.487417              0.069858   \n",
       "26                0.436059              0.384689              0.049445   \n",
       "27                0.563941              0.496326              0.076708   \n",
       "28                0.429769              0.368966              0.049412   \n",
       "29                0.559748              0.502616              0.066421   \n",
       "..                     ...                   ...                   ...   \n",
       "42                0.513627              0.484273              0.038998   \n",
       "43                0.555556              0.496325              0.050061   \n",
       "44                0.513627              0.469598              0.038027   \n",
       "45                0.526205              0.474837              0.048503   \n",
       "46                0.511530              0.460164              0.041042   \n",
       "47                0.566038              0.491079              0.075942   \n",
       "48                0.509434              0.479555              0.039982   \n",
       "49                0.570231              0.487938              0.066087   \n",
       "50                0.513627              0.461735              0.046794   \n",
       "51                0.532495              0.487942              0.044901   \n",
       "52                0.507338              0.459115              0.039165   \n",
       "53                0.568134              0.498944              0.059279   \n",
       "54                0.463312              0.426622              0.065909   \n",
       "55                0.551363              0.482175              0.057986   \n",
       "56                0.509434              0.461737              0.051083   \n",
       "57                0.536688              0.493706              0.055677   \n",
       "58                0.542977              0.478507              0.058126   \n",
       "59                0.563941              0.514668              0.057195   \n",
       "60                0.463312              0.426622              0.065909   \n",
       "61                0.551363              0.484795              0.057575   \n",
       "62                0.509434              0.461737              0.051083   \n",
       "63                0.540881              0.491085              0.055969   \n",
       "64                0.540881              0.477459              0.057250   \n",
       "65                0.563941              0.512047              0.061969   \n",
       "66                0.467505              0.427670              0.063748   \n",
       "67                0.566038              0.498945              0.067236   \n",
       "68                0.507338              0.460689              0.049087   \n",
       "69                0.557652              0.484272              0.060271   \n",
       "70                0.536688              0.474314              0.058634   \n",
       "71                0.559748              0.501567              0.058070   \n",
       "\n",
       "    rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                      66                  0.310273                  0.359189   \n",
       "1                      59                  0.320755                  0.361286   \n",
       "2                      69                  0.310971                  0.358491   \n",
       "3                      54                  0.307477                  0.358491   \n",
       "4                      62                  0.311670                  0.360587   \n",
       "5                      57                  0.307477                  0.356394   \n",
       "6                      66                  0.310273                  0.359189   \n",
       "7                      60                  0.320755                  0.361286   \n",
       "8                      69                  0.310971                  0.358491   \n",
       "9                      54                  0.307477                  0.358491   \n",
       "10                     62                  0.311670                  0.360587   \n",
       "11                     57                  0.307477                  0.356394   \n",
       "12                     68                  0.308875                  0.358491   \n",
       "13                     72                  0.314465                  0.359189   \n",
       "14                     71                  0.310971                  0.357792   \n",
       "15                     56                  0.315863                  0.352201   \n",
       "16                     62                  0.310971                  0.360587   \n",
       "17                     65                  0.321454                  0.354997   \n",
       "18                     46                  0.340321                  0.395528   \n",
       "19                     13                  0.501048                  0.570929   \n",
       "20                     48                  0.328442                  0.393431   \n",
       "21                      6                  0.502446                  0.582809   \n",
       "22                     52                  0.316562                  0.378057   \n",
       "23                      3                  0.505940                  0.582110   \n",
       "24                     47                  0.340321                  0.393431   \n",
       "25                     19                  0.493361                  0.563242   \n",
       "26                     49                  0.327743                  0.392732   \n",
       "27                     10                  0.491265                  0.572327   \n",
       "28                     53                  0.316562                  0.377358   \n",
       "29                      4                  0.498952                  0.573725   \n",
       "..                    ...                       ...                       ...   \n",
       "42                     23                  0.436059                  0.512928   \n",
       "43                     11                  0.530398                  0.589099   \n",
       "44                     35                  0.415094                  0.484976   \n",
       "45                     31                  0.560447                  0.622642   \n",
       "46                     41                  0.411600                  0.482879   \n",
       "47                     15                  0.556953                  0.613557   \n",
       "48                     28                  0.429769                  0.513627   \n",
       "49                     18                  0.515024                  0.566737   \n",
       "50                     38                  0.403913                  0.482879   \n",
       "51                     17                  0.535290                  0.587002   \n",
       "52                     42                  0.404612                  0.480084   \n",
       "53                      8                  0.524109                  0.575821   \n",
       "54                     44                  0.427673                  0.473795   \n",
       "55                     26                  0.538784                  0.586303   \n",
       "56                     36                  0.450035                  0.513627   \n",
       "57                     12                  0.556953                  0.617051   \n",
       "58                     29                  0.467505                  0.516422   \n",
       "59                      1                  0.573026                  0.633823   \n",
       "60                     44                  0.429071                  0.473096   \n",
       "61                     22                  0.542278                  0.587701   \n",
       "62                     36                  0.450035                  0.513627   \n",
       "63                     14                  0.547170                  0.612159   \n",
       "64                     30                  0.467505                  0.515723   \n",
       "65                      2                  0.568134                  0.619846   \n",
       "66                     43                  0.429769                  0.470999   \n",
       "67                      7                  0.545073                  0.589797   \n",
       "68                     39                  0.433962                  0.501048   \n",
       "69                     24                  0.519217                  0.587002   \n",
       "70                     33                  0.455625                  0.508735   \n",
       "71                      5                  0.563941                  0.596087   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.422781                  0.414396   \n",
       "1                   0.430468                  0.419986   \n",
       "2                   0.415094                  0.415793   \n",
       "3                   0.420685                  0.422082   \n",
       "4                   0.419287                  0.412998   \n",
       "5                   0.426275                  0.412299   \n",
       "6                   0.422781                  0.413697   \n",
       "7                   0.430468                  0.419986   \n",
       "8                   0.415094                  0.415793   \n",
       "9                   0.420685                  0.422082   \n",
       "10                  0.418588                  0.412998   \n",
       "11                  0.426275                  0.412299   \n",
       "12                  0.420685                  0.412299   \n",
       "13                  0.422781                  0.406709   \n",
       "14                  0.414396                  0.414396   \n",
       "15                  0.443047                  0.410203   \n",
       "16                  0.417890                  0.411600   \n",
       "17                  0.417191                  0.421384   \n",
       "18                  0.460517                  0.456324   \n",
       "19                  0.605870                  0.641509   \n",
       "20                  0.453529                  0.442348   \n",
       "21                  0.621244                  0.649196   \n",
       "22                  0.432565                  0.431866   \n",
       "23                  0.642907                  0.641509   \n",
       "24                  0.455625                  0.456324   \n",
       "25                  0.603075                  0.633823   \n",
       "26                  0.452830                  0.442348   \n",
       "27                  0.610063                  0.630328   \n",
       "28                  0.431866                  0.429071   \n",
       "29                  0.625437                  0.635919   \n",
       "..                       ...                       ...   \n",
       "42                  0.559050                  0.570231   \n",
       "43                  0.670860                  0.667365   \n",
       "44                  0.526205                  0.545772   \n",
       "45                  0.679245                  0.673655   \n",
       "46                  0.524109                  0.530398   \n",
       "47                  0.725367                  0.709993   \n",
       "48                  0.556953                  0.566737   \n",
       "49                  0.636618                  0.653389   \n",
       "50                  0.522013                  0.540182   \n",
       "51                  0.640112                  0.636618   \n",
       "52                  0.520615                  0.525507   \n",
       "53                  0.682739                  0.675751   \n",
       "54                  0.514326                  0.562544   \n",
       "55                  0.669462                  0.672257   \n",
       "56                  0.546471                  0.572327   \n",
       "57                  0.672257                  0.666667   \n",
       "58                  0.558351                  0.575821   \n",
       "59                  0.737945                  0.695318   \n",
       "60                  0.513627                  0.562544   \n",
       "61                  0.668763                  0.672956   \n",
       "62                  0.547170                  0.572327   \n",
       "63                  0.672257                  0.665269   \n",
       "64                  0.557652                  0.575821   \n",
       "65                  0.730259                  0.693920   \n",
       "66                  0.511530                  0.559050   \n",
       "67                  0.682041                  0.663871   \n",
       "68                  0.546471                  0.570929   \n",
       "69                  0.647799                  0.656883   \n",
       "70                  0.557652                  0.573026   \n",
       "71                  0.694619                  0.649196   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.376660               0.045452  \n",
       "1                 0.383124               0.044629  \n",
       "2                 0.375087               0.043715  \n",
       "3                 0.377184               0.047741  \n",
       "4                 0.376136               0.043642  \n",
       "5                 0.375611               0.047234  \n",
       "6                 0.376485               0.045308  \n",
       "7                 0.383124               0.044629  \n",
       "8                 0.375087               0.043715  \n",
       "9                 0.377184               0.047741  \n",
       "10                0.375961               0.043470  \n",
       "11                0.375611               0.047234  \n",
       "12                0.375087               0.045065  \n",
       "13                0.375786               0.042428  \n",
       "14                0.374389               0.043296  \n",
       "15                0.380328               0.049429  \n",
       "16                0.375262               0.043261  \n",
       "17                0.378756               0.042256  \n",
       "18                0.413173               0.049301  \n",
       "19                0.579839               0.051885  \n",
       "20                0.404437               0.049355  \n",
       "21                0.588924               0.055211  \n",
       "22                0.389762               0.047697  \n",
       "23                0.593117               0.055995  \n",
       "24                0.411426               0.048345  \n",
       "25                0.573375               0.052538  \n",
       "26                0.403913               0.049490  \n",
       "27                0.575996               0.053163  \n",
       "28                0.388714               0.046972  \n",
       "29                0.583508               0.054200  \n",
       "..                     ...                    ...  \n",
       "42                0.519567               0.052781  \n",
       "43                0.614430               0.058501  \n",
       "44                0.493012               0.050053  \n",
       "45                0.633997               0.047850  \n",
       "46                0.487247               0.047335  \n",
       "47                0.651468               0.069384  \n",
       "48                0.516771               0.054061  \n",
       "49                0.592942               0.055496  \n",
       "50                0.487247               0.052379  \n",
       "51                0.599755               0.042737  \n",
       "52                0.482704               0.048411  \n",
       "53                0.614605               0.067221  \n",
       "54                0.494584               0.049793  \n",
       "55                0.616702               0.056713  \n",
       "56                0.520615               0.045752  \n",
       "57                0.628232               0.046425  \n",
       "58                0.529525               0.041810  \n",
       "59                0.660028               0.062395  \n",
       "60                0.494584               0.049333  \n",
       "61                0.617925               0.055337  \n",
       "62                0.520790               0.045852  \n",
       "63                0.624214               0.050187  \n",
       "64                0.529175               0.041746  \n",
       "65                0.653040               0.063136  \n",
       "66                0.492837               0.047927  \n",
       "67                0.620196               0.055452  \n",
       "68                0.513103               0.052120  \n",
       "69                0.602725               0.055194  \n",
       "70                0.523760               0.045945  \n",
       "71                0.625961               0.049982  \n",
       "\n",
       "[72 rows x 51 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mlp = gs.cv_results_\n",
    "data = pandas.DataFrame(results_mlp)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.775499</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.874702</td>\n",
       "      <td>0.475896</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>46</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.895454</td>\n",
       "      <td>0.499611</td>\n",
       "      <td>0.376660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.694757</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>0.479616</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>64</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>0.893105</td>\n",
       "      <td>0.505436</td>\n",
       "      <td>0.383124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.782754</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.873192</td>\n",
       "      <td>0.473380</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>53</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>0.894203</td>\n",
       "      <td>0.498355</td>\n",
       "      <td>0.375087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856503</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.872280</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>58</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>0.892793</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.377184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.251008</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.875449</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>42</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0.894690</td>\n",
       "      <td>0.499144</td>\n",
       "      <td>0.376136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.101501</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.871387</td>\n",
       "      <td>0.480934</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>61</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>0.892603</td>\n",
       "      <td>0.499063</td>\n",
       "      <td>0.375611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.861001</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.874701</td>\n",
       "      <td>0.475896</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.895454</td>\n",
       "      <td>0.499445</td>\n",
       "      <td>0.376485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.975233</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.870576</td>\n",
       "      <td>0.479117</td>\n",
       "      <td>0.363201</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.893121</td>\n",
       "      <td>0.505436</td>\n",
       "      <td>0.383124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.924519</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.873200</td>\n",
       "      <td>0.473380</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>52</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>0.894205</td>\n",
       "      <td>0.498416</td>\n",
       "      <td>0.375087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.564246</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.872292</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>0.499856</td>\n",
       "      <td>0.377184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.837503</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.875455</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>41</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0.894691</td>\n",
       "      <td>0.498979</td>\n",
       "      <td>0.375961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.969244</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.480934</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>0.892624</td>\n",
       "      <td>0.499063</td>\n",
       "      <td>0.375611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.398499</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.874708</td>\n",
       "      <td>0.474220</td>\n",
       "      <td>0.357960</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>0.895418</td>\n",
       "      <td>0.498286</td>\n",
       "      <td>0.375087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.132252</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.868748</td>\n",
       "      <td>0.473078</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>66</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>0.892663</td>\n",
       "      <td>0.499085</td>\n",
       "      <td>0.375786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.191003</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.355864</td>\n",
       "      <td>51</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>0.894218</td>\n",
       "      <td>0.497853</td>\n",
       "      <td>0.374389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.965756</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.480692</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>0.892822</td>\n",
       "      <td>0.503448</td>\n",
       "      <td>0.380328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.888509</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.875250</td>\n",
       "      <td>0.477258</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>43</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>0.894625</td>\n",
       "      <td>0.498454</td>\n",
       "      <td>0.375262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.239740</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.871448</td>\n",
       "      <td>0.477777</td>\n",
       "      <td>0.360057</td>\n",
       "      <td>59</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>0.892860</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>0.378756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.984505</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.881215</td>\n",
       "      <td>0.502821</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>19</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.530166</td>\n",
       "      <td>0.413173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.233255</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.895757</td>\n",
       "      <td>0.559186</td>\n",
       "      <td>0.491612</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0.923915</td>\n",
       "      <td>0.644879</td>\n",
       "      <td>0.579839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.456001</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879712</td>\n",
       "      <td>0.497272</td>\n",
       "      <td>0.385213</td>\n",
       "      <td>28</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>0.897016</td>\n",
       "      <td>0.521912</td>\n",
       "      <td>0.404437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.227499</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.893407</td>\n",
       "      <td>0.560192</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.928074</td>\n",
       "      <td>0.654749</td>\n",
       "      <td>0.588924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20.316752</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879773</td>\n",
       "      <td>0.482566</td>\n",
       "      <td>0.369490</td>\n",
       "      <td>25</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>0.895797</td>\n",
       "      <td>0.510192</td>\n",
       "      <td>0.389762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.478493</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.893475</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>0.506809</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.928758</td>\n",
       "      <td>0.657432</td>\n",
       "      <td>0.593117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.342246</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>0.501246</td>\n",
       "      <td>0.389930</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>0.898034</td>\n",
       "      <td>0.528639</td>\n",
       "      <td>0.411426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.477503</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.896360</td>\n",
       "      <td>0.558226</td>\n",
       "      <td>0.487417</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>0.922572</td>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.573375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17.921009</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879716</td>\n",
       "      <td>0.496814</td>\n",
       "      <td>0.384689</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.896994</td>\n",
       "      <td>0.521466</td>\n",
       "      <td>0.403913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.937748</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.561888</td>\n",
       "      <td>0.496326</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.926234</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.575996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.978260</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879766</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.895780</td>\n",
       "      <td>0.509267</td>\n",
       "      <td>0.388714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.667002</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.894628</td>\n",
       "      <td>0.565379</td>\n",
       "      <td>0.502616</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.926367</td>\n",
       "      <td>0.651365</td>\n",
       "      <td>0.583508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.823998</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.567894</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0.905560</td>\n",
       "      <td>0.604135</td>\n",
       "      <td>0.519567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>18.064747</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.868652</td>\n",
       "      <td>0.545042</td>\n",
       "      <td>0.496325</td>\n",
       "      <td>67</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>0.939513</td>\n",
       "      <td>0.684445</td>\n",
       "      <td>0.614430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20.441744</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.877959</td>\n",
       "      <td>0.557799</td>\n",
       "      <td>0.469598</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0.903121</td>\n",
       "      <td>0.588428</td>\n",
       "      <td>0.493012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.879757</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.863318</td>\n",
       "      <td>0.527817</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>0.945649</td>\n",
       "      <td>0.706825</td>\n",
       "      <td>0.633997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>24.770258</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.880258</td>\n",
       "      <td>0.549196</td>\n",
       "      <td>0.460164</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>0.584665</td>\n",
       "      <td>0.487247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>26.123751</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.858753</td>\n",
       "      <td>0.525453</td>\n",
       "      <td>0.491079</td>\n",
       "      <td>71</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>0.950632</td>\n",
       "      <td>0.718143</td>\n",
       "      <td>0.651468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.667006</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.879087</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.479555</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.905278</td>\n",
       "      <td>0.602569</td>\n",
       "      <td>0.516771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15.789512</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.877404</td>\n",
       "      <td>0.545270</td>\n",
       "      <td>0.487938</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>0.933976</td>\n",
       "      <td>0.666176</td>\n",
       "      <td>0.592942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19.630752</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.877836</td>\n",
       "      <td>0.551756</td>\n",
       "      <td>0.461735</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>0.902617</td>\n",
       "      <td>0.584122</td>\n",
       "      <td>0.487247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>19.751008</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.874128</td>\n",
       "      <td>0.548880</td>\n",
       "      <td>0.487942</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>0.936242</td>\n",
       "      <td>0.677237</td>\n",
       "      <td>0.599755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20.885504</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.880582</td>\n",
       "      <td>0.549222</td>\n",
       "      <td>0.459115</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>0.905904</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.482704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.926750</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.872354</td>\n",
       "      <td>0.545427</td>\n",
       "      <td>0.498944</td>\n",
       "      <td>56</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>0.938497</td>\n",
       "      <td>0.686262</td>\n",
       "      <td>0.614605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15.441752</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887666</td>\n",
       "      <td>0.524281</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>0.914389</td>\n",
       "      <td>0.593204</td>\n",
       "      <td>0.494584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>19.335506</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.537242</td>\n",
       "      <td>0.482175</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>0.939566</td>\n",
       "      <td>0.684732</td>\n",
       "      <td>0.616702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20.795001</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.885909</td>\n",
       "      <td>0.551189</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.914588</td>\n",
       "      <td>0.609394</td>\n",
       "      <td>0.520615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>21.490246</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.872986</td>\n",
       "      <td>0.543745</td>\n",
       "      <td>0.493706</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>0.946896</td>\n",
       "      <td>0.698741</td>\n",
       "      <td>0.628232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>21.902505</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887667</td>\n",
       "      <td>0.562246</td>\n",
       "      <td>0.478507</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>0.918090</td>\n",
       "      <td>0.619567</td>\n",
       "      <td>0.529525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>24.099248</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.871116</td>\n",
       "      <td>0.544025</td>\n",
       "      <td>0.514668</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951540</td>\n",
       "      <td>0.719065</td>\n",
       "      <td>0.660028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>16.362752</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887681</td>\n",
       "      <td>0.524281</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>0.914379</td>\n",
       "      <td>0.593170</td>\n",
       "      <td>0.494584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>18.254003</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.874974</td>\n",
       "      <td>0.540667</td>\n",
       "      <td>0.484795</td>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>0.939396</td>\n",
       "      <td>0.685319</td>\n",
       "      <td>0.617925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>18.654258</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.885941</td>\n",
       "      <td>0.551149</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>0.914562</td>\n",
       "      <td>0.609597</td>\n",
       "      <td>0.520790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>21.592253</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.873579</td>\n",
       "      <td>0.540996</td>\n",
       "      <td>0.491085</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>0.946488</td>\n",
       "      <td>0.696007</td>\n",
       "      <td>0.624214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>21.988753</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887703</td>\n",
       "      <td>0.561451</td>\n",
       "      <td>0.477459</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>0.918047</td>\n",
       "      <td>0.619287</td>\n",
       "      <td>0.529175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>24.327004</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.542828</td>\n",
       "      <td>0.512047</td>\n",
       "      <td>54</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951005</td>\n",
       "      <td>0.713958</td>\n",
       "      <td>0.653040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>16.787014</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887938</td>\n",
       "      <td>0.526418</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>0.592316</td>\n",
       "      <td>0.492837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>15.897499</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.545943</td>\n",
       "      <td>0.498945</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.936629</td>\n",
       "      <td>0.679239</td>\n",
       "      <td>0.620196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>19.482251</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.885888</td>\n",
       "      <td>0.550743</td>\n",
       "      <td>0.460689</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "      <td>0.913827</td>\n",
       "      <td>0.604496</td>\n",
       "      <td>0.513103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20.434508</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.876358</td>\n",
       "      <td>0.541550</td>\n",
       "      <td>0.484272</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.679762</td>\n",
       "      <td>0.602725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>22.482255</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.560416</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>0.917312</td>\n",
       "      <td>0.614747</td>\n",
       "      <td>0.523760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16.944497</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.874643</td>\n",
       "      <td>0.549809</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>0.694192</td>\n",
       "      <td>0.625961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        4.775499  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "1        1.694757  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "2        6.782754  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "3        1.856503  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "4        6.251008  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "5        2.101501  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "6        6.861001  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "7        1.975233  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "8        5.924519  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "9        2.564246  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "10       6.837503  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "11       1.969244  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "12       6.398499  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "13       5.132252  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "14       6.191003  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "15       4.965756  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "16       6.888509  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "17       6.239740  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "18      14.984505  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "19      18.233255  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "20      17.456001  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "21      19.227499  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "22      20.316752  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "23      32.478493  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "24      17.342246  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "25      16.477503  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "26      17.921009  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "27      18.937748  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "28      18.978260  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "29      21.667002  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "..            ...                                                ...   \n",
       "42      14.823998  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "43      18.064747  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "44      20.441744  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "45      26.879757  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "46      24.770258  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "47      26.123751  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "48      14.667006  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "49      15.789512  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "50      19.630752  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "51      19.751008  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "52      20.885504  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "53      22.926750  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "54      15.441752  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "55      19.335506  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "56      20.795001  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "57      21.490246  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "58      21.902505  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "59      24.099248  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "60      16.362752  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "61      18.254003  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "62      18.654258  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "63      21.592253  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "64      21.988753  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "65      24.327004  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "66      16.787014  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "67      15.897499  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "68      19.482251  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "69      20.434508  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "70      22.482255  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "71      16.944497  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.874702           0.475896               0.359533             46   \n",
       "1        0.870542           0.479616               0.363725             64   \n",
       "2        0.873192           0.473380               0.356388             53   \n",
       "3        0.872280           0.481655               0.366870             58   \n",
       "4        0.875449           0.477281               0.360581             42   \n",
       "5        0.871387           0.480934               0.364249             61   \n",
       "6        0.874701           0.475896               0.359533             47   \n",
       "7        0.870576           0.479117               0.363201             63   \n",
       "8        0.873200           0.473380               0.356388             52   \n",
       "9        0.872292           0.481655               0.366870             57   \n",
       "10       0.875455           0.477281               0.360581             41   \n",
       "11       0.871421           0.480934               0.364249             60   \n",
       "12       0.874708           0.474220               0.357960             45   \n",
       "13       0.868748           0.473078               0.355340             66   \n",
       "14       0.873333           0.472989               0.355864             51   \n",
       "15       0.869673           0.480692               0.365297             65   \n",
       "16       0.875250           0.477258               0.360581             43   \n",
       "17       0.871448           0.477777               0.360057             59   \n",
       "18       0.881215           0.502821               0.391503             19   \n",
       "19       0.895757           0.559186               0.491612              2   \n",
       "20       0.879712           0.497272               0.385213             28   \n",
       "21       0.893407           0.560192               0.498947              8   \n",
       "22       0.879773           0.482566               0.369490             25   \n",
       "23       0.893475           0.566355               0.506809              7   \n",
       "24       0.881179           0.501246               0.389930             20   \n",
       "25       0.896360           0.558226               0.487417              1   \n",
       "26       0.879716           0.496814               0.384689             27   \n",
       "27       0.893962           0.561888               0.496326              5   \n",
       "28       0.879766           0.482411               0.368966             26   \n",
       "29       0.894628           0.565379               0.502616              3   \n",
       "..            ...                ...                    ...            ...   \n",
       "42       0.878853           0.567894               0.484273             32   \n",
       "43       0.868652           0.545042               0.496325             67   \n",
       "44       0.877959           0.557799               0.469598             34   \n",
       "45       0.863318           0.527817               0.474837             70   \n",
       "46       0.880258           0.549196               0.460164             23   \n",
       "47       0.858753           0.525453               0.491079             71   \n",
       "48       0.879087           0.563848               0.479555             31   \n",
       "49       0.877404           0.545270               0.487938             38   \n",
       "50       0.877836           0.551756               0.461735             36   \n",
       "51       0.874128           0.548880               0.487942             49   \n",
       "52       0.880582           0.549222               0.459115             22   \n",
       "53       0.872354           0.545427               0.498944             56   \n",
       "54       0.887666           0.524281               0.426622             15   \n",
       "55       0.875827           0.537242               0.482175             40   \n",
       "56       0.885909           0.551189               0.461737             17   \n",
       "57       0.872986           0.543745               0.493706             55   \n",
       "58       0.887667           0.562246               0.478507             14   \n",
       "59       0.871116           0.544025               0.514668             62   \n",
       "60       0.887681           0.524281               0.426622             13   \n",
       "61       0.874974           0.540667               0.484795             44   \n",
       "62       0.885941           0.551149               0.461737             16   \n",
       "63       0.873579           0.540996               0.491085             50   \n",
       "64       0.887703           0.561451               0.477459             12   \n",
       "65       0.873101           0.542828               0.512047             54   \n",
       "66       0.887938           0.526418               0.427670             10   \n",
       "67       0.877452           0.545943               0.498945             37   \n",
       "68       0.885888           0.550743               0.460689             18   \n",
       "69       0.876358           0.541550               0.484272             39   \n",
       "70       0.887870           0.560416               0.474314             11   \n",
       "71       0.874643           0.549809               0.501567             48   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  65                     66        0.895454   \n",
       "1                  59                     59        0.893105   \n",
       "2                  69                     69        0.894203   \n",
       "3                  54                     54        0.892793   \n",
       "4                  62                     62        0.894690   \n",
       "5                  56                     57        0.892603   \n",
       "6                  65                     66        0.895454   \n",
       "7                  60                     60        0.893121   \n",
       "8                  69                     69        0.894205   \n",
       "9                  54                     54        0.892800   \n",
       "10                 62                     62        0.894691   \n",
       "11                 56                     57        0.892624   \n",
       "12                 68                     68        0.895418   \n",
       "13                 71                     72        0.892663   \n",
       "14                 72                     71        0.894218   \n",
       "15                 58                     56        0.892822   \n",
       "16                 64                     62        0.894625   \n",
       "17                 61                     65        0.892860   \n",
       "18                 46                     46        0.898089   \n",
       "19                 14                     13        0.923915   \n",
       "20                 48                     48        0.897016   \n",
       "21                 13                      6        0.928074   \n",
       "22                 52                     52        0.895797   \n",
       "23                  6                      3        0.928758   \n",
       "24                 47                     47        0.898034   \n",
       "25                 16                     19        0.922572   \n",
       "26                 49                     49        0.896994   \n",
       "27                 10                     10        0.926234   \n",
       "28                 53                     53        0.895780   \n",
       "29                  7                      4        0.926367   \n",
       "..                ...                    ...             ...   \n",
       "42                  4                     23        0.905560   \n",
       "43                 30                     11        0.939513   \n",
       "44                 17                     35        0.903121   \n",
       "45                 40                     31        0.945649   \n",
       "46                 25                     41        0.906226   \n",
       "47                 42                     15        0.950632   \n",
       "48                  8                     28        0.905278   \n",
       "49                 29                     18        0.933976   \n",
       "50                 18                     38        0.902617   \n",
       "51                 26                     17        0.936242   \n",
       "52                 24                     42        0.905904   \n",
       "53                 28                      8        0.938497   \n",
       "54                 43                     44        0.914389   \n",
       "55                 38                     26        0.939566   \n",
       "56                 19                     36        0.914588   \n",
       "57                 33                     12        0.946896   \n",
       "58                  9                     29        0.918090   \n",
       "59                 32                      1        0.951540   \n",
       "60                 43                     44        0.914379   \n",
       "61                 37                     22        0.939396   \n",
       "62                 20                     36        0.914562   \n",
       "63                 36                     14        0.946488   \n",
       "64                 11                     30        0.918047   \n",
       "65                 34                      2        0.951005   \n",
       "66                 41                     43        0.914281   \n",
       "67                 27                      7        0.936629   \n",
       "68                 21                     39        0.913827   \n",
       "69                 35                     24        0.941911   \n",
       "70                 12                     33        0.917312   \n",
       "71                 22                      5        0.944009   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.499611                0.376660  \n",
       "1             0.505436                0.383124  \n",
       "2             0.498355                0.375087  \n",
       "3             0.499800                0.377184  \n",
       "4             0.499144                0.376136  \n",
       "5             0.499063                0.375611  \n",
       "6             0.499445                0.376485  \n",
       "7             0.505436                0.383124  \n",
       "8             0.498416                0.375087  \n",
       "9             0.499856                0.377184  \n",
       "10            0.498979                0.375961  \n",
       "11            0.499063                0.375611  \n",
       "12            0.498286                0.375087  \n",
       "13            0.499085                0.375786  \n",
       "14            0.497853                0.374389  \n",
       "15            0.503448                0.380328  \n",
       "16            0.498454                0.375262  \n",
       "17            0.501004                0.378756  \n",
       "18            0.530166                0.413173  \n",
       "19            0.644879                0.579839  \n",
       "20            0.521912                0.404437  \n",
       "21            0.654749                0.588924  \n",
       "22            0.510192                0.389762  \n",
       "23            0.657432                0.593117  \n",
       "24            0.528639                0.411426  \n",
       "25            0.641730                0.573375  \n",
       "26            0.521466                0.403913  \n",
       "27            0.647431                0.575996  \n",
       "28            0.509267                0.388714  \n",
       "29            0.651365                0.583508  \n",
       "..                 ...                     ...  \n",
       "42            0.604135                0.519567  \n",
       "43            0.684445                0.614430  \n",
       "44            0.588428                0.493012  \n",
       "45            0.706825                0.633997  \n",
       "46            0.584665                0.487247  \n",
       "47            0.718143                0.651468  \n",
       "48            0.602569                0.516771  \n",
       "49            0.666176                0.592942  \n",
       "50            0.584122                0.487247  \n",
       "51            0.677237                0.599755  \n",
       "52            0.581875                0.482704  \n",
       "53            0.686262                0.614605  \n",
       "54            0.593204                0.494584  \n",
       "55            0.684732                0.616702  \n",
       "56            0.609394                0.520615  \n",
       "57            0.698741                0.628232  \n",
       "58            0.619567                0.529525  \n",
       "59            0.719065                0.660028  \n",
       "60            0.593170                0.494584  \n",
       "61            0.685319                0.617925  \n",
       "62            0.609597                0.520790  \n",
       "63            0.696007                0.624214  \n",
       "64            0.619287                0.529175  \n",
       "65            0.713958                0.653040  \n",
       "66            0.592316                0.492837  \n",
       "67            0.679239                0.620196  \n",
       "68            0.604496                0.513103  \n",
       "69            0.679762                0.602725  \n",
       "70            0.614747                0.523760  \n",
       "71            0.694192                0.625961  \n",
       "\n",
       "[72 rows x 11 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_mlp = make_table(data)\n",
    "table_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlpclassifier__activation': 'logistic',\n",
       " 'mlpclassifier__alpha': 0.01,\n",
       " 'mlpclassifier__hidden_layer_sizes': 20,\n",
       " 'mlpclassifier__learning_rate': 'adaptive',\n",
       " 'mlpclassifier__max_iter': 250,\n",
       " 'mlpclassifier__random_state': 0,\n",
       " 'mlpclassifier__solver': 'adam'}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_index_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'complementnb__alpha': [0, 0.4, 0.5, 0.6, 1],\n",
    "    'complementnb__fit_prior': (True, False),\n",
    "    'complementnb__norm': (True, False)\n",
    "}\n",
    "\n",
    "pp = make_pipeline(MinMaxScaler(), ComplementNB())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_complementnb__alpha</th>\n",
       "      <th>param_complementnb__fit_prior</th>\n",
       "      <th>param_complementnb__norm</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034258</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.813963</td>\n",
       "      <td>0.803381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>0.179328</td>\n",
       "      <td>11</td>\n",
       "      <td>0.836478</td>\n",
       "      <td>0.450734</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.669811</td>\n",
       "      <td>0.139773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036255</td>\n",
       "      <td>0.011507</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.816455</td>\n",
       "      <td>0.819132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839273</td>\n",
       "      <td>0.877009</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862858</td>\n",
       "      <td>0.014325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.039065</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.813963</td>\n",
       "      <td>0.803381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>0.179328</td>\n",
       "      <td>11</td>\n",
       "      <td>0.836478</td>\n",
       "      <td>0.450734</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.669811</td>\n",
       "      <td>0.139773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054212</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.026475</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.816455</td>\n",
       "      <td>0.819132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839273</td>\n",
       "      <td>0.877009</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862858</td>\n",
       "      <td>0.014325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.039064</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.813820</td>\n",
       "      <td>0.802612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.447240</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.715584</td>\n",
       "      <td>0.665618</td>\n",
       "      <td>0.138401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.046873</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.816521</td>\n",
       "      <td>0.818484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.060763</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.045240</td>\n",
       "      <td>0.007587</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.813820</td>\n",
       "      <td>0.802612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.447240</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.715584</td>\n",
       "      <td>0.665618</td>\n",
       "      <td>0.138401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.056997</td>\n",
       "      <td>0.017936</td>\n",
       "      <td>0.061253</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.816521</td>\n",
       "      <td>0.818484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.054504</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.813757</td>\n",
       "      <td>0.802388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.714885</td>\n",
       "      <td>0.664396</td>\n",
       "      <td>0.139032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.816530</td>\n",
       "      <td>0.818332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.874913</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862509</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.044995</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.813757</td>\n",
       "      <td>0.802388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.714885</td>\n",
       "      <td>0.664396</td>\n",
       "      <td>0.139032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055503</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.047493</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.816530</td>\n",
       "      <td>0.818332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.874913</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862509</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.813695</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>0.180195</td>\n",
       "      <td>17</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>0.713487</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.138898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055927</td>\n",
       "      <td>0.010170</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.816552</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.050787</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.813695</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>0.180195</td>\n",
       "      <td>17</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>0.713487</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.138898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059201</td>\n",
       "      <td>0.012571</td>\n",
       "      <td>0.042966</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.816552</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050785</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>0.017027</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.813452</td>\n",
       "      <td>0.801399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.181475</td>\n",
       "      <td>19</td>\n",
       "      <td>0.820405</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.667365</td>\n",
       "      <td>0.712089</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>0.140217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.064924</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>0.051369</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.816570</td>\n",
       "      <td>0.817513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>0.107884</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870021</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.863732</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.013577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.813452</td>\n",
       "      <td>0.801399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.181475</td>\n",
       "      <td>19</td>\n",
       "      <td>0.820405</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.667365</td>\n",
       "      <td>0.712089</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>0.140217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.046872</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.039067</td>\n",
       "      <td>0.007805</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.816570</td>\n",
       "      <td>0.817513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>0.107884</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870021</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.863732</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.013577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.034258      0.001295         0.047244        0.006984   \n",
       "1        0.036255      0.011507         0.034548        0.003359   \n",
       "2        0.035156      0.006772         0.039065        0.013554   \n",
       "3        0.054212      0.013631         0.026475        0.005136   \n",
       "4        0.035156      0.006764         0.039064        0.007809   \n",
       "5        0.046873      0.011056         0.039053        0.007804   \n",
       "6        0.060763      0.018033         0.045240        0.007587   \n",
       "7        0.056997      0.017936         0.061253        0.008869   \n",
       "8        0.054504      0.012627         0.048248        0.004258   \n",
       "9        0.063504      0.017629         0.044748        0.002487   \n",
       "10       0.055000      0.010635         0.044995        0.005047   \n",
       "11       0.055503      0.007700         0.047493        0.009066   \n",
       "12       0.069002      0.022504         0.047500        0.006580   \n",
       "13       0.055927      0.010170         0.035155        0.006768   \n",
       "14       0.050787      0.006776         0.046875        0.000017   \n",
       "15       0.059201      0.012571         0.042966        0.006758   \n",
       "16       0.050785      0.006763         0.050776        0.017027   \n",
       "17       0.064924      0.013970         0.051369        0.019297   \n",
       "18       0.042969      0.006731         0.042959        0.012933   \n",
       "19       0.046872      0.000015         0.039067        0.007805   \n",
       "\n",
       "   param_complementnb__alpha param_complementnb__fit_prior  \\\n",
       "0                          0                          True   \n",
       "1                          0                          True   \n",
       "2                          0                         False   \n",
       "3                          0                         False   \n",
       "4                        0.4                          True   \n",
       "5                        0.4                          True   \n",
       "6                        0.4                         False   \n",
       "7                        0.4                         False   \n",
       "8                        0.5                          True   \n",
       "9                        0.5                          True   \n",
       "10                       0.5                         False   \n",
       "11                       0.5                         False   \n",
       "12                       0.6                          True   \n",
       "13                       0.6                          True   \n",
       "14                       0.6                         False   \n",
       "15                       0.6                         False   \n",
       "16                         1                          True   \n",
       "17                         1                          True   \n",
       "18                         1                         False   \n",
       "19                         1                         False   \n",
       "\n",
       "   param_complementnb__norm  \\\n",
       "0                      True   \n",
       "1                     False   \n",
       "2                      True   \n",
       "3                     False   \n",
       "4                      True   \n",
       "5                     False   \n",
       "6                      True   \n",
       "7                     False   \n",
       "8                      True   \n",
       "9                     False   \n",
       "10                     True   \n",
       "11                    False   \n",
       "12                     True   \n",
       "13                    False   \n",
       "14                     True   \n",
       "15                    False   \n",
       "16                     True   \n",
       "17                    False   \n",
       "18                     True   \n",
       "19                    False   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'complementnb__alpha': 0, 'complementnb__fit_...         0.813963   \n",
       "1   {'complementnb__alpha': 0, 'complementnb__fit_...         0.816455   \n",
       "2   {'complementnb__alpha': 0, 'complementnb__fit_...         0.813963   \n",
       "3   {'complementnb__alpha': 0, 'complementnb__fit_...         0.816455   \n",
       "4   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.813820   \n",
       "5   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.816521   \n",
       "6   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.813820   \n",
       "7   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.816521   \n",
       "8   {'complementnb__alpha': 0.5, 'complementnb__fi...         0.813757   \n",
       "9   {'complementnb__alpha': 0.5, 'complementnb__fi...         0.816530   \n",
       "10  {'complementnb__alpha': 0.5, 'complementnb__fi...         0.813757   \n",
       "11  {'complementnb__alpha': 0.5, 'complementnb__fi...         0.816530   \n",
       "12  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.813695   \n",
       "13  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.816552   \n",
       "14  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.813695   \n",
       "15  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.816552   \n",
       "16  {'complementnb__alpha': 1, 'complementnb__fit_...         0.813452   \n",
       "17  {'complementnb__alpha': 1, 'complementnb__fit_...         0.816570   \n",
       "18  {'complementnb__alpha': 1, 'complementnb__fit_...         0.813452   \n",
       "19  {'complementnb__alpha': 1, 'complementnb__fit_...         0.816570   \n",
       "\n",
       "    split1_test_AUC  ...  split3_test_Sensitivity  mean_test_Sensitivity  \\\n",
       "0          0.803381  ...                 0.853249               0.642534   \n",
       "1          0.819132  ...                 0.955975               0.835413   \n",
       "2          0.803381  ...                 0.853249               0.642534   \n",
       "3          0.819132  ...                 0.955975               0.835413   \n",
       "4          0.802612  ...                 0.851153               0.639914   \n",
       "5          0.818484  ...                 0.955975               0.835413   \n",
       "6          0.802612  ...                 0.851153               0.639914   \n",
       "7          0.818484  ...                 0.955975               0.835413   \n",
       "8          0.802388  ...                 0.851153               0.639914   \n",
       "9          0.818332  ...                 0.955975               0.835413   \n",
       "10         0.802388  ...                 0.851153               0.639914   \n",
       "11         0.818332  ...                 0.955975               0.835413   \n",
       "12         0.802181  ...                 0.851153               0.638865   \n",
       "13         0.818178  ...                 0.955975               0.835413   \n",
       "14         0.802181  ...                 0.851153               0.638865   \n",
       "15         0.818178  ...                 0.955975               0.835413   \n",
       "16         0.801399  ...                 0.846960               0.635196   \n",
       "17         0.817513  ...                 0.955975               0.834889   \n",
       "18         0.801399  ...                 0.846960               0.635196   \n",
       "19         0.817513  ...                 0.955975               0.834889   \n",
       "\n",
       "    std_test_Sensitivity  rank_test_Sensitivity  split0_train_Sensitivity  \\\n",
       "0               0.179328                     11                  0.836478   \n",
       "1               0.107610                      1                  0.870720   \n",
       "2               0.179328                     11                  0.836478   \n",
       "3               0.107610                      1                  0.870720   \n",
       "4               0.180396                     13                  0.828092   \n",
       "5               0.107610                      1                  0.870720   \n",
       "6               0.180396                     13                  0.828092   \n",
       "7               0.107610                      1                  0.870720   \n",
       "8               0.180396                     13                  0.826695   \n",
       "9               0.107610                      1                  0.870720   \n",
       "10              0.180396                     13                  0.826695   \n",
       "11              0.107610                      1                  0.870720   \n",
       "12              0.180195                     17                  0.826695   \n",
       "13              0.107610                      1                  0.870720   \n",
       "14              0.180195                     17                  0.826695   \n",
       "15              0.107610                      1                  0.870720   \n",
       "16              0.181475                     19                  0.820405   \n",
       "17              0.107884                      9                  0.870021   \n",
       "18              0.181475                     19                  0.820405   \n",
       "19              0.107884                      9                  0.870021   \n",
       "\n",
       "    split1_train_Sensitivity  split2_train_Sensitivity  \\\n",
       "0                   0.450734                  0.673655   \n",
       "1                   0.839273                  0.877009   \n",
       "2                   0.450734                  0.673655   \n",
       "3                   0.839273                  0.877009   \n",
       "4                   0.447240                  0.671558   \n",
       "5                   0.839972                  0.875611   \n",
       "6                   0.447240                  0.671558   \n",
       "7                   0.839972                  0.875611   \n",
       "8                   0.444444                  0.671558   \n",
       "9                   0.839972                  0.874913   \n",
       "10                  0.444444                  0.671558   \n",
       "11                  0.839972                  0.874913   \n",
       "12                  0.444444                  0.670860   \n",
       "13                  0.839972                  0.875611   \n",
       "14                  0.444444                  0.670860   \n",
       "15                  0.839972                  0.875611   \n",
       "16                  0.436059                  0.667365   \n",
       "17                  0.839972                  0.875611   \n",
       "18                  0.436059                  0.667365   \n",
       "19                  0.839972                  0.875611   \n",
       "\n",
       "    split3_train_Sensitivity  mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                   0.718379                0.669811               0.139773  \n",
       "1                   0.864430                0.862858               0.014325  \n",
       "2                   0.718379                0.669811               0.139773  \n",
       "3                   0.864430                0.862858               0.014325  \n",
       "4                   0.715584                0.665618               0.138401  \n",
       "5                   0.864430                0.862683               0.013698  \n",
       "6                   0.715584                0.665618               0.138401  \n",
       "7                   0.864430                0.862683               0.013698  \n",
       "8                   0.714885                0.664396               0.139032  \n",
       "9                   0.864430                0.862509               0.013536  \n",
       "10                  0.714885                0.664396               0.139032  \n",
       "11                  0.864430                0.862509               0.013536  \n",
       "12                  0.713487                0.663871               0.138898  \n",
       "13                  0.864430                0.862683               0.013698  \n",
       "14                  0.713487                0.663871               0.138898  \n",
       "15                  0.864430                0.862683               0.013698  \n",
       "16                  0.712089                0.658980               0.140217  \n",
       "17                  0.863732                0.862334               0.013577  \n",
       "18                  0.712089                0.658980               0.140217  \n",
       "19                  0.863732                0.862334               0.013577  \n",
       "\n",
       "[20 rows x 47 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cnb = gs.cv_results_\n",
    "data = pandas.DataFrame(results_cnb)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034258</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.792167</td>\n",
       "      <td>0.441939</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.807121</td>\n",
       "      <td>0.463429</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036255</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.797842</td>\n",
       "      <td>0.420761</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812993</td>\n",
       "      <td>0.418521</td>\n",
       "      <td>0.862858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.792167</td>\n",
       "      <td>0.441939</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.807121</td>\n",
       "      <td>0.463429</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054212</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.797842</td>\n",
       "      <td>0.420761</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812993</td>\n",
       "      <td>0.418521</td>\n",
       "      <td>0.862858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.441950</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806888</td>\n",
       "      <td>0.464422</td>\n",
       "      <td>0.665618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.046873</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.420606</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812962</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.060763</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.441950</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806888</td>\n",
       "      <td>0.464422</td>\n",
       "      <td>0.665618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.056997</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.420606</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812962</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.054504</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.791784</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806834</td>\n",
       "      <td>0.464271</td>\n",
       "      <td>0.664396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.797661</td>\n",
       "      <td>0.420680</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812953</td>\n",
       "      <td>0.418923</td>\n",
       "      <td>0.862509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.055000</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.791784</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806834</td>\n",
       "      <td>0.464271</td>\n",
       "      <td>0.664396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055503</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.797661</td>\n",
       "      <td>0.420680</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812953</td>\n",
       "      <td>0.418923</td>\n",
       "      <td>0.862509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.069002</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.791695</td>\n",
       "      <td>0.442157</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.806776</td>\n",
       "      <td>0.464853</td>\n",
       "      <td>0.663871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055927</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.797630</td>\n",
       "      <td>0.420610</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812943</td>\n",
       "      <td>0.418993</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.050787</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.791695</td>\n",
       "      <td>0.442157</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.806776</td>\n",
       "      <td>0.464853</td>\n",
       "      <td>0.663871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059201</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.797630</td>\n",
       "      <td>0.420610</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812943</td>\n",
       "      <td>0.418993</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050785</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.791362</td>\n",
       "      <td>0.441815</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.806544</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>0.658980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.064924</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.797465</td>\n",
       "      <td>0.420549</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812911</td>\n",
       "      <td>0.419207</td>\n",
       "      <td>0.862334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.791362</td>\n",
       "      <td>0.441815</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.806544</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>0.658980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.046872</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.797465</td>\n",
       "      <td>0.420549</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812911</td>\n",
       "      <td>0.419207</td>\n",
       "      <td>0.862334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        0.034258  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "1        0.036255  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "2        0.035156  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "3        0.054212  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "4        0.035156  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "5        0.046873  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "6        0.060763  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "7        0.056997  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "8        0.054504  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "9        0.063504  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "10       0.055000  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "11       0.055503  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "12       0.069002  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "13       0.055927  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "14       0.050787  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "15       0.059201  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "16       0.050785  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "17       0.064924  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "18       0.042969  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "19       0.046872  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.792167           0.441939               0.642534             11   \n",
       "1        0.797842           0.420761               0.835413              1   \n",
       "2        0.792167           0.441939               0.642534             11   \n",
       "3        0.797842           0.420761               0.835413              1   \n",
       "4        0.791874           0.441950               0.639914             13   \n",
       "5        0.797702           0.420606               0.835413              3   \n",
       "6        0.791874           0.441950               0.639914             13   \n",
       "7        0.797702           0.420606               0.835413              3   \n",
       "8        0.791784           0.442446               0.639914             15   \n",
       "9        0.797661           0.420680               0.835413              5   \n",
       "10       0.791784           0.442446               0.639914             15   \n",
       "11       0.797661           0.420680               0.835413              5   \n",
       "12       0.791695           0.442157               0.638865             17   \n",
       "13       0.797630           0.420610               0.835413              7   \n",
       "14       0.791695           0.442157               0.638865             17   \n",
       "15       0.797630           0.420610               0.835413              7   \n",
       "16       0.791362           0.441815               0.635196             19   \n",
       "17       0.797465           0.420549               0.834889              9   \n",
       "18       0.791362           0.441815               0.635196             19   \n",
       "19       0.797465           0.420549               0.834889              9   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   7                     11        0.807121   \n",
       "1                  11                      1        0.812993   \n",
       "2                   7                     11        0.807121   \n",
       "3                  11                      1        0.812993   \n",
       "4                   5                     13        0.806888   \n",
       "5                  17                      1        0.812962   \n",
       "6                   5                     13        0.806888   \n",
       "7                  17                      1        0.812962   \n",
       "8                   1                     13        0.806834   \n",
       "9                  13                      1        0.812953   \n",
       "10                  1                     13        0.806834   \n",
       "11                 13                      1        0.812953   \n",
       "12                  3                     17        0.806776   \n",
       "13                 15                      1        0.812943   \n",
       "14                  3                     17        0.806776   \n",
       "15                 15                      1        0.812943   \n",
       "16                  9                     19        0.806544   \n",
       "17                 19                      9        0.812911   \n",
       "18                  9                     19        0.806544   \n",
       "19                 19                      9        0.812911   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.463429                0.669811  \n",
       "1             0.418521                0.862858  \n",
       "2             0.463429                0.669811  \n",
       "3             0.418521                0.862858  \n",
       "4             0.464422                0.665618  \n",
       "5             0.418882                0.862683  \n",
       "6             0.464422                0.665618  \n",
       "7             0.418882                0.862683  \n",
       "8             0.464271                0.664396  \n",
       "9             0.418923                0.862509  \n",
       "10            0.464271                0.664396  \n",
       "11            0.418923                0.862509  \n",
       "12            0.464853                0.663871  \n",
       "13            0.418993                0.862683  \n",
       "14            0.464853                0.663871  \n",
       "15            0.418993                0.862683  \n",
       "16            0.464452                0.658980  \n",
       "17            0.419207                0.862334  \n",
       "18            0.464452                0.658980  \n",
       "19            0.419207                0.862334  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_cnb = make_table(data)\n",
    "table_cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complementnb__alpha': 0.5,\n",
       " 'complementnb__fit_prior': True,\n",
       " 'complementnb__norm': True}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 11 candidates, totalling 44 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  44 | elapsed:    4.7s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'gaussiannb__var_smoothing': [0.0000000001, 0.00000001, 0.000001, 0.0001, 0.1, 10, 100, 1000, 100000, 1000000, 100000000]\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), GaussianNB())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gaussiannb__var_smoothing</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>split3_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079499</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>1e-10</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-10}</td>\n",
       "      <td>0.848756</td>\n",
       "      <td>0.830094</td>\n",
       "      <td>0.800651</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062663</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-08}</td>\n",
       "      <td>0.848756</td>\n",
       "      <td>0.830094</td>\n",
       "      <td>0.800651</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093762</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.076338</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-06}</td>\n",
       "      <td>0.848756</td>\n",
       "      <td>0.830095</td>\n",
       "      <td>0.800651</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121751</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.098747</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.0001}</td>\n",
       "      <td>0.848759</td>\n",
       "      <td>0.830101</td>\n",
       "      <td>0.800648</td>\n",
       "      <td>0.806298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124741</td>\n",
       "      <td>0.015825</td>\n",
       "      <td>0.075324</td>\n",
       "      <td>0.024469</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.1}</td>\n",
       "      <td>0.849927</td>\n",
       "      <td>0.834127</td>\n",
       "      <td>0.797387</td>\n",
       "      <td>0.803957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740042</td>\n",
       "      <td>0.574929</td>\n",
       "      <td>0.120297</td>\n",
       "      <td>5</td>\n",
       "      <td>0.614256</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.612509</td>\n",
       "      <td>0.027976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.121096</td>\n",
       "      <td>0.027891</td>\n",
       "      <td>0.066414</td>\n",
       "      <td>0.017030</td>\n",
       "      <td>10</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 10}</td>\n",
       "      <td>0.902410</td>\n",
       "      <td>0.840693</td>\n",
       "      <td>0.807217</td>\n",
       "      <td>0.816855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.000579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097652</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.074227</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100}</td>\n",
       "      <td>0.917182</td>\n",
       "      <td>0.844509</td>\n",
       "      <td>0.814459</td>\n",
       "      <td>0.824082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089851</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.074209</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000}</td>\n",
       "      <td>0.918948</td>\n",
       "      <td>0.845084</td>\n",
       "      <td>0.815406</td>\n",
       "      <td>0.825040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129291</td>\n",
       "      <td>0.013739</td>\n",
       "      <td>0.088745</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000}</td>\n",
       "      <td>0.919126</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>0.825148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.028086</td>\n",
       "      <td>1000000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000000}</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144359</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.102747</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>100000000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000000}</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.079499      0.004940         0.060749        0.004810   \n",
       "1        0.062663      0.000249         0.050786        0.006753   \n",
       "2        0.093762      0.011059         0.076338        0.014632   \n",
       "3        0.121751      0.032900         0.098747        0.017100   \n",
       "4        0.124741      0.015825         0.075324        0.024469   \n",
       "5        0.121096      0.027891         0.066414        0.017030   \n",
       "6        0.097652      0.024027         0.074227        0.027898   \n",
       "7        0.089851      0.017025         0.074209        0.012958   \n",
       "8        0.129291      0.013739         0.088745        0.008550   \n",
       "9        0.092042      0.016431         0.080500        0.028086   \n",
       "10       0.144359      0.018933         0.102747        0.012047   \n",
       "\n",
       "   param_gaussiannb__var_smoothing                                    params  \\\n",
       "0                            1e-10      {'gaussiannb__var_smoothing': 1e-10}   \n",
       "1                            1e-08      {'gaussiannb__var_smoothing': 1e-08}   \n",
       "2                            1e-06      {'gaussiannb__var_smoothing': 1e-06}   \n",
       "3                           0.0001     {'gaussiannb__var_smoothing': 0.0001}   \n",
       "4                              0.1        {'gaussiannb__var_smoothing': 0.1}   \n",
       "5                               10         {'gaussiannb__var_smoothing': 10}   \n",
       "6                              100        {'gaussiannb__var_smoothing': 100}   \n",
       "7                             1000       {'gaussiannb__var_smoothing': 1000}   \n",
       "8                           100000     {'gaussiannb__var_smoothing': 100000}   \n",
       "9                          1000000    {'gaussiannb__var_smoothing': 1000000}   \n",
       "10                       100000000  {'gaussiannb__var_smoothing': 100000000}   \n",
       "\n",
       "    split0_test_AUC  split1_test_AUC  split2_test_AUC  split3_test_AUC  ...  \\\n",
       "0          0.848756         0.830094         0.800651         0.806303  ...   \n",
       "1          0.848756         0.830094         0.800651         0.806303  ...   \n",
       "2          0.848756         0.830095         0.800651         0.806302  ...   \n",
       "3          0.848759         0.830101         0.800648         0.806298  ...   \n",
       "4          0.849927         0.834127         0.797387         0.803957  ...   \n",
       "5          0.902410         0.840693         0.807217         0.816855  ...   \n",
       "6          0.917182         0.844509         0.814459         0.824082  ...   \n",
       "7          0.918948         0.845084         0.815406         0.825040  ...   \n",
       "8          0.919126         0.845140         0.815510         0.825148  ...   \n",
       "9          0.919129         0.845140         0.815510         0.825150  ...   \n",
       "10         0.919129         0.845140         0.815510         0.825150  ...   \n",
       "\n",
       "    split3_test_Sensitivity  mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                  0.823899               0.649351              0.132485   \n",
       "1                  0.823899               0.649351              0.132485   \n",
       "2                  0.823899               0.649351              0.132485   \n",
       "3                  0.823899               0.649351              0.132485   \n",
       "4                  0.740042               0.574929              0.120297   \n",
       "5                  0.008386               0.008909              0.003743   \n",
       "6                  0.000000               0.000000              0.000000   \n",
       "7                  0.000000               0.000000              0.000000   \n",
       "8                  0.000000               0.000000              0.000000   \n",
       "9                  0.000000               0.000000              0.000000   \n",
       "10                 0.000000               0.000000              0.000000   \n",
       "\n",
       "    rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                       1                  0.709294                  0.649196   \n",
       "1                       1                  0.709294                  0.649196   \n",
       "2                       1                  0.709294                  0.649196   \n",
       "3                       1                  0.709294                  0.649196   \n",
       "4                       5                  0.614256                  0.568134   \n",
       "5                       6                  0.009783                  0.010482   \n",
       "6                       7                  0.000000                  0.000000   \n",
       "7                       7                  0.000000                  0.000000   \n",
       "8                       7                  0.000000                  0.000000   \n",
       "9                       7                  0.000000                  0.000000   \n",
       "10                      7                  0.000000                  0.000000   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.689029                  0.718379   \n",
       "1                   0.689029                  0.718379   \n",
       "2                   0.689029                  0.718379   \n",
       "3                   0.689029                  0.718379   \n",
       "4                   0.622642                  0.645003   \n",
       "5                   0.009085                  0.010482   \n",
       "6                   0.000000                  0.000000   \n",
       "7                   0.000000                  0.000000   \n",
       "8                   0.000000                  0.000000   \n",
       "9                   0.000000                  0.000000   \n",
       "10                  0.000000                  0.000000   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.691474               0.026621  \n",
       "1                 0.691474               0.026621  \n",
       "2                 0.691474               0.026621  \n",
       "3                 0.691474               0.026621  \n",
       "4                 0.612509               0.027976  \n",
       "5                 0.009958               0.000579  \n",
       "6                 0.000000               0.000000  \n",
       "7                 0.000000               0.000000  \n",
       "8                 0.000000               0.000000  \n",
       "9                 0.000000               0.000000  \n",
       "10                0.000000               0.000000  \n",
       "\n",
       "[11 rows x 45 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gnb = gs.cv_results_\n",
    "data = pandas.DataFrame(results_gnb)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079499</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-10}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062663</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-08}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093762</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-06}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121751</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.0001}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495452</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516617</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124741</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.1}</td>\n",
       "      <td>0.821353</td>\n",
       "      <td>0.484189</td>\n",
       "      <td>0.574929</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.834098</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>0.612509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.121096</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 10}</td>\n",
       "      <td>0.841798</td>\n",
       "      <td>0.017491</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.009958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097652</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100}</td>\n",
       "      <td>0.850063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089851</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000}</td>\n",
       "      <td>0.851124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129291</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000}</td>\n",
       "      <td>0.851236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.092042</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000000}</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144359</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000000}</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                    params  mean_test_AUC  \\\n",
       "0        0.079499      {'gaussiannb__var_smoothing': 1e-10}       0.821454   \n",
       "1        0.062663      {'gaussiannb__var_smoothing': 1e-08}       0.821454   \n",
       "2        0.093762      {'gaussiannb__var_smoothing': 1e-06}       0.821454   \n",
       "3        0.121751     {'gaussiannb__var_smoothing': 0.0001}       0.821454   \n",
       "4        0.124741        {'gaussiannb__var_smoothing': 0.1}       0.821353   \n",
       "5        0.121096         {'gaussiannb__var_smoothing': 10}       0.841798   \n",
       "6        0.097652        {'gaussiannb__var_smoothing': 100}       0.850063   \n",
       "7        0.089851       {'gaussiannb__var_smoothing': 1000}       0.851124   \n",
       "8        0.129291     {'gaussiannb__var_smoothing': 100000}       0.851236   \n",
       "9        0.092042    {'gaussiannb__var_smoothing': 1000000}       0.851237   \n",
       "10       0.144359  {'gaussiannb__var_smoothing': 100000000}       0.851237   \n",
       "\n",
       "    mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0            0.495376               0.649351              9   \n",
       "1            0.495376               0.649351              9   \n",
       "2            0.495376               0.649351              8   \n",
       "3            0.495452               0.649351              7   \n",
       "4            0.484189               0.574929             11   \n",
       "5            0.017491               0.008909              6   \n",
       "6            0.000000               0.000000              5   \n",
       "7            0.000000               0.000000              4   \n",
       "8            0.000000               0.000000              3   \n",
       "9            0.000000               0.000000              1   \n",
       "10           0.000000               0.000000              1   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   2                      1        0.835938   \n",
       "1                   2                      1        0.835938   \n",
       "2                   2                      1        0.835938   \n",
       "3                   1                      1        0.835938   \n",
       "4                   5                      5        0.834098   \n",
       "5                   6                      6        0.846939   \n",
       "6                   7                      7        0.854538   \n",
       "7                   7                      7        0.855518   \n",
       "8                   7                      7        0.855632   \n",
       "9                   7                      7        0.855633   \n",
       "10                  7                      7        0.855633   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.516555                0.691474  \n",
       "1             0.516555                0.691474  \n",
       "2             0.516555                0.691474  \n",
       "3             0.516617                0.691474  \n",
       "4             0.516355                0.612509  \n",
       "5             0.019630                0.009958  \n",
       "6             0.000000                0.000000  \n",
       "7             0.000000                0.000000  \n",
       "8             0.000000                0.000000  \n",
       "9             0.000000                0.000000  \n",
       "10            0.000000                0.000000  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_gnb = make_table(data)\n",
    "table_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gaussiannb__var_smoothing': 0.0001}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'adaboostclassifier__n_estimators': (3, 10, 15, 20, 50),\n",
    "    'adaboostclassifier__learning_rate': (0.1, 0.15, 0.2, 0.25, 0.5),\n",
    "    'adaboostclassifier__algorithm': ('SAMME', 'SAMME.R'),\n",
    "    'adaboostclassifier__random_state': (0, 1)\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), AdaBoostClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_adaboostclassifier__algorithm</th>\n",
       "      <th>param_adaboostclassifier__learning_rate</th>\n",
       "      <th>param_adaboostclassifier__n_estimators</th>\n",
       "      <th>param_adaboostclassifier__random_state</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156501</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.049008</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>0.139308</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.788959</td>\n",
       "      <td>0.037106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196256</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>0.068751</td>\n",
       "      <td>0.017856</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>0.139308</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.788959</td>\n",
       "      <td>0.037106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.368767</td>\n",
       "      <td>0.066866</td>\n",
       "      <td>0.070736</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.354273</td>\n",
       "      <td>0.030717</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.336007</td>\n",
       "      <td>0.020204</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.114104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.362008</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>0.087259</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.114104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.446495</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>0.124510</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.771488</td>\n",
       "      <td>0.534067</td>\n",
       "      <td>0.310510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.489993</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>0.128265</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.771488</td>\n",
       "      <td>0.534067</td>\n",
       "      <td>0.310510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.325745</td>\n",
       "      <td>0.089315</td>\n",
       "      <td>0.250248</td>\n",
       "      <td>0.020525</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>0.259764</td>\n",
       "      <td>73</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.569532</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>0.093706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.029241</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.224010</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>0.259764</td>\n",
       "      <td>73</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.569532</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>0.093706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.122994</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>0.150175</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.776730</td>\n",
       "      <td>0.026731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150757</td>\n",
       "      <td>0.019215</td>\n",
       "      <td>0.075494</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>0.150175</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.776730</td>\n",
       "      <td>0.026731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.248010</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>0.070739</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.247001</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.071256</td>\n",
       "      <td>0.009980</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>0.087993</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>0.286703</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.538609</td>\n",
       "      <td>0.311503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.343245</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.087011</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>0.286703</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.538609</td>\n",
       "      <td>0.311503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.457270</td>\n",
       "      <td>0.020698</td>\n",
       "      <td>0.116230</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>0.273563</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.305858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.433242</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>0.106762</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>0.273563</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.305858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.370501</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.285247</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>0.289439</td>\n",
       "      <td>91</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.567435</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.467680</td>\n",
       "      <td>0.099647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.107773</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>0.289439</td>\n",
       "      <td>91</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.567435</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.467680</td>\n",
       "      <td>0.099647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.135008</td>\n",
       "      <td>0.015243</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>0.188299</td>\n",
       "      <td>15</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.041803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.132990</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>0.050749</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>0.188299</td>\n",
       "      <td>15</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.041803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.252509</td>\n",
       "      <td>0.010426</td>\n",
       "      <td>0.072990</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>0.268381</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.304013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.282499</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.069998</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>0.268381</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.304013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.353754</td>\n",
       "      <td>0.015236</td>\n",
       "      <td>0.085510</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>0.281047</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.513976</td>\n",
       "      <td>0.301963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.347507</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.086492</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>0.281047</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.513976</td>\n",
       "      <td>0.301963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.437012</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.106247</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>0.278356</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.722572</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.510133</td>\n",
       "      <td>0.296173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.461752</td>\n",
       "      <td>0.049831</td>\n",
       "      <td>0.106005</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>0.278356</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.722572</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.510133</td>\n",
       "      <td>0.296173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.971259</td>\n",
       "      <td>0.033903</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.975218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>0.277387</td>\n",
       "      <td>53</td>\n",
       "      <td>0.482180</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.594689</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.543501</td>\n",
       "      <td>0.046589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.987249</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>0.215497</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.975218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>0.277387</td>\n",
       "      <td>53</td>\n",
       "      <td>0.482180</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.594689</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.543501</td>\n",
       "      <td>0.046589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.168505</td>\n",
       "      <td>0.010366</td>\n",
       "      <td>0.068998</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.180498</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>0.057015</td>\n",
       "      <td>0.010241</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.344006</td>\n",
       "      <td>0.021806</td>\n",
       "      <td>0.106246</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.973914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>0.289039</td>\n",
       "      <td>89</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.404612</td>\n",
       "      <td>0.387142</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.354752</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.973914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>0.289039</td>\n",
       "      <td>89</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.404612</td>\n",
       "      <td>0.387142</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.378764</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.114990</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>0.287805</td>\n",
       "      <td>93</td>\n",
       "      <td>0.466806</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.398323</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.077371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.387517</td>\n",
       "      <td>0.025833</td>\n",
       "      <td>0.109238</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>0.287805</td>\n",
       "      <td>93</td>\n",
       "      <td>0.466806</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.398323</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.077371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.489513</td>\n",
       "      <td>0.021975</td>\n",
       "      <td>0.137992</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>0.260063</td>\n",
       "      <td>79</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.571628</td>\n",
       "      <td>0.552061</td>\n",
       "      <td>0.527254</td>\n",
       "      <td>0.038458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.503252</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>0.260063</td>\n",
       "      <td>79</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.571628</td>\n",
       "      <td>0.552061</td>\n",
       "      <td>0.527254</td>\n",
       "      <td>0.038458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.389000</td>\n",
       "      <td>0.094072</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.028454</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.977763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>0.273470</td>\n",
       "      <td>51</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.578616</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.590496</td>\n",
       "      <td>0.583159</td>\n",
       "      <td>0.030710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.157256</td>\n",
       "      <td>0.028560</td>\n",
       "      <td>0.432507</td>\n",
       "      <td>0.107543</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.977763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>0.273470</td>\n",
       "      <td>51</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.578616</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.590496</td>\n",
       "      <td>0.583159</td>\n",
       "      <td>0.030710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.166496</td>\n",
       "      <td>0.040743</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.970175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>0.122974</td>\n",
       "      <td>17</td>\n",
       "      <td>0.624039</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.708421</td>\n",
       "      <td>0.059607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.123497</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.970175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>0.122974</td>\n",
       "      <td>17</td>\n",
       "      <td>0.624039</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.708421</td>\n",
       "      <td>0.059607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.266008</td>\n",
       "      <td>0.010069</td>\n",
       "      <td>0.088001</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.974354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>0.277497</td>\n",
       "      <td>95</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.640811</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.490915</td>\n",
       "      <td>0.124339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.298004</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.102259</td>\n",
       "      <td>0.011716</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.974354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>0.277497</td>\n",
       "      <td>95</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.640811</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.490915</td>\n",
       "      <td>0.124339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.420750</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.125497</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>0.266503</td>\n",
       "      <td>67</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.565863</td>\n",
       "      <td>0.051355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.377755</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.010736</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>0.266503</td>\n",
       "      <td>67</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.565863</td>\n",
       "      <td>0.051355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.510499</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.150750</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>69</td>\n",
       "      <td>0.505241</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587701</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.549441</td>\n",
       "      <td>0.036343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.618006</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>69</td>\n",
       "      <td>0.505241</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587701</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.549441</td>\n",
       "      <td>0.036343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.620758</td>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.297504</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.270662</td>\n",
       "      <td>61</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.575122</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.598882</td>\n",
       "      <td>0.583857</td>\n",
       "      <td>0.026750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.243498</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.329497</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.270662</td>\n",
       "      <td>61</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.575122</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.598882</td>\n",
       "      <td>0.583857</td>\n",
       "      <td>0.026750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.141492</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.972684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>0.240801</td>\n",
       "      <td>99</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.343117</td>\n",
       "      <td>0.349406</td>\n",
       "      <td>0.384696</td>\n",
       "      <td>0.059384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.120506</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.055743</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.972684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>0.240801</td>\n",
       "      <td>99</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.343117</td>\n",
       "      <td>0.349406</td>\n",
       "      <td>0.384696</td>\n",
       "      <td>0.059384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.283014</td>\n",
       "      <td>0.014751</td>\n",
       "      <td>0.087988</td>\n",
       "      <td>0.008804</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>0.270415</td>\n",
       "      <td>63</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.539483</td>\n",
       "      <td>0.046320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.350001</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.092751</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>0.270415</td>\n",
       "      <td>63</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.539483</td>\n",
       "      <td>0.046320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.401513</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>0.270619</td>\n",
       "      <td>59</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.617750</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.578791</td>\n",
       "      <td>0.040029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.398753</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.117493</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>0.270619</td>\n",
       "      <td>59</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.617750</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.578791</td>\n",
       "      <td>0.040029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.540764</td>\n",
       "      <td>0.020495</td>\n",
       "      <td>0.143003</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>0.271576</td>\n",
       "      <td>49</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.020745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.530995</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.143266</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>0.271576</td>\n",
       "      <td>49</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.020745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.230754</td>\n",
       "      <td>0.026318</td>\n",
       "      <td>0.328504</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>0.273348</td>\n",
       "      <td>71</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.619846</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.591020</td>\n",
       "      <td>0.023508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.351996</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>0.464001</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>0.273348</td>\n",
       "      <td>71</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.619846</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.591020</td>\n",
       "      <td>0.023508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.156501      0.015208         0.049008        0.004058   \n",
       "1        0.196256      0.022158         0.068751        0.017856   \n",
       "2        0.368767      0.066866         0.070736        0.008678   \n",
       "3        0.354273      0.030717         0.066744        0.001778   \n",
       "4        0.336007      0.020204         0.085752        0.006415   \n",
       "5        0.362008      0.020871         0.087259        0.007717   \n",
       "6        0.446495      0.019841         0.124510        0.017552   \n",
       "7        0.489993      0.024610         0.128265        0.005767   \n",
       "8        1.325745      0.089315         0.250248        0.020525   \n",
       "9        1.029241      0.020517         0.224010        0.022558   \n",
       "10       0.122994      0.011021         0.054249        0.012654   \n",
       "11       0.150757      0.019215         0.075494        0.022909   \n",
       "12       0.248010      0.012231         0.070739        0.006830   \n",
       "13       0.247001      0.014278         0.071256        0.009980   \n",
       "14       0.370006      0.026975         0.087993        0.014729   \n",
       "15       0.343245      0.022064         0.087011        0.008653   \n",
       "16       0.457270      0.020698         0.116230        0.010020   \n",
       "17       0.433242      0.029575         0.106762        0.012076   \n",
       "18       1.370501      0.014257         0.285247        0.009751   \n",
       "19       1.107773      0.042858         0.219000        0.027040   \n",
       "20       0.135008      0.015243         0.050498        0.007152   \n",
       "21       0.132990      0.014701         0.050749        0.005253   \n",
       "22       0.252509      0.010426         0.072990        0.007780   \n",
       "23       0.282499      0.019343         0.069998        0.003242   \n",
       "24       0.353754      0.015236         0.085510        0.008315   \n",
       "25       0.347507      0.014652         0.086492        0.012050   \n",
       "26       0.437012      0.027505         0.106247        0.014572   \n",
       "27       0.461752      0.049831         0.106005        0.011595   \n",
       "28       0.971259      0.033903         0.218750        0.023670   \n",
       "29       0.987249      0.045759         0.215497        0.025590   \n",
       "..            ...           ...              ...             ...   \n",
       "70       0.168505      0.010366         0.068998        0.013514   \n",
       "71       0.180498      0.035166         0.057015        0.010241   \n",
       "72       0.344006      0.021806         0.106246        0.010735   \n",
       "73       0.354752      0.016529         0.091743        0.010641   \n",
       "74       0.378764      0.009237         0.114990        0.012137   \n",
       "75       0.387517      0.025833         0.109238        0.008927   \n",
       "76       0.489513      0.021975         0.137992        0.011679   \n",
       "77       0.503252      0.026729         0.134000        0.009907   \n",
       "78       1.389000      0.094072         0.453000        0.028454   \n",
       "79       1.157256      0.028560         0.432507        0.107543   \n",
       "80       0.166496      0.040743         0.059748        0.013162   \n",
       "81       0.123497      0.006098         0.053498        0.006028   \n",
       "82       0.266008      0.010069         0.088001        0.013468   \n",
       "83       0.298004      0.015407         0.102259        0.011716   \n",
       "84       0.420750      0.004962         0.125497        0.010739   \n",
       "85       0.377755      0.018836         0.109502        0.010736   \n",
       "86       0.510499      0.013903         0.150750        0.011525   \n",
       "87       0.618006      0.013979         0.189500        0.026257   \n",
       "88       1.620758      0.040519         0.297504        0.015976   \n",
       "89       1.243498      0.023100         0.329497        0.008074   \n",
       "90       0.141492      0.013532         0.057007        0.008685   \n",
       "91       0.120506      0.006185         0.055743        0.003696   \n",
       "92       0.283014      0.014751         0.087988        0.008804   \n",
       "93       0.350001      0.024221         0.092751        0.004818   \n",
       "94       0.401513      0.012292         0.112505        0.008952   \n",
       "95       0.398753      0.012428         0.117493        0.009650   \n",
       "96       0.540764      0.020495         0.143003        0.009674   \n",
       "97       0.530995      0.016802         0.143266        0.012136   \n",
       "98       1.230754      0.026318         0.328504        0.041100   \n",
       "99       1.351996      0.044238         0.464001        0.015536   \n",
       "\n",
       "   param_adaboostclassifier__algorithm  \\\n",
       "0                                SAMME   \n",
       "1                                SAMME   \n",
       "2                                SAMME   \n",
       "3                                SAMME   \n",
       "4                                SAMME   \n",
       "5                                SAMME   \n",
       "6                                SAMME   \n",
       "7                                SAMME   \n",
       "8                                SAMME   \n",
       "9                                SAMME   \n",
       "10                               SAMME   \n",
       "11                               SAMME   \n",
       "12                               SAMME   \n",
       "13                               SAMME   \n",
       "14                               SAMME   \n",
       "15                               SAMME   \n",
       "16                               SAMME   \n",
       "17                               SAMME   \n",
       "18                               SAMME   \n",
       "19                               SAMME   \n",
       "20                               SAMME   \n",
       "21                               SAMME   \n",
       "22                               SAMME   \n",
       "23                               SAMME   \n",
       "24                               SAMME   \n",
       "25                               SAMME   \n",
       "26                               SAMME   \n",
       "27                               SAMME   \n",
       "28                               SAMME   \n",
       "29                               SAMME   \n",
       "..                                 ...   \n",
       "70                             SAMME.R   \n",
       "71                             SAMME.R   \n",
       "72                             SAMME.R   \n",
       "73                             SAMME.R   \n",
       "74                             SAMME.R   \n",
       "75                             SAMME.R   \n",
       "76                             SAMME.R   \n",
       "77                             SAMME.R   \n",
       "78                             SAMME.R   \n",
       "79                             SAMME.R   \n",
       "80                             SAMME.R   \n",
       "81                             SAMME.R   \n",
       "82                             SAMME.R   \n",
       "83                             SAMME.R   \n",
       "84                             SAMME.R   \n",
       "85                             SAMME.R   \n",
       "86                             SAMME.R   \n",
       "87                             SAMME.R   \n",
       "88                             SAMME.R   \n",
       "89                             SAMME.R   \n",
       "90                             SAMME.R   \n",
       "91                             SAMME.R   \n",
       "92                             SAMME.R   \n",
       "93                             SAMME.R   \n",
       "94                             SAMME.R   \n",
       "95                             SAMME.R   \n",
       "96                             SAMME.R   \n",
       "97                             SAMME.R   \n",
       "98                             SAMME.R   \n",
       "99                             SAMME.R   \n",
       "\n",
       "   param_adaboostclassifier__learning_rate  \\\n",
       "0                                      0.1   \n",
       "1                                      0.1   \n",
       "2                                      0.1   \n",
       "3                                      0.1   \n",
       "4                                      0.1   \n",
       "5                                      0.1   \n",
       "6                                      0.1   \n",
       "7                                      0.1   \n",
       "8                                      0.1   \n",
       "9                                      0.1   \n",
       "10                                    0.15   \n",
       "11                                    0.15   \n",
       "12                                    0.15   \n",
       "13                                    0.15   \n",
       "14                                    0.15   \n",
       "15                                    0.15   \n",
       "16                                    0.15   \n",
       "17                                    0.15   \n",
       "18                                    0.15   \n",
       "19                                    0.15   \n",
       "20                                     0.2   \n",
       "21                                     0.2   \n",
       "22                                     0.2   \n",
       "23                                     0.2   \n",
       "24                                     0.2   \n",
       "25                                     0.2   \n",
       "26                                     0.2   \n",
       "27                                     0.2   \n",
       "28                                     0.2   \n",
       "29                                     0.2   \n",
       "..                                     ...   \n",
       "70                                     0.2   \n",
       "71                                     0.2   \n",
       "72                                     0.2   \n",
       "73                                     0.2   \n",
       "74                                     0.2   \n",
       "75                                     0.2   \n",
       "76                                     0.2   \n",
       "77                                     0.2   \n",
       "78                                     0.2   \n",
       "79                                     0.2   \n",
       "80                                    0.25   \n",
       "81                                    0.25   \n",
       "82                                    0.25   \n",
       "83                                    0.25   \n",
       "84                                    0.25   \n",
       "85                                    0.25   \n",
       "86                                    0.25   \n",
       "87                                    0.25   \n",
       "88                                    0.25   \n",
       "89                                    0.25   \n",
       "90                                     0.5   \n",
       "91                                     0.5   \n",
       "92                                     0.5   \n",
       "93                                     0.5   \n",
       "94                                     0.5   \n",
       "95                                     0.5   \n",
       "96                                     0.5   \n",
       "97                                     0.5   \n",
       "98                                     0.5   \n",
       "99                                     0.5   \n",
       "\n",
       "   param_adaboostclassifier__n_estimators  \\\n",
       "0                                       3   \n",
       "1                                       3   \n",
       "2                                      10   \n",
       "3                                      10   \n",
       "4                                      15   \n",
       "5                                      15   \n",
       "6                                      20   \n",
       "7                                      20   \n",
       "8                                      50   \n",
       "9                                      50   \n",
       "10                                      3   \n",
       "11                                      3   \n",
       "12                                     10   \n",
       "13                                     10   \n",
       "14                                     15   \n",
       "15                                     15   \n",
       "16                                     20   \n",
       "17                                     20   \n",
       "18                                     50   \n",
       "19                                     50   \n",
       "20                                      3   \n",
       "21                                      3   \n",
       "22                                     10   \n",
       "23                                     10   \n",
       "24                                     15   \n",
       "25                                     15   \n",
       "26                                     20   \n",
       "27                                     20   \n",
       "28                                     50   \n",
       "29                                     50   \n",
       "..                                    ...   \n",
       "70                                      3   \n",
       "71                                      3   \n",
       "72                                     10   \n",
       "73                                     10   \n",
       "74                                     15   \n",
       "75                                     15   \n",
       "76                                     20   \n",
       "77                                     20   \n",
       "78                                     50   \n",
       "79                                     50   \n",
       "80                                      3   \n",
       "81                                      3   \n",
       "82                                     10   \n",
       "83                                     10   \n",
       "84                                     15   \n",
       "85                                     15   \n",
       "86                                     20   \n",
       "87                                     20   \n",
       "88                                     50   \n",
       "89                                     50   \n",
       "90                                      3   \n",
       "91                                      3   \n",
       "92                                     10   \n",
       "93                                     10   \n",
       "94                                     15   \n",
       "95                                     15   \n",
       "96                                     20   \n",
       "97                                     20   \n",
       "98                                     50   \n",
       "99                                     50   \n",
       "\n",
       "   param_adaboostclassifier__random_state  \\\n",
       "0                                       0   \n",
       "1                                       1   \n",
       "2                                       0   \n",
       "3                                       1   \n",
       "4                                       0   \n",
       "5                                       1   \n",
       "6                                       0   \n",
       "7                                       1   \n",
       "8                                       0   \n",
       "9                                       1   \n",
       "10                                      0   \n",
       "11                                      1   \n",
       "12                                      0   \n",
       "13                                      1   \n",
       "14                                      0   \n",
       "15                                      1   \n",
       "16                                      0   \n",
       "17                                      1   \n",
       "18                                      0   \n",
       "19                                      1   \n",
       "20                                      0   \n",
       "21                                      1   \n",
       "22                                      0   \n",
       "23                                      1   \n",
       "24                                      0   \n",
       "25                                      1   \n",
       "26                                      0   \n",
       "27                                      1   \n",
       "28                                      0   \n",
       "29                                      1   \n",
       "..                                    ...   \n",
       "70                                      0   \n",
       "71                                      1   \n",
       "72                                      0   \n",
       "73                                      1   \n",
       "74                                      0   \n",
       "75                                      1   \n",
       "76                                      0   \n",
       "77                                      1   \n",
       "78                                      0   \n",
       "79                                      1   \n",
       "80                                      0   \n",
       "81                                      1   \n",
       "82                                      0   \n",
       "83                                      1   \n",
       "84                                      0   \n",
       "85                                      1   \n",
       "86                                      0   \n",
       "87                                      1   \n",
       "88                                      0   \n",
       "89                                      1   \n",
       "90                                      0   \n",
       "91                                      1   \n",
       "92                                      0   \n",
       "93                                      1   \n",
       "94                                      0   \n",
       "95                                      1   \n",
       "96                                      0   \n",
       "97                                      1   \n",
       "98                                      0   \n",
       "99                                      1   \n",
       "\n",
       "                                               params  split0_test_AUC  ...  \\\n",
       "0   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "1   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "2   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "3   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "4   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "5   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "6   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "7   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "8   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974442  ...   \n",
       "9   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974442  ...   \n",
       "10  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "11  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "12  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "13  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "14  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "15  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "16  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "17  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "18  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974272  ...   \n",
       "19  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974272  ...   \n",
       "20  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "21  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "22  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "23  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "24  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "25  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "26  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "27  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "28  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.975218  ...   \n",
       "29  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.975218  ...   \n",
       "..                                                ...              ...  ...   \n",
       "70  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.969984  ...   \n",
       "71  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.969984  ...   \n",
       "72  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.973914  ...   \n",
       "73  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.973914  ...   \n",
       "74  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975163  ...   \n",
       "75  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975163  ...   \n",
       "76  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976878  ...   \n",
       "77  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976878  ...   \n",
       "78  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.977763  ...   \n",
       "79  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.977763  ...   \n",
       "80  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.970175  ...   \n",
       "81  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.970175  ...   \n",
       "82  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.974354  ...   \n",
       "83  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.974354  ...   \n",
       "84  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975790  ...   \n",
       "85  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975790  ...   \n",
       "86  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976842  ...   \n",
       "87  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976842  ...   \n",
       "88  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978649  ...   \n",
       "89  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978649  ...   \n",
       "90  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.972684  ...   \n",
       "91  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.972684  ...   \n",
       "92  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976835  ...   \n",
       "93  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976835  ...   \n",
       "94  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978102  ...   \n",
       "95  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978102  ...   \n",
       "96  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978090  ...   \n",
       "97  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978090  ...   \n",
       "98  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978817  ...   \n",
       "99  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978817  ...   \n",
       "\n",
       "    split3_test_Sensitivity  mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                  0.618449               0.786182              0.139308   \n",
       "1                  0.618449               0.786182              0.139308   \n",
       "2                  0.618449               0.744252              0.114405   \n",
       "3                  0.618449               0.744252              0.114405   \n",
       "4                  0.618449               0.671390              0.038123   \n",
       "5                  0.618449               0.671390              0.038123   \n",
       "6                  0.618449               0.465390              0.275031   \n",
       "7                  0.618449               0.465390              0.275031   \n",
       "8                  0.618449               0.418204              0.259764   \n",
       "9                  0.618449               0.418204              0.259764   \n",
       "10                 0.618449               0.768890              0.150175   \n",
       "11                 0.618449               0.768890              0.150175   \n",
       "12                 0.618449               0.744252              0.114405   \n",
       "13                 0.618449               0.744252              0.114405   \n",
       "14                 0.618449               0.493688              0.286703   \n",
       "15                 0.618449               0.493688              0.286703   \n",
       "16                 0.618449               0.473244              0.273563   \n",
       "17                 0.618449               0.473244              0.273563   \n",
       "18                 0.618449               0.349534              0.289439   \n",
       "19                 0.618449               0.349534              0.289439   \n",
       "20                 0.563941               0.726968              0.188299   \n",
       "21                 0.563941               0.726968              0.188299   \n",
       "22                 0.563941               0.451766              0.268381   \n",
       "23                 0.563941               0.451766              0.268381   \n",
       "24                 0.563941               0.476912              0.281047   \n",
       "25                 0.563941               0.476912              0.281047   \n",
       "26                 0.651992               0.481629              0.278356   \n",
       "27                 0.651992               0.481629              0.278356   \n",
       "28                 0.651992               0.434447              0.277387   \n",
       "29                 0.651992               0.434447              0.277387   \n",
       "..                      ...                    ...                   ...   \n",
       "70                 0.618449               0.744252              0.114405   \n",
       "71                 0.618449               0.744252              0.114405   \n",
       "72                 0.618449               0.350059              0.289039   \n",
       "73                 0.618449               0.350059              0.289039   \n",
       "74                 0.618449               0.347962              0.287805   \n",
       "75                 0.618449               0.347962              0.287805   \n",
       "76                 0.618449               0.414534              0.260063   \n",
       "77                 0.618449               0.414534              0.260063   \n",
       "78                 0.647799               0.435497              0.273470   \n",
       "79                 0.647799               0.435497              0.273470   \n",
       "80                 0.563941               0.704942              0.122974   \n",
       "81                 0.563941               0.704942              0.122974   \n",
       "82                 0.563941               0.335910              0.277497   \n",
       "83                 0.563941               0.335910              0.277497   \n",
       "84                 0.649895               0.422395              0.266503   \n",
       "85                 0.649895               0.422395              0.266503   \n",
       "86                 0.649895               0.420824              0.262703   \n",
       "87                 0.649895               0.420824              0.262703   \n",
       "88                 0.647799               0.431829              0.270662   \n",
       "89                 0.647799               0.431829              0.270662   \n",
       "90                 0.536688               0.301324              0.240801   \n",
       "91                 0.536688               0.301324              0.240801   \n",
       "92                 0.643606               0.430256              0.270415   \n",
       "93                 0.643606               0.430256              0.270415   \n",
       "94                 0.645702               0.432877              0.270619   \n",
       "95                 0.645702               0.432877              0.270619   \n",
       "96                 0.647799               0.442836              0.271576   \n",
       "97                 0.647799               0.442836              0.271576   \n",
       "98                 0.645702               0.419772              0.273348   \n",
       "99                 0.645702               0.419772              0.273348   \n",
       "\n",
       "    rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                       1                  0.730957                  0.794549   \n",
       "1                       1                  0.730957                  0.794549   \n",
       "2                       9                  0.626136                  0.794549   \n",
       "3                       9                  0.626136                  0.794549   \n",
       "4                      21                  0.511530                  0.688330   \n",
       "5                      21                  0.511530                  0.688330   \n",
       "6                      37                  0.000000                  0.688330   \n",
       "7                      37                  0.000000                  0.688330   \n",
       "8                      73                  0.378057                  0.522013   \n",
       "9                      73                  0.378057                  0.522013   \n",
       "10                      3                  0.730957                  0.794549   \n",
       "11                      3                  0.730957                  0.794549   \n",
       "12                      9                  0.626136                  0.794549   \n",
       "13                      9                  0.626136                  0.794549   \n",
       "14                     27                  0.000000                  0.688330   \n",
       "15                     27                  0.000000                  0.688330   \n",
       "16                     35                  0.000000                  0.634521   \n",
       "17                     35                  0.000000                  0.634521   \n",
       "18                     91                  0.378057                  0.358491   \n",
       "19                     91                  0.378057                  0.358491   \n",
       "20                     15                  0.730957                  0.794549   \n",
       "21                     15                  0.730957                  0.794549   \n",
       "22                     39                  0.000000                  0.688330   \n",
       "23                     39                  0.000000                  0.688330   \n",
       "24                     33                  0.000000                  0.634521   \n",
       "25                     33                  0.000000                  0.634521   \n",
       "26                     31                  0.000000                  0.634521   \n",
       "27                     31                  0.000000                  0.634521   \n",
       "28                     53                  0.482180                  0.515024   \n",
       "29                     53                  0.482180                  0.515024   \n",
       "..                    ...                       ...                       ...   \n",
       "70                      9                  0.626136                  0.794549   \n",
       "71                      9                  0.626136                  0.794549   \n",
       "72                     89                  0.378057                  0.358491   \n",
       "73                     89                  0.378057                  0.358491   \n",
       "74                     93                  0.466806                  0.358491   \n",
       "75                     93                  0.466806                  0.358491   \n",
       "76                     79                  0.470999                  0.514326   \n",
       "77                     79                  0.470999                  0.514326   \n",
       "78                     51                  0.538784                  0.578616   \n",
       "79                     51                  0.538784                  0.578616   \n",
       "80                     17                  0.624039                  0.688330   \n",
       "81                     17                  0.624039                  0.688330   \n",
       "82                     95                  0.378057                  0.358491   \n",
       "83                     95                  0.378057                  0.358491   \n",
       "84                     67                  0.515723                  0.515024   \n",
       "85                     67                  0.515723                  0.515024   \n",
       "86                     69                  0.505241                  0.522013   \n",
       "87                     69                  0.505241                  0.522013   \n",
       "88                     61                  0.545073                  0.575122   \n",
       "89                     61                  0.545073                  0.575122   \n",
       "90                     99                  0.487072                  0.359189   \n",
       "91                     99                  0.487072                  0.359189   \n",
       "92                     63                  0.471698                  0.522013   \n",
       "93                     63                  0.471698                  0.522013   \n",
       "94                     59                  0.514326                  0.577219   \n",
       "95                     59                  0.514326                  0.577219   \n",
       "96                     49                  0.568134                  0.583508   \n",
       "97                     49                  0.568134                  0.583508   \n",
       "98                     71                  0.559050                  0.579315   \n",
       "99                     71                  0.559050                  0.579315   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.834382                  0.795947   \n",
       "1                   0.834382                  0.795947   \n",
       "2                   0.785465                  0.795947   \n",
       "3                   0.785465                  0.795947   \n",
       "4                   0.785465                  0.795947   \n",
       "5                   0.785465                  0.795947   \n",
       "6                   0.676450                  0.771488   \n",
       "7                   0.676450                  0.771488   \n",
       "8                   0.632425                  0.569532   \n",
       "9                   0.632425                  0.569532   \n",
       "10                  0.785465                  0.795947   \n",
       "11                  0.785465                  0.795947   \n",
       "12                  0.785465                  0.795947   \n",
       "13                  0.785465                  0.795947   \n",
       "14                  0.734451                  0.731656   \n",
       "15                  0.734451                  0.731656   \n",
       "16                  0.734451                  0.731656   \n",
       "17                  0.734451                  0.731656   \n",
       "18                  0.567435                  0.566737   \n",
       "19                  0.567435                  0.566737   \n",
       "20                  0.676450                  0.735849   \n",
       "21                  0.676450                  0.735849   \n",
       "22                  0.676450                  0.735849   \n",
       "23                  0.676450                  0.735849   \n",
       "24                  0.776380                  0.645003   \n",
       "25                  0.776380                  0.645003   \n",
       "26                  0.722572                  0.683438   \n",
       "27                  0.722572                  0.683438   \n",
       "28                  0.594689                  0.582110   \n",
       "29                  0.594689                  0.582110   \n",
       "..                       ...                       ...   \n",
       "70                  0.785465                  0.795947   \n",
       "71                  0.785465                  0.795947   \n",
       "72                  0.407407                  0.404612   \n",
       "73                  0.407407                  0.404612   \n",
       "74                  0.562544                  0.398323   \n",
       "75                  0.562544                  0.398323   \n",
       "76                  0.571628                  0.552061   \n",
       "77                  0.571628                  0.552061   \n",
       "78                  0.624738                  0.590496   \n",
       "79                  0.624738                  0.590496   \n",
       "80                  0.785465                  0.735849   \n",
       "81                  0.785465                  0.735849   \n",
       "82                  0.640811                  0.586303   \n",
       "83                  0.640811                  0.586303   \n",
       "84                  0.629630                  0.603075   \n",
       "85                  0.629630                  0.603075   \n",
       "86                  0.587701                  0.582809   \n",
       "87                  0.587701                  0.582809   \n",
       "88                  0.616352                  0.598882   \n",
       "89                  0.616352                  0.598882   \n",
       "90                  0.343117                  0.349406   \n",
       "91                  0.343117                  0.349406   \n",
       "92                  0.587002                  0.577219   \n",
       "93                  0.587002                  0.577219   \n",
       "94                  0.617750                  0.605870   \n",
       "95                  0.617750                  0.605870   \n",
       "96                  0.624738                  0.595388   \n",
       "97                  0.624738                  0.595388   \n",
       "98                  0.619846                  0.605870   \n",
       "99                  0.619846                  0.605870   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.788959               0.037106  \n",
       "1                 0.788959               0.037106  \n",
       "2                 0.750524               0.071928  \n",
       "3                 0.750524               0.071928  \n",
       "4                 0.695318               0.114104  \n",
       "5                 0.695318               0.114104  \n",
       "6                 0.534067               0.310510  \n",
       "7                 0.534067               0.310510  \n",
       "8                 0.525507               0.093706  \n",
       "9                 0.525507               0.093706  \n",
       "10                0.776730               0.026731  \n",
       "11                0.776730               0.026731  \n",
       "12                0.750524               0.071928  \n",
       "13                0.750524               0.071928  \n",
       "14                0.538609               0.311503  \n",
       "15                0.538609               0.311503  \n",
       "16                0.525157               0.305858  \n",
       "17                0.525157               0.305858  \n",
       "18                0.467680               0.099647  \n",
       "19                0.467680               0.099647  \n",
       "20                0.734451               0.041803  \n",
       "21                0.734451               0.041803  \n",
       "22                0.525157               0.304013  \n",
       "23                0.525157               0.304013  \n",
       "24                0.513976               0.301963  \n",
       "25                0.513976               0.301963  \n",
       "26                0.510133               0.296173  \n",
       "27                0.510133               0.296173  \n",
       "28                0.543501               0.046589  \n",
       "29                0.543501               0.046589  \n",
       "..                     ...                    ...  \n",
       "70                0.750524               0.071928  \n",
       "71                0.750524               0.071928  \n",
       "72                0.387142               0.020120  \n",
       "73                0.387142               0.020120  \n",
       "74                0.446541               0.077371  \n",
       "75                0.446541               0.077371  \n",
       "76                0.527254               0.038458  \n",
       "77                0.527254               0.038458  \n",
       "78                0.583159               0.030710  \n",
       "79                0.583159               0.030710  \n",
       "80                0.708421               0.059607  \n",
       "81                0.708421               0.059607  \n",
       "82                0.490915               0.124339  \n",
       "83                0.490915               0.124339  \n",
       "84                0.565863               0.051355  \n",
       "85                0.565863               0.051355  \n",
       "86                0.549441               0.036343  \n",
       "87                0.549441               0.036343  \n",
       "88                0.583857               0.026750  \n",
       "89                0.583857               0.026750  \n",
       "90                0.384696               0.059384  \n",
       "91                0.384696               0.059384  \n",
       "92                0.539483               0.046320  \n",
       "93                0.539483               0.046320  \n",
       "94                0.578791               0.040029  \n",
       "95                0.578791               0.040029  \n",
       "96                0.592942               0.020745  \n",
       "97                0.592942               0.020745  \n",
       "98                0.591020               0.023508  \n",
       "99                0.591020               0.023508  \n",
       "\n",
       "[100 rows x 48 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_abc= gs.cv_results_\n",
    "data = pandas.DataFrame(results_abc)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboostclassifier__algorithm': 'SAMME',\n",
       " 'adaboostclassifier__learning_rate': 0.1,\n",
       " 'adaboostclassifier__n_estimators': 3,\n",
       " 'adaboostclassifier__random_state': 0}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156501</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.658165</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.666451</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196256</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.658165</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.666451</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.368767</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.853937</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.855969</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.354273</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.853937</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.855969</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.336007</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856433</td>\n",
       "      <td>0.639291</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.858755</td>\n",
       "      <td>0.653211</td>\n",
       "      <td>0.695318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.362008</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856433</td>\n",
       "      <td>0.639291</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.858755</td>\n",
       "      <td>0.653211</td>\n",
       "      <td>0.695318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.446495</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856693</td>\n",
       "      <td>0.443687</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.882185</td>\n",
       "      <td>0.508378</td>\n",
       "      <td>0.534067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.489993</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856693</td>\n",
       "      <td>0.443687</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.882185</td>\n",
       "      <td>0.508378</td>\n",
       "      <td>0.534067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.325745</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.875720</td>\n",
       "      <td>0.423286</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>33</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>0.909647</td>\n",
       "      <td>0.594287</td>\n",
       "      <td>0.525507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.029241</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.875720</td>\n",
       "      <td>0.423286</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>33</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>0.909647</td>\n",
       "      <td>0.594287</td>\n",
       "      <td>0.525507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.122994</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843520</td>\n",
       "      <td>0.657874</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.666585</td>\n",
       "      <td>0.776730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150757</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843520</td>\n",
       "      <td>0.657874</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.666585</td>\n",
       "      <td>0.776730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.248010</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854374</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.856960</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.247001</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854374</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.856960</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.858815</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>65</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.891960</td>\n",
       "      <td>0.512983</td>\n",
       "      <td>0.538609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.343245</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.858815</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>65</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.891960</td>\n",
       "      <td>0.512983</td>\n",
       "      <td>0.538609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.457270</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860518</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>0.904527</td>\n",
       "      <td>0.509098</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.433242</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860518</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>0.904527</td>\n",
       "      <td>0.509098</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.370501</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880423</td>\n",
       "      <td>0.345517</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>21</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>0.913291</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.467680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.107773</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880423</td>\n",
       "      <td>0.345517</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>21</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>0.913291</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.467680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.135008</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.846258</td>\n",
       "      <td>0.647820</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>0.851212</td>\n",
       "      <td>0.661795</td>\n",
       "      <td>0.734451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.132990</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.846258</td>\n",
       "      <td>0.647820</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>0.851212</td>\n",
       "      <td>0.661795</td>\n",
       "      <td>0.734451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.252509</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854708</td>\n",
       "      <td>0.441291</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>75</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>0.880532</td>\n",
       "      <td>0.506505</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.282499</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854708</td>\n",
       "      <td>0.441291</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>75</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>0.880532</td>\n",
       "      <td>0.506505</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.353754</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860456</td>\n",
       "      <td>0.449162</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>0.903854</td>\n",
       "      <td>0.505644</td>\n",
       "      <td>0.513976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.347507</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860456</td>\n",
       "      <td>0.449162</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>0.903854</td>\n",
       "      <td>0.505644</td>\n",
       "      <td>0.513976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.437012</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.869285</td>\n",
       "      <td>0.453064</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.908527</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.510133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.461752</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.869285</td>\n",
       "      <td>0.453064</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.908527</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.510133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.971259</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880774</td>\n",
       "      <td>0.424548</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.607536</td>\n",
       "      <td>0.543501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.987249</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880774</td>\n",
       "      <td>0.424548</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.607536</td>\n",
       "      <td>0.543501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.168505</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.849465</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.851323</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.180498</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.849465</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.851323</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.344006</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.862814</td>\n",
       "      <td>0.344632</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>53</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>0.905575</td>\n",
       "      <td>0.489506</td>\n",
       "      <td>0.387142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.354752</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.862814</td>\n",
       "      <td>0.344632</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>53</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>0.905575</td>\n",
       "      <td>0.489506</td>\n",
       "      <td>0.387142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.378764</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.875492</td>\n",
       "      <td>0.345363</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>35</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.446541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.387517</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.875492</td>\n",
       "      <td>0.345363</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>35</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.446541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.489513</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.419785</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>0.918729</td>\n",
       "      <td>0.598195</td>\n",
       "      <td>0.527254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.503252</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.419785</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>0.918729</td>\n",
       "      <td>0.598195</td>\n",
       "      <td>0.527254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.389000</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.885628</td>\n",
       "      <td>0.422660</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>51</td>\n",
       "      <td>0.925995</td>\n",
       "      <td>0.636283</td>\n",
       "      <td>0.583159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.157256</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.885628</td>\n",
       "      <td>0.422660</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>51</td>\n",
       "      <td>0.925995</td>\n",
       "      <td>0.636283</td>\n",
       "      <td>0.583159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.166496</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.852996</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0.854229</td>\n",
       "      <td>0.660239</td>\n",
       "      <td>0.708421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.123497</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.852996</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0.854229</td>\n",
       "      <td>0.660239</td>\n",
       "      <td>0.708421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.266008</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.865025</td>\n",
       "      <td>0.341528</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>49</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>0.564754</td>\n",
       "      <td>0.490915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.298004</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.865025</td>\n",
       "      <td>0.341528</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>49</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>0.564754</td>\n",
       "      <td>0.490915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.420750</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878162</td>\n",
       "      <td>0.423262</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>0.917192</td>\n",
       "      <td>0.622853</td>\n",
       "      <td>0.565863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.377755</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878162</td>\n",
       "      <td>0.423262</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>0.917192</td>\n",
       "      <td>0.622853</td>\n",
       "      <td>0.565863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.510499</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.880590</td>\n",
       "      <td>0.423816</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>19</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.616510</td>\n",
       "      <td>0.549441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.618006</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.880590</td>\n",
       "      <td>0.423816</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>19</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.616510</td>\n",
       "      <td>0.549441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.620758</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.422198</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>61</td>\n",
       "      <td>0.927662</td>\n",
       "      <td>0.639455</td>\n",
       "      <td>0.583857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.243498</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.422198</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>61</td>\n",
       "      <td>0.927662</td>\n",
       "      <td>0.639455</td>\n",
       "      <td>0.583857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.141492</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.853406</td>\n",
       "      <td>0.332148</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>81</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0.899946</td>\n",
       "      <td>0.486668</td>\n",
       "      <td>0.384696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.120506</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.853406</td>\n",
       "      <td>0.332148</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>81</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0.899946</td>\n",
       "      <td>0.486668</td>\n",
       "      <td>0.384696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.283014</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878403</td>\n",
       "      <td>0.424421</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.539483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.350001</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878403</td>\n",
       "      <td>0.424421</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.539483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.401513</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.886102</td>\n",
       "      <td>0.426331</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>0.924347</td>\n",
       "      <td>0.633775</td>\n",
       "      <td>0.578791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.398753</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.886102</td>\n",
       "      <td>0.426331</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>0.924347</td>\n",
       "      <td>0.633775</td>\n",
       "      <td>0.578791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.540764</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.882417</td>\n",
       "      <td>0.429917</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.926289</td>\n",
       "      <td>0.641037</td>\n",
       "      <td>0.592942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.530995</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.882417</td>\n",
       "      <td>0.429917</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.926289</td>\n",
       "      <td>0.641037</td>\n",
       "      <td>0.592942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.230754</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.868296</td>\n",
       "      <td>0.406926</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>45</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>0.930508</td>\n",
       "      <td>0.645410</td>\n",
       "      <td>0.591020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.351996</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.868296</td>\n",
       "      <td>0.406926</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>45</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>0.930508</td>\n",
       "      <td>0.645410</td>\n",
       "      <td>0.591020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        0.156501  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "1        0.196256  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "2        0.368767  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "3        0.354273  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "4        0.336007  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "5        0.362008  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "6        0.446495  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "7        0.489993  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "8        1.325745  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "9        1.029241  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "10       0.122994  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "11       0.150757  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "12       0.248010  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "13       0.247001  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "14       0.370006  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "15       0.343245  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "16       0.457270  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "17       0.433242  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "18       1.370501  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "19       1.107773  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "20       0.135008  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "21       0.132990  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "22       0.252509  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "23       0.282499  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "24       0.353754  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "25       0.347507  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "26       0.437012  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "27       0.461752  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "28       0.971259  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "29       0.987249  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "..            ...                                                ...   \n",
       "70       0.168505  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "71       0.180498  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "72       0.344006  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "73       0.354752  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "74       0.378764  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "75       0.387517  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "76       0.489513  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "77       0.503252  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "78       1.389000  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "79       1.157256  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "80       0.166496  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "81       0.123497  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "82       0.266008  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "83       0.298004  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "84       0.420750  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "85       0.377755  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "86       0.510499  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "87       0.618006  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "88       1.620758  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "89       1.243498  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "90       0.141492  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "91       0.120506  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "92       0.283014  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "93       0.350001  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "94       0.401513  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "95       0.398753  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "96       0.540764  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "97       0.530995  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "98       1.230754  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "99       1.351996  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.843328           0.658165               0.786182             99   \n",
       "1        0.843328           0.658165               0.786182             99   \n",
       "2        0.853937           0.655670               0.744252             79   \n",
       "3        0.853937           0.655670               0.744252             79   \n",
       "4        0.856433           0.639291               0.671390             71   \n",
       "5        0.856433           0.639291               0.671390             71   \n",
       "6        0.856693           0.443687               0.465390             69   \n",
       "7        0.856693           0.443687               0.465390             69   \n",
       "8        0.875720           0.423286               0.418204             33   \n",
       "9        0.875720           0.423286               0.418204             33   \n",
       "10       0.843520           0.657874               0.768890             97   \n",
       "11       0.843520           0.657874               0.768890             97   \n",
       "12       0.854374           0.655670               0.744252             77   \n",
       "13       0.854374           0.655670               0.744252             77   \n",
       "14       0.858815           0.451345               0.493688             65   \n",
       "15       0.858815           0.451345               0.493688             65   \n",
       "16       0.860518           0.451267               0.473244             57   \n",
       "17       0.860518           0.451267               0.473244             57   \n",
       "18       0.880423           0.345517               0.349534             21   \n",
       "19       0.880423           0.345517               0.349534             21   \n",
       "20       0.846258           0.647820               0.726968             93   \n",
       "21       0.846258           0.647820               0.726968             93   \n",
       "22       0.854708           0.441291               0.451766             75   \n",
       "23       0.854708           0.441291               0.451766             75   \n",
       "24       0.860456           0.449162               0.476912             59   \n",
       "25       0.860456           0.449162               0.476912             59   \n",
       "26       0.869285           0.453064               0.481629             43   \n",
       "27       0.869285           0.453064               0.481629             43   \n",
       "28       0.880774           0.424548               0.434447             17   \n",
       "29       0.880774           0.424548               0.434447             17   \n",
       "..            ...                ...                    ...            ...   \n",
       "70       0.849465           0.655670               0.744252             87   \n",
       "71       0.849465           0.655670               0.744252             87   \n",
       "72       0.862814           0.344632               0.350059             53   \n",
       "73       0.862814           0.344632               0.350059             53   \n",
       "74       0.875492           0.345363               0.347962             35   \n",
       "75       0.875492           0.345363               0.347962             35   \n",
       "76       0.878974           0.419785               0.414534             23   \n",
       "77       0.878974           0.419785               0.414534             23   \n",
       "78       0.885628           0.422660               0.435497              5   \n",
       "79       0.885628           0.422660               0.435497              5   \n",
       "80       0.852996           0.654837               0.704942             83   \n",
       "81       0.852996           0.654837               0.704942             83   \n",
       "82       0.865025           0.341528               0.335910             49   \n",
       "83       0.865025           0.341528               0.335910             49   \n",
       "84       0.878162           0.423262               0.422395             27   \n",
       "85       0.878162           0.423262               0.422395             27   \n",
       "86       0.880590           0.423816               0.420824             19   \n",
       "87       0.880590           0.423816               0.420824             19   \n",
       "88       0.884374           0.422198               0.431829              7   \n",
       "89       0.884374           0.422198               0.431829              7   \n",
       "90       0.853406           0.332148               0.301324             81   \n",
       "91       0.853406           0.332148               0.301324             81   \n",
       "92       0.878403           0.424421               0.430256             25   \n",
       "93       0.878403           0.424421               0.430256             25   \n",
       "94       0.886102           0.426331               0.432877              1   \n",
       "95       0.886102           0.426331               0.432877              1   \n",
       "96       0.882417           0.429917               0.442836             11   \n",
       "97       0.882417           0.429917               0.442836             11   \n",
       "98       0.868296           0.406926               0.419772             45   \n",
       "99       0.868296           0.406926               0.419772             45   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   1                      1        0.847761   \n",
       "1                   1                      1        0.847761   \n",
       "2                   9                      9        0.855969   \n",
       "3                   9                      9        0.855969   \n",
       "4                  21                     21        0.858755   \n",
       "5                  21                     21        0.858755   \n",
       "6                  37                     37        0.882185   \n",
       "7                  37                     37        0.882185   \n",
       "8                  61                     73        0.909647   \n",
       "9                  61                     73        0.909647   \n",
       "10                  3                      3        0.847966   \n",
       "11                  3                      3        0.847966   \n",
       "12                  9                      9        0.856960   \n",
       "13                  9                      9        0.856960   \n",
       "14                 29                     27        0.891960   \n",
       "15                 29                     27        0.891960   \n",
       "16                 31                     35        0.904527   \n",
       "17                 31                     35        0.904527   \n",
       "18                 87                     91        0.913291   \n",
       "19                 87                     91        0.913291   \n",
       "20                 19                     15        0.851212   \n",
       "21                 19                     15        0.851212   \n",
       "22                 43                     39        0.880532   \n",
       "23                 43                     39        0.880532   \n",
       "24                 35                     33        0.903854   \n",
       "25                 35                     33        0.903854   \n",
       "26                 25                     31        0.908527   \n",
       "27                 25                     31        0.908527   \n",
       "28                 53                     53        0.915354   \n",
       "29                 53                     53        0.915354   \n",
       "..                ...                    ...             ...   \n",
       "70                  9                      9        0.851323   \n",
       "71                  9                      9        0.851323   \n",
       "72                 93                     89        0.905575   \n",
       "73                 93                     89        0.905575   \n",
       "74                 89                     93        0.912598   \n",
       "75                 89                     93        0.912598   \n",
       "76                 79                     79        0.918729   \n",
       "77                 79                     79        0.918729   \n",
       "78                 69                     51        0.925995   \n",
       "79                 69                     51        0.925995   \n",
       "80                 15                     17        0.854229   \n",
       "81                 15                     17        0.854229   \n",
       "82                 95                     95        0.909961   \n",
       "83                 95                     95        0.909961   \n",
       "84                 63                     67        0.917192   \n",
       "85                 63                     67        0.917192   \n",
       "86                 57                     69        0.922842   \n",
       "87                 57                     69        0.922842   \n",
       "88                 75                     61        0.927662   \n",
       "89                 75                     61        0.927662   \n",
       "90                 99                     99        0.899946   \n",
       "91                 99                     99        0.899946   \n",
       "92                 55                     63        0.920777   \n",
       "93                 55                     63        0.920777   \n",
       "94                 51                     59        0.924347   \n",
       "95                 51                     59        0.924347   \n",
       "96                 49                     49        0.926289   \n",
       "97                 49                     49        0.926289   \n",
       "98                 83                     71        0.930508   \n",
       "99                 83                     71        0.930508   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.666451                0.788959  \n",
       "1             0.666451                0.788959  \n",
       "2             0.664336                0.750524  \n",
       "3             0.664336                0.750524  \n",
       "4             0.653211                0.695318  \n",
       "5             0.653211                0.695318  \n",
       "6             0.508378                0.534067  \n",
       "7             0.508378                0.534067  \n",
       "8             0.594287                0.525507  \n",
       "9             0.594287                0.525507  \n",
       "10            0.666585                0.776730  \n",
       "11            0.666585                0.776730  \n",
       "12            0.664336                0.750524  \n",
       "13            0.664336                0.750524  \n",
       "14            0.512983                0.538609  \n",
       "15            0.512983                0.538609  \n",
       "16            0.509098                0.525157  \n",
       "17            0.509098                0.525157  \n",
       "18            0.553363                0.467680  \n",
       "19            0.553363                0.467680  \n",
       "20            0.661795                0.734451  \n",
       "21            0.661795                0.734451  \n",
       "22            0.506505                0.525157  \n",
       "23            0.506505                0.525157  \n",
       "24            0.505644                0.513976  \n",
       "25            0.505644                0.513976  \n",
       "26            0.506344                0.510133  \n",
       "27            0.506344                0.510133  \n",
       "28            0.607536                0.543501  \n",
       "29            0.607536                0.543501  \n",
       "..                 ...                     ...  \n",
       "70            0.664336                0.750524  \n",
       "71            0.664336                0.750524  \n",
       "72            0.489506                0.387142  \n",
       "73            0.489506                0.387142  \n",
       "74            0.534591                0.446541  \n",
       "75            0.534591                0.446541  \n",
       "76            0.598195                0.527254  \n",
       "77            0.598195                0.527254  \n",
       "78            0.636283                0.583159  \n",
       "79            0.636283                0.583159  \n",
       "80            0.660239                0.708421  \n",
       "81            0.660239                0.708421  \n",
       "82            0.564754                0.490915  \n",
       "83            0.564754                0.490915  \n",
       "84            0.622853                0.565863  \n",
       "85            0.622853                0.565863  \n",
       "86            0.616510                0.549441  \n",
       "87            0.616510                0.549441  \n",
       "88            0.639455                0.583857  \n",
       "89            0.639455                0.583857  \n",
       "90            0.486668                0.384696  \n",
       "91            0.486668                0.384696  \n",
       "92            0.606900                0.539483  \n",
       "93            0.606900                0.539483  \n",
       "94            0.633775                0.578791  \n",
       "95            0.633775                0.578791  \n",
       "96            0.641037                0.592942  \n",
       "97            0.641037                0.592942  \n",
       "98            0.645410                0.591020  \n",
       "99            0.645410                0.591020  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_abc = make_table(data)\n",
    "table_abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.156501</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.658165</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.666451</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.836223</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888507</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.952666</td>\n",
       "      <td>0.715934</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.477503</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.896360</td>\n",
       "      <td>0.558226</td>\n",
       "      <td>0.487417</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>0.922572</td>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.573375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.143254</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>0.505637</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.542564</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.121751</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.0001}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495452</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516617</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.054504</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.791784</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806834</td>\n",
       "      <td>0.464271</td>\n",
       "      <td>0.664396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.129594</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.688934</td>\n",
       "      <td>0.426242</td>\n",
       "      <td>0.431863</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653024</td>\n",
       "      <td>0.430286</td>\n",
       "      <td>0.428022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123743</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.0}</td>\n",
       "      <td>0.822635</td>\n",
       "      <td>0.333549</td>\n",
       "      <td>0.223790</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943239</td>\n",
       "      <td>0.578753</td>\n",
       "      <td>0.414046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                             params  \\\n",
       "7       0.156501  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "2       1.836223  {'gradientboostingclassifier__learning_rate': ...   \n",
       "8      16.477503  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "3       0.143254  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "6       0.121751              {'gaussiannb__var_smoothing': 0.0001}   \n",
       "5       0.054504  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "1      16.129594  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "4       0.123743                  {'kdeclassifier__bandwidth': 1.0}   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity rank_test_AUC  \\\n",
       "7       0.843328           0.658165               0.786182            99   \n",
       "2       0.888507           0.564609               0.538248             2   \n",
       "8       0.896360           0.558226               0.487417             1   \n",
       "3       0.822152           0.505637               0.581745             7   \n",
       "6       0.821454           0.495452               0.649351             7   \n",
       "5       0.791784           0.442446               0.639914            15   \n",
       "1       0.688934           0.426242               0.431863            11   \n",
       "4       0.822635           0.333549               0.223790            10   \n",
       "\n",
       "  rank_test_F_score rank_test_Sensitivity  mean_train_AUC  mean_train_F_score  \\\n",
       "7                 1                     1        0.847761            0.666451   \n",
       "2                 1                     2        0.952666            0.715934   \n",
       "8                16                    19        0.922572            0.641730   \n",
       "3                 1                     5        0.846864            0.542564   \n",
       "6                 1                     1        0.835938            0.516617   \n",
       "5                 1                    13        0.806834            0.464271   \n",
       "1                 7                     2        0.653024            0.430286   \n",
       "4                 1                     1        0.943239            0.578753   \n",
       "\n",
       "   mean_train_Sensitivity  \n",
       "7                0.788959  \n",
       "2                0.662648  \n",
       "8                0.573375  \n",
       "3                0.620720  \n",
       "6                0.691474  \n",
       "5                0.664396  \n",
       "1                0.428022  \n",
       "4                0.414046  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.DataFrame(columns = [\"mean_fit_time\", \"params\", \"mean_test_AUC\", \"mean_test_F_score\", \"mean_test_Sensitivity\",\n",
    "                                 \"rank_test_AUC\", \"rank_test_F_score\", \"rank_test_Sensitivity\", \"mean_train_AUC\", \n",
    "                                 \"mean_train_F_score\", \"mean_train_Sensitivity\"])\n",
    "\n",
    "df.loc[1] = table_svc.iloc[6]\n",
    "df.loc[2] = table_gbt.iloc[21]\n",
    "df.loc[3] = table_qda.iloc[5]\n",
    "df.loc[4] = table_kd.iloc[0]\n",
    "df.loc[5] = table_cnb.iloc[8]\n",
    "df.loc[6] = table_gnb.iloc[3]\n",
    "df.loc[7] = table_abc.iloc[0]\n",
    "df.loc[8] = table_mlp.iloc[25]\n",
    "\n",
    "df = df.sort_values(by=['mean_test_F_score'], ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selección de características\n",
    "\n",
    "Para esta sección es necesario correr nuevamente la primeras casillas (antes de la sección de modelos) para reasignar los valores a data ya que esta fue modificada durante la sección de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_1</th>\n",
       "      <th>variable_2</th>\n",
       "      <th>r</th>\n",
       "      <th>abs_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BounceRates</td>\n",
       "      <td>ExitRates</td>\n",
       "      <td>0.913004</td>\n",
       "      <td>0.913004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ExitRates</td>\n",
       "      <td>BounceRates</td>\n",
       "      <td>0.913004</td>\n",
       "      <td>0.913004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>PageValues</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>0.492569</td>\n",
       "      <td>0.492569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Revenue</td>\n",
       "      <td>PageValues</td>\n",
       "      <td>0.492569</td>\n",
       "      <td>0.492569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Administrative_Duration</td>\n",
       "      <td>ProductRelated_Duration</td>\n",
       "      <td>0.355422</td>\n",
       "      <td>0.355422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ProductRelated_Duration</td>\n",
       "      <td>Administrative_Duration</td>\n",
       "      <td>0.355422</td>\n",
       "      <td>0.355422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Informational_Duration</td>\n",
       "      <td>ProductRelated_Duration</td>\n",
       "      <td>0.347364</td>\n",
       "      <td>0.347364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ProductRelated_Duration</td>\n",
       "      <td>Informational_Duration</td>\n",
       "      <td>0.347364</td>\n",
       "      <td>0.347364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>SpecialDay</td>\n",
       "      <td>Month</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>0.286015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Month</td>\n",
       "      <td>SpecialDay</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>0.286015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  variable_1               variable_2         r     abs_r\n",
       "37               BounceRates                ExitRates  0.913004  0.913004\n",
       "47                 ExitRates              BounceRates  0.913004  0.913004\n",
       "65                PageValues                  Revenue  0.492569  0.492569\n",
       "115                  Revenue               PageValues  0.492569  0.492569\n",
       "2    Administrative_Duration  ProductRelated_Duration  0.355422  0.355422\n",
       "22   ProductRelated_Duration  Administrative_Duration  0.355422  0.355422\n",
       "13    Informational_Duration  ProductRelated_Duration  0.347364  0.347364\n",
       "23   ProductRelated_Duration   Informational_Duration  0.347364  0.347364\n",
       "73                SpecialDay                    Month -0.286015  0.286015\n",
       "83                     Month               SpecialDay -0.286015  0.286015"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def tidy_corr_matrix(corr_mat):\n",
    "    '''\n",
    "    Función para convertir una matrix de correlación de pandas en formato tidy\n",
    "    '''\n",
    "    corr_mat = corr_mat.stack().reset_index()\n",
    "    corr_mat.columns = ['variable_1','variable_2','r']\n",
    "    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n",
    "    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n",
    "    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n",
    "    \n",
    "    return(corr_mat)\n",
    "\n",
    "corr_matrix = data.select_dtypes(include=['float64', 'int']).corr(method='pearson')\n",
    "tidy_corr_matrix(corr_matrix).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAIPCAYAAAC8OuBnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3hU1dbH8e9CSIAkIDUBRToTehVQAUW9KlZEpQkYCEWRJtKRKiVSpUhNaAKKVxHrtV5QLBg6AcIgAoqAJJRrJpUk7PePcxgmIRBGJsT73vV5Hh5mzj7lN/tMWbP3GRBjDEoppZRS6toVyO8ASimllFL/bbSAUkoppZTykhZQSimllFJe0gJKKaWUUspLWkAppZRSSnlJCyillFJKKS8VzO8A6n9WeeBjoBYQCGTkVxCHwzEbaALscDqdAz2WLwbqAAbo63Q699jLBdgFzHM6nZH5nHEO0AAoDAx2Op3fOxyOksAioDTwtdPpnHwD8hUElgOVgY+dTmdEtvYFwDPASM8+czgc5YHDQB2n03koH/N9BNwMnAeeczqdvzscjtHAi8Ayp9P5Sh5mu9K5rYN1HgV4wel07snp3DocjmeAoVjP0ylOp/MDH+fLre/uBSYDqUBXu+822c3FgV+dTmfbnJ6rPszoTR+uAGoCKcASp9O51uFw3A1Mw+rDFU6nc5Gvsv2FfJc97xwOxz+AV+3MLzidzgO+zOejjCvI1q/5nDHHPvPl+7eOQKn8cha4D9iSnyEcDkcjIMDpdLYE/BwOx+0ezRFOp/MuoDswzmP540Dc3yTjEKfTeTfQHhhlLxsHjHU6nffeiOLJ9jgQ63Q6WwAtHA5HSLb2V7E+5LMbxI15DuSWb4DdvxHAS/aySODZvAyVy7l9FeiEdW5ftZfldG5fAu6x/wzOg5i59d0Y4AFgBDASwOl03uN0Ou8BVmF9UYKcn6vX7S/0IcCzdsaLH/IvYxX4d2K93n3mL+TL6Xk3Fuv9sjMwwZf5fJgRLu/X/Mx4pT7z2fv3XyqgRORJETEiEnqF9hUi8rQX+ysvIu9ew3qfisjNV2kfJCJFr/W4HtuFiUh5j/uRIlLL2/1cZf9HRSTG/rNfRCaJiL8P99/WM6+ITBSR+321/zySCpzL7xDAHcBX9u2vgOYXG5xO5xH7ZjqQ6bFNJ2DdDUlnuVrGdPtmILDbvl0HGOVwODY6HI478iHjRsDzzQ2n03ky+wYOh6MMEAQczetw5J7v4rnOwD7XTqfzFNaIxI3KleXcAiWdTucxp9N5HGskB3I+t04gAOs5kJDHGbP0ncPhKAqkOJ1Ol9Pp/AlrRNnT48AHcMXnqq/zXUsfGmCVw+H4yOFwVLSX7bPb/YEkH2bzOt+VnndOpzPJfh1V9XE+X2XMqV/zLaOdM6c+89n7918dgeoEfAd09EUIY8wJY0yuBZcx5mFjzH+ussogIMcCSkRuusp2YVhTSheP09MYsz+3PF5qbYypCzQFqgBLvNk4l/xt8XjjMsaMNcZ8dZX11SU3c+lD50+gRA7rTAXmAjgcjgeBb7ixU45XzehwON4HvuDSm8udWJk7AtP/DhmvYBAwP88SZZVrPofDcRMwGlh8gzLllqtADrdzOrfrgR3Y0xI3OGMJshZt7vcph8NRFjBOpzPeY1n252pe58upD192Op13Aq8BM+1lG7BGyg4Aa3yY7a/ky5HD4Qh2OByhWNNkvuaLjDn1qy95nTF7n/n6/dvra6BEJBC4C2gNfAiMFxHBeuHeCxzBmoe8uP5RYK29fiGgN9YbQDVgujFmkYhUAj42xtQRkTCsby1FsarG940xwzz21QRrTvMd4FasF+yrQDBWEbRRRE4bY1qLSCIwC3gQeFlE7gUeA4oAPwB9gKfsfa4RkRSsKvdfwBCsb1qVPY4fBjQ2xvQXkS7AAMAP+Anoa4zxHKXIkTEmUUSeB46JSEmgHjDEGPOofYz5wDZjzAr78S7DGh6fLyJBdv/5AYeArljXFDwO3C0ir9iPZ4zdn++KyH3ADKxzvRV4wRiTZu97pd0fhYBnjDGXzauLSG/7mFR/tGPj8o3vyu0heuX1sCd5edWG9MwL1/9Ff3q3tl5v81zf/gQVv5mth39n4CvjOXs6nq2Hf3cP9372/ns81TWMJzt37bj18O80bdGK54eOZMs3/yYzM5Oth39f6s3x4hISvc7YrkdvAosV55NdBwgbPIL/nD3NJ7sOuDPOWvcB507Hs3L2tK8/2XWAcrdVYuj0OfsB5o8bySe7DnjVuXcXv/bvVav+uZ7vordyc/FizJk4rmfikYO81CucCuXLkXjk4ErPdccNHkhmZiaJRw4udSUm8sh9rZk4dPCocTNm07Nzh+cSjxy8pmNmJl97H7654SO+376Tm4sFMXv0sJ5/7tvBwLAuVAgJ5s99O7Lk6/jIQziqVOaR1q0O/blvBwALJ44hek8Mf+7bMfqaDwqk3Zbj4PxlXho2gptLlCDOlcykaTOIjztFnCt5AkD9Ro2IcyUbgAaNGhPnSjZVq1dnxVvv7Afo16sHca5kU61GDeYviQJg6MD+n8e5knM9brGUP3NdZ8Xat9n844+UKF6cedMieqbGneTlF/ty2623kBp3ciXAli8/Y8iYcaTGnewA0KRBfVLjThqAsUOHkJqW6r4PsHvzJv44FceQseO+To27bFAyC1fRK042ZPHS8BHcfHMJ4hNTmDx9JnGnThGfmDIBrH6LT0yx+rCxdfu77buIT0zhu+27eKFHGPGJKaZ+w0aMnTSFkqVK8VLf5yOPxZ+NLFykyFWPW9hc22fwsGHDKFGiBC6Xi2nTphEXF4fL5ZoA0KhRI1wul8l+e9GiRURHR+NyuUYDLFu2jDfeeOOPcuXKER8f717PV3yRcdu2bbhcLrZt20Z4ePg1Z0y4xlJmwJBh3HxzCY6fczF+6jROx8Vx/JyVsV7DRhw/Zx2vvn173tJlRC1844/gkHKcPh3P8XMu0+re+xg1biL//vILMjMzOX7OdU3v37eUCJKclv+VEai2wGfGmIPAWRFpBDwJOIC6QC+sb0mejhlj7gA2AyuAp7GG3yZe4RgNgA72/jqISIVs7Q8BJ4wx9Y0xdew8c4ETWCM9re31AoC9xphmxpjvgPnGmNvtbYoAjxpj3gW2Ac8aYxoYY1I8jvMu0M7jfgdgnYjUtG/fZYxpgDXkf83XShhjErAKzerXsHqqMaaFMeZtYL2dvz4QC4QbY37AKmSH2vl/ubihiBTG6u8O9uhXQeAFj32fNsY0AhZiFYw5ZV1ijGlijGni6+Lp76BazVrs22V9WO7duYNqoZdmIGK2b+Ng7D7aduriXvbHiePMnjiWT9e/y+cb1nPi2G95nrFS9VAOxuwB4GDMbipWd7jbMtKtWZHCRYrgX7gwAGXLlSfh3FnSUlO5cCHXmv66dHumHUumT2V0/xeJ3mnNymzbs4daNa7+1D76+3F+O36CfqPH8dPOXUyZuyBP8nVt+xiLXh3LyOd7snXPXgC2791PrWpZZ0E++GojiPBI61Z5kuNKaterx/boaAC2Rf9ErTr13G3FihUn7tQpTsfHERgYCECF2ypy+nQ8KSkpZGZa57ZQIT/8CxemcJEi7ueDL4R17kjUvDm8MvRlordbr5GtO3dSO/RScVikcGFS09JITk4mZn8sVSpVcrdt3PwdrVu2cN8/f/48AEWLFqFI4asXJ96oU7c+27dafbj1py3UrlvX3RZUrJhHHwYBkJRoFeC/HT1KUJC1rECBAgQFBVGoUCFEhIwM3w0w16tXj2j7HEdHR1OnTh13W7FixTh16hTx8fHuc3ylfSxevJgePXpQuXJln2XzZcZEu1+PevSrL9WqU48d26yMO7ZGU9MjY1CxYsTHneJ0fDwBdsbadesxa8Fing3rQcVKVp8dP3aMMcOH8M+3VvPeurX8dvTodWX6K7/C6wS8bt9+275fCHjLHoE5ISL/zrbNh/bfMUCgMcYFuEQk9QrXNH1tjPkTQET2AxWBYx7tMcAMEXkNa6Rl8xWyZgLvedxvLSLDsEa3SmLNe390pQdqjIkXkcMi0hz4GatI/B7r1weNga3W4BtF8P6itBwr2hx4ztXWEZFJWEOZgcDnuWzrAI7YxS5YI04vcun8rbf/3k7WQjHP3VSgANO6PEbV4FJM6/I4kV9vIfb4qRsZAYDK1WpQyM+PiUMGUrFKVUqVKcsHb63hiU7PsnLhPIoUDWDy8Jcpd+uthA8YzJQ3rJnXb7/8jMzMTMpXuC3PM95apSqF/Aoxb9xIylesRInSZfhy/Tv8o117Vr0+nZTkJC5cuMAjnboC8GD7Trw5dybp59N44GmfzLLnqmXzpnw983V6DB5Gi6ZNKFOqJM5fDhP78yHaPvQAUW+t47ON32CA+LNn6f1sJ1a8PgMAewQqb/M1acS/f4ym16jx3NmoAaVLluDgkaPE/nKEJ+5vzbQly6hVvSrPj5lIo9o16d3xGT74aiPvffYlfyYm4kpMYljvHj7P5QitiZ+/Hy/27EG16jUIDglhVVQk3cJ70qPP84wfNQJjDIOHjwSgR5/nmTBqJGlpaXTv1RuAtk8/Q99w67rnx9r5/mV89113MvabCJ7r24+WzZtTpnQpDvz8M/udB2n36CP06taFPoOH4Ofnx6TRVs7EpCRciYmUD7l0vfmwcRNwJSWRmZnJgD69fJbPUbMmfn5+9A3vTrUaNQgOKcfKqKU8F96L8OdfYNzI4cClPpzwyihcCQmICENGWgOLz4Z1Z+ALfShQoADN77yLQB8WAKGhofj7+9OzZ0+qV69OSEgIUVFRhIeH06dPH0aNGoUxhuHDhwOwYcMG3n33XRISEnC5XAwfPpyoqCiio6MpXrw4o0b57Pp7n2YcM2YMCXa/jhgxwucZa4SG4ufnz8A+PalSvTrBISGsXh5Fl+7hhPXsw6uvWBkHDrUyrl4exY6t0RQrXpyXRlh9tvRN69r2zz7+iMzMTG7zKPj/CjHm2kcCRaQU8DtWsWCwps8M8D6wyxiz3F5vPbDWnkI6CjQxxpy2p8CaGGP62esdxZo+CyTrFJ7nOh8DM4wxm7LtqyTwMPA88IUxZqJnu71tojEm0L5dGPjVbj8mIuMBjDHjRWQT1jTaNntd930RCQdqY82NO4wxL4tIf6C8MWbkNfZb9lxBdj9Wsvc9yhjzsN0WCXznMYXnud0RoK0xZrfdT/cYY8JEZIXdf+/a663Ams8/BMw1xrSyl98HvGiMaZetL5vYfXzP1R7HPePn5/UFtdflr0zh3Wh/ZQrvRvNmCi8/eDOFl1+udQovv1zLFF5+u9YpvPxyrVN46uqudQovP/lqCu9pYJUxpqIxppIxpgLWVNRZoKOI3CQi5bCud8oz9i/mko0xq7Gu72lkN7mwftWTk8L236ft67g8L1q/2nbrsaYtPa/c/xp4WkTK2nlKisg1/erAPvYCYIMx5hxWUVdLRPxFpDjWzy6vJAg4KSKFyDpleKX8B4BKIlLNvt8V6wI6pZRSSl0Hb6fwOmH9Oyme3sO6wv1nrKm1g+T9h3RdYLqIXMD6ifnF63qWAP8SkZMe10EBYIz5j4gstTMexbqg+qIVwCKPi8g9tztnTyPWMsZE28v22xdsfyEiBewML2IVQ1ey0b7YvgDWiN2r9r6Oicg7wB6sPtx5lX2Mwbpg/Vf7cVwsmt4GlorIADwKQ2NMqoh0B/4pIhcvIvfpPxCnlFJK/S/yagpP/W/TKbzrp1N410+n8K6fTuFdP53C843/pSk8pZRSSqn/efp/4fmQiPyE9S/ZeupqjInJjzxKKaWUyhtaQPmQMaZZfmdQSimlVN7TKTyllFJKKS9pAaWUUkop5SUtoJRSSimlvKQFlFJKKaWUl7SAUkoppZTykhZQSimllFJe0gJKKaWUUspLWkAppZRSSnlJCyillFJKKS9pAaWUUkop5SUxxuR3BvVfYuvh3//WT5ahqzbkd4RcDX/y/vyOkKvW5Yrnd4SrMhnn8ztCrjKCSuZ3hP966TcVyu8IVxWYlpjfEXKVVjD7f83695Ns/v7jOMHFAiSn5X//5EoppZRSfzNaQCmllFJKeUkLKKWUUkopL2kBpZRSSinlJS2glFJKKaW8pAWUUkoppZSXtIBSSimllPKSFlBKKaWUUl7SAkoppZRSyktaQCmllFJKeUkLKKWUUkopL2kBpZRSSinlJS2glFJKKaW8pAWUUkoppZSXtIBSSimllPKSFlBKKaWUUl4qmN8B1P9vqxcv4PDPTipVq0635/u5l0fNncXvvx5FEML6DeC2ylUBMMYw+sXe/OPxtrR+6JH8ig1AqaAApnZ+hEplStJmymIyL5h8yfH+ikiOHf6FWytXoV33Xu7l65cv5fjRI6Snn6dtt3CqhNZkzRtzOHX8GIX8/Lnz/gdo3OLuPMuVkZHB2KmvcfzkSVrdeQfhXZ7N0v7T9h3MXxqJn58fU14ZRXDZsoT3HwiAKymJ8sHBvD51MgCpaWk83L4jU8aMpnmTJr7PmpnJuNdmcPzkH7S6oxk9OnfM0j559ly++mYz/Xv1oN0jba64LC/MnDmT2NhYQkNDGTJkiHv5oUOHmDp1KsYYRo4cSfXq1ZkxYwYHDx4kLS2Nl156iQYNGhAVFcU///lPHn/8cfr27evzfBkZGUyYMIETJ07QsmVLwsLCsrRv3bqVBQsW4Ofnx8SJEwkODmbEiBGcPXuWzMxMxowZQ6VKlYiIiOCrr76iX79+tG3b1qcZX58xnQP79+EIrclLw4a7l/9y6GemTZ6EMTBs1Giq1agBQGpqKk89+jDjJk2hafPmvDp2DEePHMbfvzBPPPUUD7Z52Kf5MjIyGDNpCsdPnKRVizvp2a1rlvaftm1n7qLF+Pv5MWX8WELKlgUgLj6eNk+15/01b3JbhVvp/kI/DAZBeD68O82aNPZZRm+eh5MnT+aXX35BRBgxYgTVq1cHrPfvzp0706FDB5+fY0/zZs3AGbuf6o6aDBwy1L388KFDzIyYgjGGl0eMpGr1GowfNYKzZ86Qnn6etNQ0lq1922c5fDYCJSKJ17BOSxHZJyK7RKSIr46dyzEbiMjDHvcfF5EReXCcKz5+EakkIikislNEYkUkWkSe8/HxB4lIUY/7n4rIzb48hreOHDpIWmoqY2fMITM9g1+cB9xtj7XvxLiZc+k9eCjvr3nTvXzHlh8odnO+xnZzpaQyeOUH7P/9VL5lOHb4F86npTFg4lQyMjL47dDP7rYnunan/4QphL00jK/e/6d7edcBL9N//OQ8LZ4ANn3/A1UqVmTlgvns3BPD6TNnsrQvWbmKRbNmMLBPb6JWrwUgat4coubN4bEHH6TVnXe4133vw4+oVrlynmX95vsfqXzbbayYN5udMfs4ffZslvbe3Z5l0PO9cl3mawcOHCA1NZXIyEjS09PZt2+fu23RokVMnjyZiIgIFi5cCMCgQYNYsmQJERERLF++HIC2bdsyadKkPMv47bffUrlyZaKioti1axenT5/O0h4ZGcn8+fPp378/K1asAGDSpEksWbKEvn37sm7dOgB69uzJwIEDfZ7PGRtLakoKi5atID09nf379rrblix4g4lTX2PStGksWfCGe/kH69+jStVqWfYzfvJUFkRG+bx4Ati0+TuqVKrIqiUL2bl7z2WvlcXLlrNkzmwG9X2ByJWX3g9Xv/0O9WrXzrJu5Lw5LF8436fFk7fPw7CwMJYtW8a4ceNYsmSJe91vvvmGEiVK+CxXTpwHrPM9f+kyMjLSifXIGrV4IWMnTWHC1NeIXGRlHT8lgrmLl9Kp63Pc0bKlT7Pc6Cm8Z4EZxpgGxpiU3FYWkZt8cMwGgPsVYYz50BgT4YP9eusXY0xDY0xNoCPwkoh0v9aNxXK18zUIcBdQxpiHjTH/+etxr9+h2P3UbtgIgNoNG/HLgVh3W9mQcgDcVLAgUuDSw/px079p1uqeG5rzSs5nZJKYmpavGY4ePECNuvUBcNStz9Gfne62mwpaA8hpqamUr2gVHyKwZv5slkZM4mx8XJ5m27N3n/tN/PZGDdl74FK2lNRUCvv7E1C0KPVq1+KXo0ezbLvp+++5p0ULANLT04nZH0vDenXzLOvu/ftp1rihlbVhffZ5ZAUoU6rUZdvktMzX9uzZQ9OmTQFo2rQpMTEx7raEhARCQkIoW7YsiYnW97OC9jlPTk6mhj2aUiqPc8bExLgzNmnShP3797vbUlNT8ff3JyAggDp16nD48OEsOVNSUqhWzSpUSpcunTf59uymSbNmANzerDn79uxxtyX8mUBwSAhlywa7+zA9PZ19MTHUb9jQvZ4ITBzzCkMG9ufkiRM+z7h7716a3347AE0bNWLv/kvvhSkefVivTm0OHzkCwNlz50hKTqZ8uRD3ugUKCL36D2LoK2P5888En+Xz9nl4yy23ANZ5vummSx/Tn3/+OQ888IDPcuVk3549NG5qne/GTZuxf69H1j//JDgkhDJly5KUmHVMY/OmjbRqfa9Ps/i8gBKRe0Rkk4i8KyIHRGSN/eHfE2gPjPVYNl1E9opIjIh08Nh+o4isBWLs0ZsDIhJpr7tGRO4Xke9F5GcRaWpv11REfrBHeX4QEYeI+AETgQ72qFcHEQkTkfn2NhVF5GsR2WP/fZu9fIWIzLX3c1hEnraXB9rr7bAzP/FX+sgYcxgYDAyw9zteRNxjpvbjrGT/iRWRBcAOoIKILBSRbfZI3gR7/QFAeWCjiGy0lx0VkdL27cH2PveKyCB72cV9L7X39UVOo4Ii0ts+3rb331rj1eNMTkyiSFGrpisaEEBSouuyddYtj+TBx58EYM/2rYTWrZ/lBfm/LiU5icJ2HxYuWpTkpKxvClHTp7Bo0jhq1LOKrCe69WDQpGnc17YdH6xalqfZXImJBAZY2YICAnC5Lp3fBJeLgKLuep4LFy64b585dw4RoWQJa6Rxw6f/4tEH8/ZN15WY5M4aGBBAgivXAfMbwuVyERAQAEBgYGCWPvTsM2MuTR8PGTKEfv36uT/wbkTGwMBAd8aEhEsf3AkJCe78cClzeno64eHhTJ8+nbp1864wBkh0uQgIsPIFZMtnjEcf2tk+/mADbR55NMs++g8ewtKVq+ga1p15s2b6PKPLlehxngNI8HytJLgI9OjDTDvn6rffodMzT2XZz6wpk1m+cD73tGzB4uUrfJjP++chwPz58+nY0ZoO//HHH2ncuHGev38nJnpkDQjEdYXzfcHjdkZGBocPHcIRWtOnWfJqBKoh1ohILaAKcJcxJhL4EBhqjHkWaIc1OlQfuB+YLiLl7O2bAqONMbXs+9WAOUA9IBToDLQAhgCj7HUOAK2MMQ2BscAUY8x5+/Y6e9RrXbac84FVxph6wBpgrkdbOfsYjwIXR6xSgSeNMY2A1sBMEZG/2Ec77MeSG4edsaEx5lesfmmC1Rd3i0g9Y8xc4ATQ2hjT2nNjEWkMdAeaAc2BXiJy8atXdeANY0xt4D9A1lcrYIxZYoxpYoxp8mSnZ7M3X1XRwABSkpMBSElOpqj9JnzRZ++/xy23VcRRx3qD3fTZp7R64CGvjvH/XZGiAaTafZiakkKRogFZ2sOHjmLQ5Gl88pY17B8QGARAldBaJPwnbwYgV6x9m/D+A9m4+TsSk6xsiUnJBHmc32JBQSTZuQEKeLxMNm3+ntYt7gKsN7YforfSonmzvMn69juEDxrCpu9+cGdNSkrKkjU/BQUFkZSUBNi5goLcbQU8RmY932ZmzJjB8uXLeeONS1NSeWHVqlX07t2bTZs2uUceEhMTs2QsVqyYO79nzkKFChEVFUVERASLFi3K05yBQUEk2V8skpOy5vMctJcCBcjIyOCnH3/gDnv086LixYsDUL9hI86cyTpFeT2Wr15D9xf68e9vN7v76bLXSrEgEj36sIAUIMHl4o+4OKpVqZItZzEA7rv7bg7Zo32+8Feeh2vXrqVKlSo0aNAAgA0bNvDYY4/5LNOVBAZ6Zk0k8Arnu4DH7Z3bt9Ggse+mPN3H8PkeLdHGmN+NVQ7uAirlsE4L4C1jTKYx5hTwDXC7x/ZHPNY9YoyJsfe3D/jaWKVwjMe+iwP/FJG9wGwg68Rxzu4A1tq337QzXbTBGHPBGLMfCLaXCTBFRPYAXwG3eLR561oLr1+NMVs87rcXkR3ATqzHWCvnzdxaAO8bY5KMMYnAeuDiRPARY8wu+/Z2cj5Pf1m1mrXYt2sHAHt37qBa6KWoMdu3cTB2H207dXEv++PEcWZPHMun69/l8w3rOXHsN1/G+a9UqUYoB/daUxIHY3ZTqYbD3ZaRng6Af5Ei+PkXBnAXW6dO/E6RgADyQljnjkTNm8MrQ18mert1frfu3Ent0EvfB4oULkxqWhrJycnE7I+lSqVK7raNm7+jdUvrpXbm3DlOxcXxwstD+eSLL5m7eGmWb+fXnbVje6Jen8ErgwcSvWOnlXXXbmqH1vDZMa5HvXr1iI6OBiA6Opo6deq424oVK8apU6eIj493jwCdP38egICAAIoUydvLSLt168aSJUsYNWoUW7duBWD79u3U9rgmp3DhwqTZ53nv3r1UqVIFYwwZGRnunP7+/nmas269+myL/gmArT/9RO169dxtxYoXI+7UKeLj4ggMDOTs2TPE/XGKQS++wGeffsKieXNJSEhwT/f8evRolg/k69W9y7MsXzifscOH8tO2bVbG7TuoU+vSSEgRjz6M2befqpUrcfTX3/j12DGeHzSYLVu3MvG16QDuQmvnnj1UsKfRfMHb5+GWLVvYs2cP4eHh7vWOHTvGkCFDWL16NWvXruVotml7X6ldrx7bt1pZt0dHU8tjhLNY8eLEnTrF6fh4AjyK1M0bN9LyntaX7et65dWv8DwvHMm8wnGuVkAkZbvvub8LHvcveOz7VWCjMeZJEakEbLrGrJ48xyc9j3kx67NAGaCxMSZdRI4Chf/CccAapbs4EZ5B1mLWc5/uvhCRylijbrcbY86JyIprOP7V+jn7efLpO3LlajUo5OfHxCEDqVilKqXKlOWDt9bwRKdnWblwHkWKBjB5+MuUu/VWwgcMZsob1sWI3375GWiEMWwAACAASURBVJmZmZSvcJsv43jtpgIFmNblMaoGl2Jal8eJ/HoLscdv7AXlFapUpVChQswdO5LyFStRonQZvlj/Dg+0a8+K2dNJTUniQuYFHu1s/arnzbmzSE5KRER4pufzeZrt7rvuZOw3ETzXtx8tmzenTOlSHPj5Z/Y7D9Lu0Ufo1a0LfQYPwc/Pj0mjRwLWB4ArMZHyIdZ1HcFlyrB26WIAFi5bTsN6dSnmww+wi1rd2ZyvIjYT1v8lWjRrSplSpThw6BdinQd58pE2LF29ln99tREwxJ8+Q5/nuuS4zNdCQ0Px9/enZ8+eVK9enZCQEKKioggPD6dPnz6MGjUKYwzDh1u/LBs5ciSJiYlkZmbSr5/1q9YNGzbw7rvvkpCQgMvlcq/rK61atWL8+PGEh4dz1113Ubp0aZxOJ7GxsbRt25YePXrw4osv4ufnx4QJEzh//jz9+/dHRBARd56oqCg+//xzjDHEx8fTq5dvLtB31KyJv58/z/cIo1qNGoSElGNF5FLCevai1/N9GTNiGMbAkJEjKVs2mGVrrO/MkYsWUq9BQ4oVK8aQgf1xJSQgIgwdNdonuTzd3bIFX06cRLfeL9DyzuaUKV2aAwcPsv+Ak3aPP0avsOfoNWAQ/n5+TB77CuVCQlgTab0fjp44iT7dwwAIf7E/hf39rdfUmFd8ls/b5+G0adMIDAykT58+VKxYkdGjR7N2rdWvH330EZmZmVTy+NLkS47Qmvj5+dOvVw+qVq9BcHAIq5ZF0q1HT3r07sOE0SMxxvDSMOu3YsYY9sXsYdAw374uACT7nOZf3pFIojEmUETuAYYYYx61l88HthljVtgf+B8bY94VkXZAH6wLvEsC27CmmUKzbV/J3qaOfd9zH+42EXkfWG2MeU9ExgNhxphKIvIU8Lgx5jl7+zCgiTGmn4h8CPzTGPOmvfwJuwBzHyPbYxsIVDPG9BeR1sC/gcrGmKMX17lC32R/DJWwRoLmGWOWi0gX4FFjTEcRaQRsBaram3tuVx9YhVV8lQH2AMPtvo2xH+cRe92jQBPgNmAF1vSdAD8BXYFz2fY9BAg0xoy/0jneevj3/Pkd/zUaumpDfkfI1fAn78/vCLlqXa54fke4KpNxPr8j5CojqGR+R/ivl35TofyOcFWBaX+P6+iuJq1g3o7++UKy+fv/c5TBxQJyHIjIz+TvYxUAu7EKkWHGmD+uY3/TgKki8j3geRXbRqDWxYvIs20zAOhuT8l1BXL7je0aoImIbMMajTqQy/qeqtoXuMcC72AXT3bbe0BJEdkFvAAczGkHxpjdWFN3+4BlwPcezUuAf128iNxjmx1YBVQ0VvEUaYzZ6UVupZRSSmXjsxEo9f+fjkBdPx2Bun46AvW/QUegrp+OQPnG33EESimllFLqv5L+Vy4+JCJ1sX7N5ynNGJM3v9FWSimlVL7QAsqHjDExWP+2lVJKKaX+H9MpPKWUUkopL2kBpZRSSinlJS2glFJKKaW8pAWUUkoppZSXtIBSSimllPKSFlBKKaWUUl7SAkoppZRSyktaQCmllFJKeUkLKKWUUkopL2kBpZRSSinlJTHG5HcG9V/ik10H/tZPlgI5/n/Zfy+vvf9VfkfI1Yed7s7vCFeVkZSQ3xFylVmjUX5HuKrCJiO/I+QqLuXvnTEk5Ux+R8jVhZLB+R0hVyeTz+d3hFzVCC6V46eLjkAppZRSSnlJCyillFJKKS9pAaWUUkop5SUtoJRSSimlvKQFlFJKKaWUl7SAUkoppZTykhZQSimllFJe0gJKKaWUUspLWkAppZRSSnlJCyillFJKKS9pAaWUUkop5SUtoJRSSimlvKQFlFJKKaWUl7SAUkoppZTykhZQSimllFJe0gJKKaWUUspLBfM7gPr/bcPKSI4d/oVbK1fhybBe7uXvr1jK8aNHyEg/zxNdw6kcWpOkRBfvLl1IkiuB6nXq8Y927W9IxvdXXMrYrvuljOuXWxnT08/Ttls4VUJrsuaNOZw6foxCfv7cef8DNG5x9w3JmJNSQQFM7fwIlcqUpM2UxWReMPmWJSMzk4lz3uDEqTha3N6YsKefzNIesXApX3//Iy9260zbB+4H4K0PP+HzbzdTQAowKPw56oU68jTf5EXLOBEfz10N69PtiUeytA+ZPofEpGQKFizI2BfCKVuqJK8ujOLo8RP4+/nxxH138+BdzfMk29yZ0zmwfz81QkMZNHS4e/nhQ4eYPmUSYHh55GiqVa/B5HFjOHrkCP6F/Xn8yad4oM3DvLksii0/fE9aWipdu/fk7nvv9Wm+jIwMJkyYwIkTJ2jZsiVhYWFZ2rdu3cqCBQvw8/Nj4sSJBAcHM2LECM6ePUtmZiZjxoyhUqVKnDx5ktdee42UlBTatGlD27ZtfZoTYMnc1/nZGUvVGg6eHzjYvfztVcv5eP17/OORR3mu1/Pu5WlpqfRo344hY8bTsElTn+fJLiMzk/EzZnPij1O0bHY73TtmfY+bOvcNvtz8Hf17hPFkmwcBGDhmAq6kJAoVLMjEoYMJLlPap5lmzpxJbGwsoaGhDBkyxL380KFDTJ06FWMMI0eOpHr16jkuGzlyJGfOnCE9PZ20tDTWrl3L8uXL+eGHH0hLS6N79+60bt3ap5mXzpvDIecBqlavQe+BL7mXr1u1gk/fX8/9Dz9C1159AJg9ZRK//3oUP39/HnzsCe75xwM+y6EFlMozvx/+hfNpafSfMJV3Ixfy26Gfua1adQAe79KdmwoW5Gx8HO9FLaLXiLF88e7bPNS+M8G33HrDMh6zMw6YOJV3lmbN+ETXSxnfjVxE75FjAeg64GXKhJS7YRmvxJWSyuCVHzCp48P5HYVvf9pK5VtvYeLgAbw0cQqnz91D6RIl3O09OzxN7RrVyMzMdC/7+OuNvDl7GqfPnWP64iimjxqWZ/m+276LireUY9yLvXh52uuc+c+flLq5uLt98HOdKV+2DNF79vH2p18woGtHAMb3602FkOA8y+WMjSUlJYUFUcuZMWUysfv2UrN2HQCWLnyD8VMiKFBAmBkxlYhZrwMwbvIUbq1wm3sfnbp2o2uPcJKTkxn0Qh+fF1DffvstlStX5tVXX2XQoEGcPn2a0qUvfYhHRkYyf/58jhw5wooVKxg+fDiTJk2iYMGCbN++nXXr1jF8+HAWLFjAuHHjKOHxvPClQ84DpKamMP2Nxcyf8RoHY/dTo2YtAB589HFq1qnHru1bs2zzrw8/oGLlKnmSJyff/LiFyrdVYNLwIQwYM57Hzp6ldMmS7vaez3aidmgNMjMvuJcN69uHW8qFsGX7Ttas38DgPj19lufAgQOkpqYSGRnJ1KlT2bdvH7Vr1wZg0aJFTJ48mQIFChAREcGsWbNyXDZ16lQANm7cSGxsLABdu3ale/fuJCcn07dvX58WUIecTtJSU3ht/kIWzJye5Tw/8Ojj1KxTl93bt2XZ5uUx4yl/q+8/V3KdwhORTBHZJSJ7ReSfIlL0rx5MRMJEZP51bFve4/4mEXGKyG4R2SoiDa5hH5tEpEku6wzy9jGKyD0i8nEu2eNFZKeI/Cwin4vInd4cI5fj3ywifT3ulxeRd321/7/q6M8HqFG3PgDV69bn15+d7rabClq1+/nUVMpXrAzAH8d+4+v3/8kbE0Zz9OCBG5Px4KWMjrr1OZpDxjSPjCKwZv5slkZM4mx83A3JeCXnMzJJTE3L1wwXxTgP0rRBPQAa163D/p9/ydJeuuTlH5oVyoVwPj0dV1IyxYOC8jbfz79wex3rTbZxrVD2/3IkS3v5smUAuOmmAhQoYL0tisDEBZEMmT6Hk/Gn8yTX3pjdNGnaDIAmzZqxLybG3eZKSCA4JIQyZYNJTHTZmYRJY19h2KAB/HHyBAAFCxUCrNGUKlWr+TxjTEwMTZtaozNNmjRh//797rbU1FT8/f0JCAigTp06HD582Mpkv3ZSUlKoVq0aGRkZnDx5kilTptCvXz9+/fVXn+eM3beXBk1uB6BBk9s5sG+vu61EyVKIZF0/PT0d5/591KpX3+dZrmTP/gM0a2h9VN1evx77nD9naS9TquRl29xSLgTI+tz0WZ49e9zntmnTpsR4PP8SEhIICQmhbNmyJCYmXnHZRRs3buReu3i/eP7T0tKoWrWqTzM79+2lfmPrPNdv3ATn/n3uthIlSyLZTrQIzJ48kYkjhhL3x0mfZrmWs5FijGlgjKkDnAee92wUy424lioMKJ9t2bPGmPrAAmC6j44zCPjLReJVrDPGNDTGVAcigPUiUvNaNxaRq40W3gy4CyhjzAljzNN/PapvpCQl4V/E6soiRYqSnJT1BbdsxhQWTR7nLmCOOA9wX9un6TZwKB+tXnFjMiYnUbiolbFw0cszRk2fwqJJ46hhv8k+0a0HgyZN47627fhg1bIbkvG/gSspiQC7HwMDiuLK9uaak9vr1+WZvgPpP/ZVOjzaJk/zJSYlE1CkiJWvaFFcSUmXrZN54QIrNnxM2/vvAaB/lw4snTiaro+1Yd7qdXmTy+UiIDAQgIDAQFwJCe62CxcujUIYe3q230svs2j5Kp59rjvzZ89yt8+YOpmwDu1pdPvtPs/ocrkItDMGBgaS4JExISGBgICAyzKnp6cTHh7O9OnTqVu3Lv/5z3/4+eefGTVqFC+99BJz5871ec6kRBdF7SwBgYEkulxXXf/LTz/m3gcf8nmOq0nM8joJuKbXCUBmZiZRb63jqUd8+zpxuVzu8xcYGIjLo8+yPP+MueIysKZ5Dx06RGhoqHtZREQEHTt25HYfPycTvTzP4S8OYPrCJTzVuQtRb8zzaRZvC5/NQDURqSQisSKyANgBVBCRTiISY49UvXZxAxHpLiIHReQb4C6P5StE5GmP+4ket4fZ+9otIhH2ek2ANfZoWJFsuX4EbvHY/gER+VFEdtijZoHZH4iILBSRbSKyT0Qm2MsGYBVpG0Vk49X2JSIPicgBEfkOaOdNJxpjNgJLgN72vtwjYyJSWkSO2rfD7GN+BHwhIoEi8rWdJUZEnrB3GQFUtftmun1+9tr7KCwiy+31d4pIa499rxeRz+xRsWk5ZRWR3nY/bfvsvXe8eZgUCQggLSUZgNSUFIp4vNEC9BgyioGTpvHJW28CUKZceYJvrUDQzTdf9i0irxQpGkBqskfGolkzhg8dxaDJlzIGBFojJVVCa5Hwn//ckIx/Z2+u/4A+o8byzZatJNn9mJScQmC2c51dYnIyH361kfWL5rF8xlTmr1qTJ/lWf/Qv+k58jW+37SApJcXKl5JCUNHLvyPNffNt2rS8k1uDywJQ3C4a6ofW4Myff+ZJvsCgIJLsD9HkpCQCPUbiPEcbpID1eihW3Jp2rN+wIWdOXxoVGzJyNGvee59VyyJ9lm3VqlX07t2bTZs2uUcbEhMTCfLIWKxYMZI8itGLr9tChQoRFRVFREQEixYtIjAwkCpVqlCiRAmqVq3Kn3nQnwGBQSTbWZKTkggIuuxt3y0zI4Md0Vu4vbnPJgKuauU779FryAg2/vDjpddJUjJBgVd/nVw0a0kUj95/HxXK+/bSgaCgIPf5S0pKynJuszz/7POa0zKAbdu20bhx4yz7HjFiBO+99x7Llvn2i2Zg9vMceOXzDBBUrBgAtevV59zZsz7Ncs0FlD0C0ga4OMbnAFYZYxoC6cBrwL1AA+B2EWkrIuWACViF0z+AWtdwnDZAW6CZPbo0zRjzLrANa8SpgTEmJdtmDwEb7O1LA68A9xtjGtnbDeZyo40xTYB6wN0iUs8YMxc4AbQ2xrS+0r5EpDCwFHgMaAmE5Pa4crADCM11LbgDeM4Ycy+QCjxpZ2kNzBTrWTwC+MXum6HZtn8RwBhTF+gErLTzg3WuOgB1gQ4iUiH7wY0xS4wxTYwxTR56yruLuitVD+VgzB4ADsbspmL1SxcJZ6SnA1C4SBH8C1txypYrT8K5s6SlpnLhQublO8wDlWqEcnDvpYyValye0b9IEfz8rYwXi61TJ36/rCD8X9S13RMsnjKRkX17E73bemvYFrOX2tWvPpVUQITC/n4UKlSIwKJFScmjqcguj7VhwdjhDO/5HNv2WtdnbN9/gJpVK2dZ78ON3yIiPNzK/R2PpGTrbebXEycJzKHg8oU6deuzfWs0AFt/2kLtunXdbUHFihF36hSn4+MItAv3i8XWb0ePuj/szp8/D4B/4cIEBFz9w8Qb3bp1Y8mSJYwaNYqtW61rh7Zv3+6+RgagcOHCpKWlkZyczN69e6lSpQrGGDIyMgAICAjA39+fwoULU6RIEVJTU4mLi8syauUrNWvXcV/7smvbVkJr1bniuufOnSU+Lo4xLw9i4xefs2LxQlyuhCuuf72ea/8US2dEMHpgP6J37gZg6+491KpRI9dtN/zrcwR49B/3+TxXvXr1iI62nn/R0dHUqXOpz4oVK8apU6eIj493j0DmtAxg06ZNWa5zcj8n7eldX3J4nuftVz/PgLvY+v23X3Mttrx1LReRFxGRXfbtzUAU1ijNr8aYLfby24FNxph4ABFZA7Sy2zyXrwNye8bcDyw3xiQDGGOuVjKuEZEA4Cagkb2sOVah9r1dIfthjVBl115EemP1QTl7mz3Z1rnSvkKBI8aYn+3HtRp7NMkL1zrE8qVHHwgwRURaARewRt1yu8K1BTAPwBhzQER+5dI5+NoY8yeAiOwHKgLHrv0hXN2tVapSyK8Q88aNpHzFSpQoXYYv17/DP9q1Z9Xr00lJTuLChQs80qkrAA+278Sbc2eSfj6NB57u6KsYV1WhSlUKFSrE3LGXMn6x/h0eaNeeFbOnk5qSxIXMCzza2cr45txZJCclIiI80/P5XPaet24qUIBpXR6janAppnV5nMivtxB7/FS+ZGnVtAnj58yn5/BXuKtxQ0qXLIHz8BEOHDrMEw/cx7J33uOzbzYDEH/2HL06PkPzBvXpMXQUmRcu0LNj3s44t2hUn43R2+kzfgp3NKhH6RI3c/Dobxw4cpTHW7dixrLV1Kpamb4TX6NhTQe9nmnLuDeW4EpKQhCGhnfNk1yOmjXx8/Ojb3h3qtWoQXBIOVZGLeW58F6EP/8C40YOBwyDh48EYMIro3AlJCAiDBk5GoA5M6bx69EjZKSn07nbcz7P2KpVK8aPH094eDh33XUXpUuXxul0EhsbS9u2benRowcvvvgifn5+TJgwgfPnz9O/f39EBBFh+HDrl4Xh4eH069ePzMxMhg7N/j3v+lVzhFLIz4+hL/ahcrXqlAkO4e1Vy+nYrTuff/whn7z/Hi5XAokuFy8OHsqcpcsBWL1sKbXr1ScoqJjPM2XXqnkzxm+eTY+XhnJX0yaUKVUS5y+/EHvwEG3bPEjk2rf5bOM3GGOIP3OG3l06M3X+Amo7HPQaMoJG9erwQrcuPssTGhqKv78/PXv2pHr16oSEhBAVFUV4eDh9+vRh1KhRGGPc5zCnZcYY9uzZw7Bhl34EMmPGDI4ePUpGRgZdu/r2tVPN4cDPz4/h/V6gctVqlAkOZt2qFXToFsYXH3/EpxvW40qwzvMLg4cw49XxJLpciAh9Bw/J/QBeEM95zBxXEEk0xgRmW1YJ+Ni+LgoRaQu0M8Z0s++HA7WBb7FGTJ6zlw8Aahhj+olIJPCFMeYdexQlzRjjJyKzgP3GmMhsx9wEDDHGbPO8D+zGmsKqbIxpJyKPAZ2NMZ1yeCwXtzkDfAncbow5JyIrsAq9Ffb0WRNjzOkr7UusC9bnGGPutu8/DvQ2xjx6hT4Ms/fZz2PZRCDQGDNYRL4CRhljokXkVuA7Y0yl7NvZ99sAXYwx6XbWe+xdep4P9/kRkQ3AXGPMv+22zVijUo2y7ftjYIYxZlNOjwHgk10H8u938tegwI2Z9bsur73/VX5HyNWHnfLvn2a4FhlJeTdS4CuZNRrlvlI+Kmwy8jtCruJS/t4ZQ1LO5HeEXF0omXe/IPWVk8nn8ztCrmoEl8rx08VXF3//hDUNVlpEbsKaKvrGXn6PiJQSkULAMx7bHAUuTpo+ARSyb38B9BD7l3AicvFnCS7gsp/qGGPSsabZmot1UfYW4C4RqWZvX1REso96FQOSgD9FJBirKLnI8zhX2tcBoLKIXPx5wWXF2tWIyN1YI1ZL7UVHudQXV/sqXhyIs4un1lgjRtkzZ/ct8Kx93BrAbYDzCusqpZRS6hr4pIAyxpwERgIbsUaEdhhjPrCXj8ea9voK67qfi5ZiFV3RQDOsggZjzGfAh8A2e+rw4pjbCmCR5HARuX1N1EysEap4rF/svSUie7CKoNBs6+8GdgL7gGXA9x7NS4B/icjGK+3LGJOKVQB9Yl9Efi2/ye1gZz8IjAKeMsbE2m0zgBdE5Afgav9K2hqgiYhswyqKDtiP5wzWNONeEcn+a8QFwE0iEgOsA8KMMX+P374rpZRS/6VyncJT6iKdwrt+OoV3/XQK7/rpFN710yk839ApPKWUUkqp/yH6X7n4kIh0BwZmW/y9MebF/MijlFJKqbyhBZQPGWOWA8vzO4dSSiml8pZO4SmllFJKeUkLKKWUUkopL2kBpZRSSinlJS2glFJKKaW8pAWUUkoppZSXtIBSSimllPKSFlBKKaWUUl7SAkoppZRSyktaQCmllFJKeUkLKKWUUkopL+l/5aKu2d3F/971dsGAoPyOkKu7Ot2d3xFy9fhb3+R3hKsqVrRwfkfI1VulgvM7wlUlliiX3xFydWuBtPyOcFWZN/39Pz79M/7efQhQPul0fke4BqVyXPr3/kRUSimllPob0gJKKaWUUspLWkAppZRSSnlJCyillFJKKS9pAaWUUkop5SUtoJRSSimlvKQFlFJKKaWUl7SAUkoppZTykhZQSimllFJe0gJKKaWUUspLWkAppZRSSnlJCyillFJKKS9pAaWUUkop5SUtoJRSSimlvKQFlFJKKaWUl7SAUkoppZTyUsH8DvB3IyKZQAwgQCbQzxjzQz5n2gSUA1KB80AvY8yuXLYZBCwxxiTnfcIry8jMZMLM1zn+xylaNrud7h2eydI+df4Cvtr8Pf3CuvFkmwfdy+PPnOHxsF68s3g+FcqX932ujAzGTn2N4ydP0urOOwjv8myW9p+272D+0kj8/PyY8soogsuWJbz/QABcSUmUDw7m9amTAUhNS+Ph9h2ZMmY0zZs08X3WzEwmznmDE6fiaHF7Y8KefjJLe8TCpXz9/Y+82K0zbR+4H4C3PvyEz7/dTAEpwKDw56gX6vB5rmtRKiiAqZ0foVKZkrSZspjMCyZfcgCE39uMasFl+OXUaSL/vcW9vEHFW3i2ZWPSMjJY+MX3HD/7J/fXrUGHOxoSe/wUsz7ZdEPyZWRmMn7GbE5cfK10bJ+lfercN/hy83f07xHmfq0MHDMBV1IShQoWZOLQwQSXKe3zXPNmzcAZu5/qjpoMHDLUvfzwoUPMjJiCMYaXR4ykavUajB81grNnzpCefp601DSWrX2b1cuXseXH7zmfmkaX7j1o1fpen+bLyMhg7JQIjp/8w3otd738tTxv8VL8/fyYPHY0IWXLcuKPP5gy83VSUlJ45MEHaPfYI3z02eesW7+BwMBAJo0eQelSpXyaE+xzPG0mx//4g5bNm9GjU4cs7VPmzOPLbzbTP7w77R5pc8VlPs+VkcGYSVM4fuIkrVrcSc9uXbO0/7RtO3MXLcbfz48p48cSUrYsAHHx8bR5qj3vr3mT2yrcSvcX+gGQmJRIuZAQ5k6LyJu8mZlMmDXX/lxpQvf2T2dpnzp/kfW50r0rTz70AAC9h43GGIOI0LNzB5o2qHfdOXQE6nIpxpgGxpj6wEhgan4Hsj1rZ1oATL+G9QcBRfM2Uu6+/fEnKt9WgWWzprFr335Onz2Xpb1np44MDO9+2XZr3/+Qunn4ob/p+x+oUrEiKxfMZ+eeGE6fOZOlfcnKVSyaNYOBfXoTtXotAFHz5hA1bw6PPfggre68w73uex9+RLXKlfMs67c/baXyrbcQ+dokdu+P5fS5bH3Y4WkGdM/6hvfx1xtZNm0KESNe5s31H+RZtty4UlIZvPID9v9+Kt8yAFQJLkXhQoUY+dbHFLrpJqqFXCo0OtzZkFfWfcrMjzbSuUVjAH469Ctj3/nXDc34zY9brNfK7Ons3Lef02fPZmnv+WwnBvXqkWXZsL59WDZrGt07PMOa9Rt8nsl5IJbUlBTmL11GRkY6sfv2uduiFi9k7KQpTJj6GpGLFgIwfkoEcxcvpVPX57ijZUsAOnbtyvwlUby+aAlrV63wecZN3/1A5YoVWbnwCq/l5StZPHsmA1/oQ9SbawCYtziSiaNGEDV/Du0ee4SMjAzWrd/AyoXzGdC7J8tWv+XznADf/PAjlW+rwPI5s9gVs/eyc9yrS2de6t0z12W+tmnzd1SpVJFVSxayc/eey/pw8bLlLJkzm0F9XyBy5Zvu5avffod6tWu77y9fOJ/lC+fzWJuHuPuuO/Ms77dboqlc4VaWzYxg177Yyz9XOrdnYM+wy7ZbGPEqS6ZN9knxBFpA5aYYcA5ALNNFZK+IxIhIB3v5PSLy8cUNRGS+iITZt4+KyAQR2WFvE2ov/z927js+qir///jrBEibCR1CEekdqaGIgKKuLiLIYqH3jkFa6EiTJk1KpISEJkVFXGV3batLE4XQA6RQY6H3TEtIhvP7Y4ZJAoEwMEPw+/s8H488SO655Z07ued+5twzGJVSK53LYpRSbzqXv6KU+tW5/kallDGLTL8CJTMcb4lSaq9S6qhSarJz2XtACWCLUmrL/fatlJqplIp15pjj6RMYExdPgzq1AQipVZPYY8cztRcpVPCuba5dv4HFZqN4cFFPx0nPdeQoDUMcN8v6detwJD7B1WZLTsbfzw9DYCA1q1fjZGJipm237tzJC02aAJCamsrh2Djq1HzGa1kPJxxzXfD1nqlB7PGT2W7JiwAAIABJREFUmdoLFyxw1zalihfjZmoqJouVfEFBXsuWnZtpdszJKTl2/NuqlCjKwcQzABz87QyVS2T+20pJTeOaxUax/I5zZbKlYL9167FmjImNp6HzWqlfqyZHE7K/VkoWLwZArlw++Ph4vjs/GhNDvQYNAajXoCGxRw672pJu3CC4WDGKFC2KxWzOtN2OrVtcI025c+cB4GZKCmXLV/B4xkNHjtKovmPkt37dOhyJi3e12ZKT8fPzw2BwXMunTieSmpbG2fPn+WDWHPoPDSPx9z+4kZREcNEi5MqVi0oVyhOToVD0pJjYOBrWqwNASO1aHI0/lqm9SBajXlkt87RDR47QqH59ABrUrcuR2DhXW/o5NFCzRnVOnT4NwNVr17BYrZRw/g1mtHXHzzRv1tRreR33lVoAhNR85u77SsG7rxXloxg4ZgJjZszmhsnkkRxSQN0tQCl1UCkVD0QCHziXtwVqA7WAl4HZSqniD7C/y1rrusASIMy57H3ghtb6Ga11TeB/SqnCwHjgZef6e4FhWezv70DGt5rjtNYhQE3geaVUTa31QuAs0Fxr3fxe+1ZKFQT+AVR35pj6AL+PW0wWC4bAAACMgYEk3dHRZmX9V1/TrlVLT0fJnMtsxmhwDNAFGQyYMlxQSSYThsD0wbtbGW6kV65dQylFwQL5Afjqm295/dVXvJvVYnHlMRoCMT3AOaxf6xneHjiYQRM+oN3r3hn2/ysx+Plhu5kKgDXlJkZ/v0zt+QMDKFkwH6UK5s+JeACYM73Ohgd6nQHsdjtRGz7jTS883jGbTRgMBmcmI6akJFeb1unXxa0M36elpXHqxAkqV6nqWjZv5gy6d2hH3ZD6Hs9oMptc17LRYCDpjmvZ6MwPYL91i+vXb3D85CneHxlG2KB3+WjxUvLny8eZs+ex2mzs2X+AG0meucHendWc6TVOMnvnOO4ymczpr7PxjnOYdPc5BMfoU4e337xrX1eu3u4j735j57G8d/SJD3JfmTVuFBGzptGsUQOiNnzukRxSQN3t9iO8KjiKlTVKKQU0ATZore1a6wvANuBBeoMvnf/uA8o4v38Z+Pj2Clrra0AjoBqwUyl1EOgGlM6wn3VKqT+BUcCiDMvfUUrtBw4A1Z37uNO99p2EY15VpFKqLXDXfCmlVF/nCNfeFRs+e4Bf12HNxi/pO2IMW3/dhcVqA8BitRGU4ULMisls5sKly5QvU/q+6z2sVes/pdegwWzZ8TNmi+PXNVusBBnTB/vyBgVhsaafCh+lXN9v3bGT5k2eAxw3il+i99CkUUOvZP3ky6/pN3YC23btceWxWG2ZOrOsmK1WNv+4hS+XLmLlnBmEr1nnlXx/JZaUFAJ8HSMhgb6+WJJvutpWbYsmrHVz3mpYi7gzj/9R4+rPN9EnbDRbfvk1/XW2WAky3v91vm1eRBSvv/wSpUo8yPs59xiNQVgsFmcmM8YMo5lKpd8+fDJ8f2DfXmrXq5dpP8NGj2HtF5v4ZEWUx7KtXLeBnqGZr2WL1UKQMT1j3qAgzM78AD4+PgQFGSlXpjQFC+SnQrmyXL9xg1y5ctGvRzfeDRvF9l92UfrppzyWE2D1ZxvpPWwEW3ZmeI2tmfudnLBy7Tp6DAjlf9t3uF7nu/rDvHecQ+VDksnE+YsXqVCu3F373LJ9h9dGn9Z88SV9R45j6y+7M/WJD3Kt3B6Jb964EScSf/dIHimg7kNr/StQGCiCY1J5VtLIfB7972i//fzCTvqkfQXcOZtWAf91Fm+1tdbVtNa9MrR3AsoC63EWX0qpsjhGtV5yjiD9J4vj33PfWus0oAGwCWgDfJfFOYjQWodorUPunPB4P13fbkvE7BmMG/Qu0QcOAbA3JoZqlSred7vEP8/w+5mzhI6byO4DB5m+cPEDH/NBdO/YnqhFCxg/YjjR+/YDsOfAAapXqeJaJ8Dfn+SUFKxWK4dj4yhXpoyrbcuOn2ne1PH47sq1a1y4eJEBw0fwnx/+y8JlyzO9c3tUXdq+wbLpUxgzsC/RhxyPTvYePkL1ivd/DOKjFP5+vuTJkwdjYCC2J+ARWk6LP3uRWqUdH0aoVaYECWcvutoSzl5k/Kff8Pmug/xx5fpjz9btnTdZPmcm4waHuq6VPYdiqFapUrbbfvXt9yjg9b+95JVs1WvWZN+eaAD2RUdT7Zn0R9V58+Xj4oULXL50CUOGG+6OLVto+kJz1883bzqKVT8/fwwPWBQ+iB6dOrAifAHvjwhj9959AETvP0CNqpmv5ZRM13Jp/P38CAwIwJaczIVLl1xvSJo3fY6VHy/kxWZNqFvLM3NkbuvW7m0i581m/ND32L3f8fmfvQcPUb1y9q+xN/Xo3ImVS8KZMGoEu/fuBWDPvv3UqJY+epjpHB6NpXzZMiT+9ju//fEH/YcMY9eePUz5MH1a7v+2b+fFZs28krfrW22JmDWNce8NJPpgDAB7Dx3O9r4CuIrsQ7FxPJXFY8eHIZ/Cuw/nnKVcwBVgO9BPKbUaKAg0A0YAeYBqSik/HMXLS8DP2ez6ByAUx0RvlFIFgF3Ax0qpClrrE0qpQOAprbXrIbnWOlUpNR44qZSqCvgCFuCGUioYaAFsda5uAoKAy/faN47HfIFa62+UUruAEw97ru6laaMG/DR3Pj2HjaRJgxCKFCpIwslTxB0/QZu/v0LUhs/4bss2NHDp6lX6durAqvmOqVgT53xE744PXrS54/nnGjNh20y6DQylaaNGFClciPjjx4lNOEbb11vSp2tn+g0Lw9fXl6njxgCORywms5kSxRwXX3CRIqxfvgyAJStWUqfmM+T1wnyjZg1CmLQgnN6jxvNcvToULliAhFOniT9xijdeeYkVn2/iu207ALh09Rp92r9No9q16DliLPZbt+jd/q1sjuA9uXx8mNW5FeWDCzGrc2sif9qVI6M8py5c4WaanRkdXifx0hUumcy83ag2G3cd5O1GtalVpgQmWwqLv3dcuiHlS/FWw1oUy5+X0W+8xMyvf/J6xmaNGjJpx0f0HDqC51zXyknijp2gTYtXiVz/qeNa0ZpLV67Qt3NHZoQvpnrlyvQJG03dmjUY0LWzRzNVrlIVX18/Qvv0pHzFSgQHF2PNiki69uxNz779mDxuDFprho4cDYDWmqOHYxgycpRrHwvnzub3xERSU1Np37mrR/MBPN+kMT9O3Ua3AaE0adTQcS0fc17LrVrSp1sX+g4Zjp+vL1PHjwWgb/eu9B8aht1uZ8xQx6drZ8ybz4nTiZQoFsy4sKxmTzy6Zs824qcdc+gxeBhNGtSnSKFCJJw4Seyx4/zjtb8TuW493/60Ba0dn0Tu17Vzlss87fmmTfjvlKl07TuApo0bUaRwYeKPHSM2PoG2rVvRp3s3+rw3xPlJxvEUL1aMdZERAIybMpV+PboDzj7SZM5yXpQnNW1Yn59+/oWew0fTpH49ihR03ldOnKTNq38jasPnfLd1u/NauUrfTu3pP2Y8fr5++PnmYdKwwR7JobTOuY8VP4ky/DcG4Bi5Gau1/o/zMd4sHEWKBqZqrT9zbjMLeAM4juO/GdistV6llEoEQrTWl5VSIcAcrfULzgncHwP1cIxMTdZaf6mUehH4ELg9QWO81nqz878xCNNa73UebzhQTWvdSym1CmgInMIx2nX72IOAd4FzznlQd+0b2AN8jaPwU858q+91bsynjz3Rfyy5DTk3WfpB3bx2OacjZKv1hm05HeG+8gZmNcj6ZNnQ7oWcjnBf5gKef9znaflvPhnzg+7Fnpyc0xGylduQs48IH8RfoU80lquS5RMoKaDEA5MC6tH9FToLKaAenRRQj04KqEcnBZRn3KuAkjlQQgghhBBukgJKCCGEEMJNUkAJIYQQQrhJCighhBBCCDdJASWEEEII4SYpoIQQQggh3CQFlBBCCCGEm6SAEkIIIYRwkxRQQgghhBBukgJKCCGEEMJNUkAJIYQQQrhJCighhBBCCDdJASWEEEII4SYpoIQQQggh3JQ7pwOIvw671ZzTEe4rl59fTkfIVpolKacjZCtvoH9OR7ivJGtyTkfI1q2UJzvjTbs9pyNk61Zqak5HuC+VKxd2myWnY9yX9n+yr2UAu82a0xEemoxACSGEEG560osn4X1SQAkhhBBCuEkKKCGEEEIIN0kBJYQQQgjhJimghBBCCCHcJAWUEEIIIYSbpIASQgghhHCTFFBCCCGEEG6SAkoIIYQQwk1SQAkhhBBCuEkKKCGEEEIIN0kBJYQQQgjhJimghBBCCCHcJAWUEEIIIYSbpIASQgghhHCTFFBCCCGEEG6SAkoIIYQQwk25czrA/1VKKTtwOMOiT7XWM++z/jdAR+ePHbXWi53LywBxQALgC+wFemmtU++zrzJAY631+of/DTwjzW7ng/ClnL1wiSYhdejW9o1M7cOmz8ZksZAnd24mDhpAcOFCrPjin3zx7Q+0eukFBnRs5/V8Ez+cw5lz52n2bEN6dmyfqX3aRwv5cdsOBvXpSduWLe65zNsZpy1dwdlLl3iuTi26vtEyU3vY7AWYLVZy587NhAG9KFqoIB8siSLxzFn8fH1546XnefW5Rl7N2OvFhlQILsLJC5eJ/N8u1/LapUvSqWk9UtLSWPLDTs5cvcHLz1Si3bN1iDtzgXn/2erVXNkpFGRgRseWlClSkBbTl2G/pXMsS5rdzuT54Zy9cJGm9evR/e22mdpnLo7gx52/Etq1E21efRmAvmMmAGC2WilepDBzx49+LFkXz5/Hsbg4KlauzLvDwlzL161cweZNG3n19db07D/gsWRJS7Mz8cPZnDl3jmbPNqJnpw6Z2qP3HyA8cgW+vr5MGzua4KJF+PcP/+XzrzZjNBiYMnokhQsV5Iet21j96ecopejVqQPNmzzn+ax2O5PnLeTM+Qs0bRhCj3feytQ+I3wpP+7YSWiPLvzj768A0HfkOLTWKKXo3bEdDWrX9HgugLS0NCZMn+noCxs/S68unTK17963n0XLluPn68u0CeMoVrQoZ8+fZ/rc+dhsNlq++gptW7Uk7P2JXLl6jVu3bjFp9EjKln7a81mfkPuKjEB5j01rXTvD1z2LJwCt9Wta6+tAfmDgHc0ntda1gWeAp4B3sjl2GdKLsRy1Y88+ypQsyfLpkzgYl8Dla9cztQ/v1Y3l0ybR7R+t2fCvbwB44+XmTBkS+ljybdv5K2WffppViz7iwOGjXL56NVN7366dGNK/T7bLvOnnfQcpXbI4yyaN5VDCca5cv5GpfVi3jiydNIaurV/j029+cC2fFNqXxRNGeb14KhdcCP88eRiz4d/kyZWLCsUKu9raNa7D+M++Ye6/ttCxST0Adp/4jQmff+vVTA/KZEtm2Oqvif3zQk5HYfvuPZQt9RRRs6ZxMDaey9euZWrv3f5tBvfokmlZxIwpRMyYQsvmz9O0fshjyXk8Pp5km435y5aTmppGfOxRV9trrd9gzOQPHkuO27b98gtlSz/NqvAFHDh8hMtXMl/DEWvWsmTOhwzu25uo9RtIS7Pz+VebWbloPqG9e7Jyw6cArN24icj5c4mcP5e1Gzd5Jev2XdGULfUUK+bO5ODROC5fveM17vgOg3t3v2u7JTM/IGLWNK8VTwBbf/6FsqVLs3pJOAdiDnP5ypVM7RErV7Pso7kMHtCPqE/WAbBoWSRTxo4mKnwBbVs53tjNmPg+Kz9eyKC+vVn/hXfO45NyX5EC6jFSSuVTSiUopSo7f96glOrj/D5RKVUYmAmUV0odVErNzri91toORAMlnduUUUrtUErtd341dq46E2jq3MdQpVQupdRspdQepVSMUqqfc/viSqntzvWOKKWaevp3PpxwnAa1agAQUqMacSdOZmovGVwUgFy5cuGTy/HnWCh/fpRSno6SpUOxsTSsVweA+nVqcTQ+IVN7kUKF7tomq2XedPj4SerXqAZAvWpViD15OlN7iaJFAMiVywcfH8c5VAqmLI4kbPYCzl267NV8VUoU5WDiGQAO/naGyiWKZmpPSU3jmsVGsfxBAJhsKdhv3fJqpgd1M82OOTklp2MAcDg+wXWDDKlZg9jjJzK1Fy5Y4J7bbo/ew/ON6ns1322xR2KoW78BAHXrNyDuyBFXW4FChYDHc+3eduhoLA3r1QWgfp3aHE1Iv4Ztycn4+flhCAzkmWpVOZX4GzeSkihapAi5cuWiUvnyHI6NA6BMqVLYkpOx2mwYAgO9kjUmLp4GdWoBEFLzGWKPHc/UXqRgwbu2UT6KgWMmMGbGbG6YTF7JBXDoyFEaOYvw+nXrcCQu3tXmOo+GQGpWr8ap04mkpqVx9vx5Ppg1h/5Dw0j8/Q8A8uR2PNiy2mxUKl/eK1mflPuKFFDeE+AsTG5/tdNa3wBCgVVKqfZAAa318ju2G41zxElrPSJjg1LKH2gIfOdcdBH4m9a6LtAOWJhhHzuc+/gI6AXc0FrXB+oDfZRSZXGMUn3vHN2qBRy885dQSvVVSu1VSu1dtfFLt0+CyWLBEBAAgCEwkCSz5a517PZbrNz0FW1fednt/T8qk9mC0eDoLI0GA0km82PPkB2zxeo6h8bAQEyWLM7hrVus+urftHn5BQAGdW7H8inj6NKqBYvWfubVfAY/P2w3HU+UrSk3Mfr7ZWrPHxhAyYL5KFUwv1dz/NWZLFaMGV7nrK6VrFy9fgOlFAXy5fNmPBezyUygwQCAwWjAbEp6LMe9F5PZjDEw4zWcXmQkmdLbAG7dukX+fHk5e+48NpuNPQcPuoqSl5o1oUOfAbTr3Y/2bdt4J6vF4irOjIZAkszZ9zezxo0iYtY0mjVqQNSGz72SC8BkNt3RF2Y8jyaMztccHP3N9es3OH7yFO+PDCNs0Lt8tHgpAKmpqXQbEMqMjxZQs3o172R9Qu4rMgfKe2zOwiQTrfV/lVJvAx/jKFoeRHml1EGgIvCF1jrGuTwPEK6Uqg3YgUr32P4VoKZS6vYD93zOfe0BViil8gBfaa3vKqC01hFABMCNo/sfeILIJ1/9i537DnDqjz9p87cXAbDYbJQqFnzXugtWfcJrzzflqSzavGXVp5+zY1c0pxJ/c81jslgslCpZ8rFlyM7af33LLwdiOP3nGVq/2AxwnMOngovete7CTz6lRdPGrrZ8RiMAtapUYvGnX3g1pyUlhQDfPAAE+vpiSb7palu1LZqw1s25dMNM3Jmcf0z2JFrz5Vf8vGc/p37/wzW3yWyzUip/8QfaftvuaJ5v+HhGnwCMQUasziLearFgNAY9tmNntOrTz9jx627HNfy64/GRxWqlVMkSrnXyBhkxW62un5VS5MqVi77dOhM6ehyVK1Sg9FNPAfBx1Co2rYoEIHTUWBp78JHomi++5OfofZz87XfX3CaL1UapEsWy3TZfkOP8Nm/ciH/9938ey3TbynUb2PHrLk6ePs2brVs5s1koVfIp1zp5g4IwZ3jj5uPjQ1CQkXJlSlOwQH4KFsjP9RuOqQV58uRh9ZJwYuMT+DhyBQs/nO6xrE/afUVGoB4zpZQPUBWwAXeP12bt9hyoCkAjpVRr5/KhwAUchVgIjknmWR4WGJRhPlZZrfUPWuvtQDPgDPCJUqrrw/1Wd+vSphVLP5jAmP692RPjGOLfdySWahUyD+l+/eMWUIqWzZt56tAPpHv7d4iaP4fxwwYTvf8AAHsOHqJ6lXvVoI9f51YtWDxhFKN6d2PvEcdjhn2x8VQtXzbTepu3bEcpxWvN0ie9Wqw2AH47ey7TO3BviD97kVqlHTetWmVKkHD2oqst4exFxn/6DZ/vOsgfV67faxf/X+vatg0RM6Yw9t1+7Dnk+NzJvpgjVK9Y4YG237ormhcaNfBmxEyq1ajJgb17ANi/J5qqNZ55bMfOqHv7dkQtmMf44UOI3r8fgD0HDlK9SmXXOgH+/qSkpGC12jgcF0+5MqUBeOG5xkQtmEfzJo2pU9OR39c3D/5+/gT4+5OalubRrF3fakvErGmMe28g0Qcd73/3HjpMtUoVs93WbHEUgIdi43iqePYFl7t6dOrAivAFvD8ijN179wGOifc1qlZxrZN+Hq0cjo2jXJnS+Pv5ERgQgC05mQuXLmE0GNBau86dwWDA388vy2M+rCftviIjUI/fUByfqhuLY/Tn2Ts+UWcCsnxLp7U+p5QaDYwBNuMYSfpTa31LKdUNyHWPfXwPDFBK/U9rnaqUqoSjaCoMnNFaL1dKGYC6wBqP/aZA05C6/O/XaPqMnUTjurUpXLAAx04nEnfyNG+83JxZESuoVrE8/d+fQt3qVenb/m2+/nELm777LzfMZkxmCyP79vRkpEyaNW7EjzN30H3QUJo0bECRQoWIP3GSuIRj/KNlC5avXc+3P24BNJcuX6Fft85ZLvOmJnVrsSV6H/0mTefZ2jUpXCA/xxJ/J/50Iq2bN2POirVUK1+WgVM+pE7VyvR5uw0TP47AZLGgUIzo1SX7gzyCUxeucDPNzowOr5N46QqXTGbeblSbjbsO8naj2tQqUwKTLYXF3/8MQEj5UrzVsBbF8udl9BsvMfPrn7ya735y+fgwq3MrygcXYlbn1kT+tCvHRsqaNQhh0vxweo0cx3MhdSlcsAAJp04Td+IkbV55majPvuD7bTvQwKWrV+nT4R3MVismi4XiRe8elfSWilWq4Ovry5B+fShfsSJFiwWzbuUKOvXoybebv2bzpi8wJSVhNiXx3ohRXs/TrPGzjms4dDBNGjV0XMPHTxB37Dj/aNmC3p070j9sJL6+vnwwZiQAMxcs4mTibxQPDmbs0PcAeLt1K7oPGgzAm6+3vOfxHkXThvX56edf6Dl8NE3q16NIwYIknDzleI1f/RtRGz7nu63b0Vpz6cpV+nZqT/8x4/Hz9cPPNw+Thg32Si6A55s05sep2+g2INRxHgsXIv7YcWITjtG2VUv6dOtC3yHD8fP1Zer4sQD07d6V/kPDsNvtjBk6mJs3bzJg+EjHLDilGDd8iFeyPin3FaV1zn1s9/+yLP4bg++AFcDXQAOttUkpNQ8waa0nKqUSgRCt9WWl1HqgJvAtjkd9/9Za13DuV+GYqxQKnAc2AVZgC45RJqPzkdx3OAqkVcACYCrQCsdo1CWgjfNrBJAKmIGuWuvMM5QzcOcRXk7wLfB4J3c/DNv5P3M6Qra6/xiX0xHuK8manNMRsvWvji/kdIT7ul7E8x8t97TCyTeyXykH2W0PNkctJ+XJ++TPO0y+cDanI2QrX/W6Wc4+lwJKPDApoB6dFFCPTgqoRycF1KOTAsoz/soFlMyBEkIIIYRwkxRQQgghhBBukgJKCCGEEMJNUkAJIYQQQrhJCighhBBCCDdJASWEEEII4SYpoIQQQggh3CQFlBBCCCGEm6SAEkIIIYRwkxRQQgghhBBukgJKCCGEEMJNUkAJIYQQQrhJCighhBBCCDdJASWEEEII4abcOR1A/HWkPF0lpyPclw/2nI6QLXveojkdIVsbCgXndIT7upWSnNMRstVq/dacjnBf34e+ldMRsnXDWCinI9yXObcxpyNkq4jfk3+L33ghNacjZKt39ayXywiUEEIIIYSbpIASQgghhHCTFFBCCCGEEG6SAkoIIYQQwk1SQAkhhBBCuEkKKCGEEEIIN0kBJYQQQgjhJimghBBCCCHcJAWUEEIIIYSbpIASQgghhHCTFFBCCCGEEG6SAkoIIYQQwk1SQAkhhBBCuEkKKCGEEEIIN0kBJYQQQgjhJimghBBCCCHclDunA4j/mxbOnUNCXCyVqlRhcNhI1/JTJ04wZ8Y0tNYMHzOWChUrkXTjBnNmTOPG9evUq9+Arr16s+XH/7JhzWpQii49etL0heYezTd37lzi4uKoUqUKYWFhruUnTpxgxowZaK0ZM2YMFStWZM6cORw7doyUlBSGDh1K7dq1iYqKYuPGjbRu3ZqBAwd6NNttC+fOJj7WcQ6HjBjlWn7qxAlmT58KaIaPGUeFipWYNvF9Ek+fxs/fj9b/eJNXWrzGJyui2PXLTlJSkunSozfPv/iiV3ICpNntTJrzEWfPX6Bpw/r0aP9OpvYZCz/mvzt+ZlDP7vyjxasADH5/MiaLhTy5czNlxDCCixT2ar7J88M5e+EiTevXo/vbbTO1z1wcwY87fyW0ayfavPoyAH3HTADAbLVSvEhh5o4f7bV891MoyMCMji0pU6QgLaYvw35LP9bjp6WlMWH6TM6cO0+zxs/Sq0unTO279+1n0bLl+Pn6Mm3COIoVLcrZ8+eZPnc+NpuNlq++QttWLQG4eOkyr73TgS8/WcnTTz3lsYyP2t9ktcxbli2cz/H4OCpUqkz/IcNcyzesXsm/v9zEKy1fp1vf/o7fa9ZMfjt9ElCEDh9B2QoVvZbLnT4xq/5v2rRpnDx5EqUUo0ePpmJF72X938Z1nP/tNMFPl+Gldzpnaku9eZOI94fTsns/ylStQWLcEX7evIncvr78rUM3ChUr4bEcMgLlBqWUXSl1UCl1RCm1USkV6MF9l1FK/amU8rlj+UGlVIP7bNddKRXuqRyekBAfR3KyjY8jV5Camkrc0aOutsili5k4bQZTZs4iasliAFYuX0av/gNYsDTC1XF9vn4tCyOWs2jZcj5bv9aj+eLj40lOTiYyMpLU1FSOZsi3dOlSpk2bxsyZM1myZAkAQ4YMISIigpkzZ7Jy5UoA2rRpw9SpUz2aK6OEuDhsNhuLo1aSlppG3NEjrrblSz5m0vSZTJk5i0jnOQSYOG064RFRvNLiNQA6dOnKx5ErWLgsknWrV3otK8C2X3dR9ulSrPhoNgeOxnL56tVM7b07dWBIn56Zlo0c2I8V82bRo93brPvyK6/m2757D2VLPUXUrGkcjI3n8rVrmfO1f5vBPbpkWhYxYwoRM6bQsvnzNK0f4tV892OyJTNs9dfE/nkhR46/9edfKFu6NKuXhHMg5jD4jNJ9AAAgAElEQVSXr1zJ1B6xcjXLPprL4AH9iPpkHQCLlkUyZexoosIXuIongLWfb6Rm9WoezeeJ/iarZd5wIiGeZJuNOYuXkZqWSkJcrKvt761aM3Li5Ezrv9O5K3OXLGfY2PGsWxnltVzu9olZ9X/du3dnxYoVTJw4kYiICK9lvfB7Iqk3U+gYNh57WhrnEk9lao/5eQuFi5d0/fzLf77inSGjeb3nAHb++0uPZpECyj02rXVtrXUN4CbQ31M71lonAn8ATW8vU0pVAYK01tGeOs7jcDQmhpAGDQEIadCQo4djXG1JSTcILlaMIkWLYjabATh18iSfrIjivX59OBJzCIBSpctgs9mw2qwYDEaP5ouJiaFBA0dN2qBBAw4fPpwhXxLFihWjaIZ8uXM7BmqtViuVKlUCoFChQh7NdKcjhw+ln8OGDTmaIaMpKcl5DoMxm00AKKWYOmE8I4e8x/lzZx258+QBICUlmXLlK3g1b0xsPA3r1Aagfq2aHE04nqm9SKGCd21TsngxAHLl8sHHx7td0eH4BBrUrglASM0axB4/kam9cMEC99x2e/Qenm9U36v57udmmh1zckqOHf/QkaM0chaQ9evW4UhcvKvNlpyMn58fBkMgNatX49TpRFLT0jh7/jwfzJpD/6FhJP7+BwBXr13HYrVSolgxj+bzRH+T1TJviDtyhDohjr+lOiH1ic/wxqhAwUKoO9YvVsIxWpI7d258fHJ5LZe7fWJW/V/JkiVdWXPl8l7WM6dOULpKdQBKV6nOudPp17I9LY1zp0/yVIVKmbbx9fPDmC8/1y9d9GgWKaAe3g6gAoBS6iul1D6l1FGlVN/bKyileimljimltiqllt8eKVJKFVFKbVJK7XF+PefcZAPQPsMx2juXoZRqpZTarZQ6oJT6USkVfGcgpdQqpdRbGX42Z/h+hPNYMUqpyc5lBqXUf5RSh5yjau08cWLMJhMGgwEAo9GIyZTkatMZHj9ofQuAIzGH6NyjJ5Omz2TxgvkAPN/8RXp37kjPju15s13GU/LoTHflM7nabt26lSFfetawsDBCQ0NdnYy3mU0mDEZH4WgwGjElpZ/DTBmd5zN06HCWrlxDp249CP9onqt9zoxpdG/3DnXre7cAMFssGAIdA7JGgwGT2ZzNFg52u52oDZ/xZssW3oyHyWLFGBAAgDEwkCSz5YG2u3r9BkopCuTL5814TzST2YTRkP7aJmW4XpJMJozOawnAfusW16/f4PjJU7w/MoywQe/y0eKlgGP0qcObmR+deoIn+puslnmD2Wwi0JnVYDBiznAu72fl0iW0futtr+V6mD7xXsLDw2nf3rN9dkYpNit+/o5r2S8ggGSr1dV2+NftVGv43F3bWJJucOX8Wa6eP+vRLFJAPQSlVG6gBXC7TO+pta4HhADvKaUKKaVKAO8DjYC/AVUy7GIB8JHWuj7wJhDpXP450Ma5f4B2wKfO738GGmmt6ziXjeQBKaVeASoCDYDaQD2lVDPg78BZrXUt56jad1ls21cptVcptXfNyhUPdDxjUBAWi+MGZbFYCAoKSt+fT/p7rNtPK0s9/TRlypajYKFC+DjbI5cuZs1nX7B245esWu7Z4eCg++TLOBKiVHrWOXPmsHLlSj7++GOPZrkXY1AQFmcRYrVYMN4ro/N85XXe4GvVqcOVy5dd7WFjxrFu0z9ZsyISb1j9+Sb6hI1myy+/YnF2ZBaLlSCjIZstHeZFRPH6yy9RqkRxr+Rb8+VX9B0zga27ojHbbACYbVaCDA+Wb9vuaJ5vmHOjTzlp5boN9AwdzJYdP2O2OF9bq4UgY/rfYt6gIMyW9GLUx8eHoCAj5cqUpmCB/FQoV5brN26QZDJx/uJFKpQr6/GcnuhvslrmDUZjEFZnVqvVgtGY/ej6Pz//lKfLlqVGrdpey/UwfWJW1q9fT7ly5ahd23tZ/QICSUl2XMs3k234BTiK+1t2O4mxhylXo1am9Z9v245/RS1m9/f/pkT5Snft71FIAeWeAKXUQWAv8Dtw+6H0e0qpQ8AuoBTpxco2rfVVrXUqsDHDfl4Gwp372gzkVUoFaa3PA0eBl5RStYFUrfXtMd6ngO+VUoeBEUB1N3K/4vw6AOzHUcxVxFEAvqyU+lAp1VRrfePODbXWEVrrEK11SNcePe9szlL1mjXZF+146rg3ejfVatR0teXNm4+LFy5w+dJFV+dR6unSXL58CZvNht1uByBPHl/8/P3xDwggLTXVjV81ezVr1iTamS86OpoaNWpkyJeXCxcucOnSJVe+mzdvAmAwGAhwjmJ4W41narFvjyPjnt27qP7MM662oLx5M5xDR0d3u9j6PTHR1fndzu3n7+/xx6C3dXvnTZbPmcm4waFEH3A8+thzKIZqlbLvqL769nsU8PrfXvJKNoCubdsQMWMKY9/tx55Djvc7+2KOUL3igz3S3LormhcaPZ5RxydNj04dWBG+gPdHhLF77z4AovcfoEbV9PeCAf7+pKSkYLVaORwbR7kypfH38yMwIABbcjIXLl3CaDCQ+Psf/P7Hn/QfNoJde/bywex59zqs2zzR32S1zBuq1qjBwX17ATiwZw9Vqte47/r7oncTd/gwHbr18FomcL9PzMquXbuIiYmhV69eXs1aslwFfot3zB37Lf4oJcqVB8BiuoHp2lU2LppNbPQv7Ph6I8kWCyXLVaT90DE826K1RyeQg3wKz102rXWm0lop9QKOguhZrbVVKbUV8Ie7Hmdn5ONc35ZF2+3HeBec39+2CJintd7sPOakLLZNc+4b5Xir4Hs7JjBDa73szg2UUvWA14AZSqkftNZT7pP7gVSuUhVfP1/e7d2TChUrEVysGGuiIunaqzc9+/Vn0tjRaK0ZNmoMAD379Wfy2DGkpKTQo4/jCWibt95mYC9Hp9GqrWeH/atUqYKfnx+9e/emYsWKFCtWjKioKHr16kW/fv0YO3YsWmtGjXJ88m3MmDGYzWbsdjuhoaEAfPXVV3zxxRckJSVhMplc63pK5apV8fX1ZWCvHlSoVIngYsVZHbWcbr360Kv/ACaOGQWkn8PJ48diSkpCKUXYmHEALJgzi98ST5OWmkrHrt08mu9OzRo1ZNKOj+g5dATPNQihSKGCJJw8SdyxE7Rp8SqR6z/luy3b0Fpz6coV+nbuyIzwxVSvXJk+YaOpW7MGA7p2zv5AD5uvQQiT5ofTa+Q4ngupS+GCBUg4dZq4Eydp88rLRH32Bd9v24EGLl29Sp8O72C2WjFZLBQvWtRruR5ELh8fZnVuRfngQszq3JrIn3YRd+bxTSh/vkljfpy6jW4DQmnSqCFFChci/thxYhOO0bZVS/p060LfIcPx8/Vl6vixAPTt3pX+Q8Ow2+2MGTqY6lWrsDbCMQF5/NQZ9O3e5X6HdIsn+puslnlDhcpV8PX1JWxgP8pVqEiR4GJsWL2SDt168P2/N/PvLzdhMiVhNpl4d/gIlnw0l0CDgVGDBvLU06V5b6R3Pgnqbp+YVf83a9YsjEYj/fr1o3Tp0owbN84rWYOfLkPuPHlYP2cqRZ96mqAChfj1280826I1XUY7JuHv/PeXlCxfCX+DgV+/3cxv8UcJMBh5pWN3j2ZRD/JMUzgopcxaa+Mdy94AemutWzknfR/E8WjsOLATqAOYgJ+Aw1rrUKXUeuCA1nq2cx+1tdYHnd/nBxIAK/Ci1vq0c/kB53H2KaVWAmW11i8opboDIc79jscx6XyUUqoN8E+ttXI+wvsAeElrbVZKlQRScRTQV7XWyc71u2ut29zr979osj7RfywBeO+do6ckqyf/PUvglTM5HeG+bqUk53SEbLVavzWnI9zX96FvZb9SDrvhlzenI9xXTk7sf1BF/J/8/uazPbHZr5TDer/YMMsBkSf/7D75vgP6K6VicBQ+uwC01meUUtOB3cBZIBa4/YjsPeBj5za5ge04P9Gntb6ulNoFBN8unpwmARuVUmecx8hqMsFy4GulVDSOgs3i3OcPSqmqwK/OZ9hmoDOOSfCzlVK3cBRUAx79dAghhBD/98kIlBcppYzOEZ/cwD+BFVrrf+Z0roclI1CPTkagHp2MQD06GYF6dDIC5Rl/5REomUTuXZOcE8WPAKcB7/5vgUIIIYR4LJ788vQvTGsdlv1aQgghhPirkREoIYQQQgg3SQElhBBCCOEmKaCEEEIIIdwkBZQQQgghhJukgBJCCCGEcJMUUEIIIYQQbpICSgghhBDCTVJACSGEEEK4SQooIYQQQgg3SQElhBBCCOEmKaCEEEIIIdyktNY5nUH8RSRfPPfE/7GkBhhzOsJfnlU/+e+rbtrtOR3hvorarTkdIVuvhn+R0xHu64u/V8/pCNkylK6Q0xGylWbMn9MR7iuPNSmnI2TLP7ikymr5k99TCvGApHj6/8OTXjz9FTzpxdNfgRRPQgooIYQQQgg3SQElhBBCCOEmKaCEEEIIIdwkBZQQQgghhJukgBJCCCGEcJMUUEIIIYQQbpICSgghhBDCTVJACSGEEEK4SQooIYQQQgg3SQElhBBCCOEmKaCEEEIIIdwkBZQQQgghhJukgBJCCCGEcJMUUEIIIYQQbpICSgghhBDCTVJACSGEEEK4KXdOB/irUkqNAzoCduAW0E9rvdtD+/4G6Ki1vn6fdRKBEK31ZaWUHTgM5AHSgNXAfK31LU/kcVdaWhoTZnzImXPnaNb4WXp17pSpffe+/YQvj8TX15fp48cSXLQovQYNBsBksVAiOJj5M6bx4YJFJBw/QcrNm4SFDqROzWc8mnHy5MmcPXuWpk2b0r1790zte/bsYfHixfj6+jJlyhSCg4MZPXo0V69exW638/7771OmTBlmzpzJjz/+SGhoKG3atHni8p07d44PP/wQm81GixYtPJoRYNG8OSTExVKxclUGh41wLT914gRzZ05Ha83w0WMoX7ESk8aO5uqVK6Sm3iQlOYUV6z9l7coV7Pp1JzeTU+jcoyfNmr/o0XxZWTx/Hsfi4qhYuTLvDgtzLV+3cgWbN23k1ddb07P/AK/nAOe1Mn0mZ86dd1wrXe6+VhYtW46fry/TJoyjWNGinD1/nulz52Oz2Wj56iu0bdUSgIuXLvPaOx348pOVPP3UU48lP0ChIAMzOrakTJGCtJi+DPst/diOfac0u53pkWs4d+kyjWs/Q5fX/56pfeRHH2Oy2siTOxfj+3SnaMECRB+JZfmX/8IvTx5GdOtI6RLFPJ8rzc7ED2c7+sRnG9GzU4dM7dH7DxAeuQJfX1+mjR1NcNEirPviS7796X/4KMXwdwdQq3o1eg0eBoDZYqF4cDDzp03xUD73+5us+r6+ffs68pnNFC9enLlz53oknyOjnQkzP3RcK882olfnjpnad+/b7zqH08eNcZzDjZv45sef8PHxIezdAdSqUZ1Bo8diMlvIkzs3U53n+mHJCNRDUEo9C7wO1NVa1wReBv7w1P611q/dr3jKgk1rXVtrXR34G/AaMNFTedy1decvlCtdmtWLwzkQc5jLV65kao9YvYal8+YwuF9fotauByBq0QKiFi2g1auv0qzxswAMe3cAK8IXMHvKRKI+WefRjNu3b6ds2bJERUVx8OBBLl++nKk9MjKS8PBwBg0axKpVqwCYOnUqERERDBw4kM8++wyA3r17M3jwYI9m82S+xYsXM3HiRJYtW+bx4ikhPo5km43w5StIS0sl7uhRV1vUsiVMmDqdyTM+JHLpEgAmTZ/JwmXL6dClG882bQpA+y5dCI+IYv7SCNavWeXRfFk5Hh9Pss3G/GXLSU1NIz42PfNrrd9gzOQPvJ4ho60//0LZ0qVZveQe18rK1Sz7aC6DB/RzXQOLlkUyZexoosIXuIongLWfb6Rm9WqPNT+AyZbMsNVfE/vnhcd+7Dv9fCCG0iWKsWT8CGKOneTK9RuZ2od0bseScWF0bvkqn33/EwArv/6GhaOGMGlALyL/+S+v5Nr2yy+ULf00q8IXcODwES5fuZqpPWLNWpbM+ZDBfXsTtX4DAJu/+541Hy9k9uSJrNrguJ6jFswjasE8Xn/1bzR7tpHH8j1Mf5NV3xcREUFERAQtW7akqfMa95StO3c67isfL7znOVw6dxaD+/Vx3Ve+/u57PlkSzpwpE1m54VMARr03iFXhC+jZqQOffP7FI2WSAurhFAcua61TALTWl7XWZ5VSiUqpD5VS0c6vCgBKqSJKqU1KqT3Or+ecy41KqZVKqcNKqRil1JvO5YlKqcLO779SSu1TSh1VSvXNLpjW+iLQFwhVDmWUUjuUUvudX42d+/1EKfXG7e2UUuuUUq09cXJijhylYUg9AOrXrcOR+ARXmy05GX8/PwyBgdSsXo2TiYmZtt26cycvNGkCQJ7cjgFSq81GpQrlPRHN5fDhwzRo0ACAkJAQYmNjXW3Jycn4+flhMBioUaMGp06dAiC3M4/NZqNChQoAFC5c2KO5PJkvLS2Nc+fOMX36dEJDQ/ntt988mvFoTAz1GjQEoF6DhsQeOexqS7pxg+BixShStCgWsznTdju2bnGNNOXOnQeAmykplC1fwaP5shJ7JIa69R3ntW79BsQdOeJqK1CoEKC8niGjQ0eO0qh+COC8VuLiXW021+vsuFZOnU4kNS2Ns+fP88GsOfQfGkbi7473bVevXcditVKimOdHT7JzM82OOTnlsR83K0dOnKJ+tSoA1K1aibjTmf/mSxRxXK+5cuXCxyf9tQ7w86Nw/nycuXjJK7kOHY2lYb26ANSvU5ujCZn7RD9nn/hMtaqcSnRkLlWyJDdTUzGZzeTPmzfT/rbt/JUXmjT2WL6H6W/u1/dt376d559/3mP5AGLuOIdH4jNfK/6+vo77SrWqnHT2dU+XLJHhHOYD4KkSxQHH30CuXI9WAkkB9XB+AEoppY4ppRYrpTL+pSRprRsA4cB857IFwEda6/rAm0Ckc/n7wA2t9TPOkaz/ZXGsnlrrekAI8J5SqlB24bTWp3C8tkWBi8DftNZ1gXbAQudqkUAPAKVUPqAx8M2d+1JK9VVK7VVK7Y1asza7QwNgMpsxGgIBCDIYMJlMrrYkkwlDYKDr51u30p8yXrl2DaUUBQvkdy0bMnY8/YeNcBVknmIymTAajQAYjUaSkpLSMyYlYTAY7sqYmppKr169mD17Ns8847nHid7Kd/36dY4fP87YsWMZOnQoCxcuxJPMZpMrh9FgxJQhY8anx7cyfJ+WlsapEyeoXKWqa9m8mTPo3qEddUPqezRflplNZgKdmQ1GA2ZTUjZbeJfJbHJdK0aDgaQ7rhVjhtfZfusW16/f4PjJU7w/MoywQe/y0eKlgGP0qcObbR9v+CeQ2WrFEBAAgCEgAJPFetc69lu3WLP5W954oZlr2dUbSfx29jy/nT3vlVwmsxlj4L1e5/Q2SL+eG9atwz+69mDAiFF0ePMf6Vlv95P50/vJR873EP3NvVy9ehWlFAUKFPBYPrh9X3HkCDIaMJnS35glmUyZM9rtADSoV5c2nbvRf/hIOr6Vfg7tdjuRn6zjrdavP1ImKaAegtbaDNTDMdJzCfhMKdXd2bwhw7/POr9/GQhXSh0ENgN5lVJBzuUfZ9jvtSwO955S6hCwCygFVHzAmLffXuUBliulDgMbgWrOY20DKiiligIdgE1a67QsftcIrXWI1jqkV9fO9z3gqvWf0mvQYLbs+Bmzs+MyW6wEOS9MgLxBQVis6Z2aj0p/F7h1x06aN3ku0z7nT5/K2qWLWRSx/AF/7ftbs2YNffv2ZevWrZidIyNms5mgoKD0jHnzYrFYXD8rZ8Y8efIQFRXFzJkzWbp0qUfyeDOf0WikXLlyFChQgPLly3PjRubHGY/KaAxy5bBYzBgzZFQqvWvxyfD9gX17qV0vczE8bPQY1n6xiU9WRHk0X1aMQUaszsxWiwWjMSibLbxj5boN9AzNfK1YrBaCMuTJGxSEOcPr7OPjQ1CQkXJlSlOwQH4qlCvL9Rs3SDKZOH/xIhXKlX3sv8eTYt03PxA6Yy479h/CYrMBYE1OxhgYcNe6izZ8wd+fa8RTwY65L++2a8uExZF88p/veKaiZ0e6V336Gb0GD2Prz79gtt5+ne/sE42uNnBcz2aLha++/Y7Na1ezdnE4CyIiXe1bdv7CC895ZvTpUfqbe9m2bZtHR59WbfiUXu8NZcuOna7rwXFfSS+Y8gYFZcro4+PjOIf/+ZZ/rf+EdUsXs2Bp+j1k7sdLaPXqK5QqWfKRskkB9ZC01nat9Vat9UQgFMfIEkDGGZS3v/cBnnXOU6qttS6ptTbhKHLuOeNSKfUCjiLrWa11LeAA4J9dNqVUORyT2y8CQ4ELQC0co1i+GVb9BOiEYyRqZXb7zU73ju2JWrSA8SOGE71vPwB7DhygepUqrnUC/P1JTknBarVyODaOcmXKuNq27PiZ5k2buH6+efMmAIGBAQT4390RPoyuXbsSERHB2LFj2bNnDwD79u2jevXqrnX8/f1JcWY8cuQI5cqVQ2tNWpqjvjQYDPj5+Xkkjzfz+fv7ExAQQHJyMhcvXsz0Ds0Tqtesyb490Y6M0dFUyzAqlzdfPi5euMDlS5cwZLhZ7NiyhaYvNHf9fPs19vPzx2D0bL6sVKtRkwN7Hed1/55oqtbw7kjivfTo1IEV4Qt4f0QYu/fuAxwTiWtUzXytpGS6Vkrj7+dHYEAAtuRkLly6hNFgIPH3P/j9jz/pP2wEu/bs5YPZ83Lkd8pJnV57hfAxwxnRvRN7Yx2Px/bHJVC1XJlM6/1r204U0KJJ+vyhGhXKET5mGN1ataCMhyeQd2/fjqgF8xg/fAjR+2/3iQepXqWya53019nG4bh4ypUpjY/ywd/Pjzx58mA0GkhOTnatv+Xnu99oPqyH7W/uZ+vWrbzwwgseyQfQvUN7ohZ+xPiwoUTvPwDc475y86bjHDqvFR/lQ4C/v+sc2pzn8Mt/f4NSilZ/f+WRs8mn8B6CUqoycEtrfdy5qDbwG/AMjsdkM53//ups/wFHkTXbuX1trfXBDMuHOJcXuGMUKh9wTWttVUpVAbKdNaiUKgIsBcK11tr5eO5PrfUtpVQ3IFeG1VcB0cB5rfXRu/f2cJ5/rjETts2k28BQmjZqRJHChYg/fpzYhGO0fb0lfbp2pt+wMHx9fZk6bgzg+FSJyWzONIdj5MTJmCwW7HY77/Xr46l4ADRr1oxJkybRq1cvnnvuOQoXLkxCQgJxcXG0adOGnj178u677+Lr68vkyZO5efMmgwYNQimFUopRo0YBEBUVxffff4/WmkuXLtGnj2dyeipfr169CA0NxW63M2LEiGyO6p7KVari6+tHaJ+elK9YieDgYqxZEUnXnr3p2bcfk8eNQWvN0JGjAdBac/RwDENGjnLtY+Hc2fyemEhqairtO3f1aL6sVKxSBV9fX4b060P5ihUpWiyYdStX0KlHT77d/DWbN32BKSkJsymJ90aMyn6Hj+j5Jo35ceo2ug0IpUmjho5r5ZjzWmnVkj7dutB3yHD8fH2ZOn4sAH27d6X/0DDsdjtjhg6metUqrI1wTNQfP3UGfbt38XrujHL5+DCrcyvKBxdiVufWRP60i7gzOTOhvEntmkzdu4oBU2fTqFYNCufPx7Hf/iAh8XdaPf8cc9dsoGq5MoTOmEvtypXo3bYVqzd/w56j8eQzGhjZo1P2B3kIzRo/y48zd9A9dLDjdS5UiPjjJ4g7dpx/tGxB784d6R82El9fXz4YM5LAwACerR9C14GDsN+6Rb+ujtfU0U9aKFEs2LP53OxvIOu+z2w2YzKZKF68uEfzgfO+Mv1Dur37Hk1vXyvHTzjvK6/Rp0sn+g0f4bivjB3lOoddBjj6v37dHf3L9I8WUKNqFXq9N5R6tWsxsGf3h86ktM65j5z+VSml6gGLgPw4/tuAEzge5+3FMZLzGo5Rpw5a6xPOCeEfA1VxFK3btdb9lVJG5/J6OEaMJmutv7z9XxQAJuAroCSQABQBJmmtt2bz3xh8AsxzFk0VgU2AFdgCDNJau4YElFLfAV9prbN9JpV88dwT/ceSGmDMfiWRLat+sgembzrnNzzJitrvnnvzJHk1/NE+ffQ4fPH36tmvlIMMpb3/oYdHlWb03Dwpb8ljzdl5iA/CP7hkls8tpYDyoIxFTU5neRBKqUAchVddrXW2E2SkgPr/gxRQ/4+9O4+Lqt7/OP76ooIIuC+UlVsMbuG+ZGZ165Zds9t2b2bmipKKSYoL7pkobpmGosjinu22p7d7s819CxcYQ7Pd3WSGRQG/vz/mMA6CwOiMQ/0+z8fDhzPf75lz3nO2+cz3HOD6SQF1/aSAun5SQLnG1Qqo8n2mFG6jlHoASANeK0vxJIQQQojL5B4oF9JaN/R0hrLSWn8O3ObpHEIIIcSfkYxACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZyktNaeziD+JE5Zs8v1zlLBS3k6QqnOWbM9HaFUt3hd8HSEEl3KzfV0hFJZ/Gt5OkKJvA5u93SEUj312UFPRyhR0vBnPR2hVHUqV/R0hFKdysnzdIRSNa5To9gPFxmBEkIIIYRwkhRQQgghhBBOkgJKCCGEEMJJUkAJIYQQQjhJCighhBBCCCdJASWEEEII4SQpoIQQQgghnCQFlBBCCCGEk6SAEkIIIYRwkhRQQgghhBBOkgJKCCGEEMJJUkAJIYQQQjhJCighhBBCCCdJASWEEEII4SQpoIQQQgghnCQFlBBCCCGEkypMmzbN0xnEn0TWxbxpZZ120fy5rExM4PvDZjrf1dXefjQ9nQmRo/j4/fdo2rwFNWvVInrqZFYnJ7Hp04+pWLESTQ1ppwAAACAASURBVIKC2Lt7F1PGj+WjDe+Rn59P0+YtSl2ml1Jlfi+vzpvLioTlfG82c6dDviPp3xM1ehQfbthAMyMfQE5ODv/s/iBBpmDq33ILL0+ZzKqkRDZ+8gkVK1Xi9qCgMi0352JemTM6il/0KutXJXM0/Xvad77T3r5+VTIxUydjsWTQul17e/uFCzn0e/JRGgcFcdPN9Z1aVlWVX6bp8vLymDRjJqvWv8nZc+do2yqkUP/23XsYP206H2/8D53at8Pfz4/fjh8n6qUZvPPBh+TnX6JZsIkPP9vIy3Pms+mLL+ncvh1VqlQpcbn60qUyv5e8vHwmz5rD6jff4uy5P2gTckeh/h179hL18kw+/s9/6dS2Lf5+fny06T/MmP8q/9n8FZ3ataVKFV82bf6SKTFzee/jT6lRvRqNbrutxOVe9C75PThaNH8eq5ISSD9spnOXu+ztR9PTmThmNB+9v4GmzZtTs1YtMs6fJ3raFDa8/RanTpygVdu2xbaVRp36tcz5CuTl5zNj+UrWf/Y55zIstDLdXqh/7ILFvPu/r9i0dTvtmjfFz9eXHQcOMW1pEpu27KBFk0ZUD/Av8/LeTD/ldMarqRXgx8IBjzPi4btZ981utL7+ef6zY0jpExVj2aJXWb8ymaPfFz6WX19pO5atDsfy0ldf4Y01q/jkvXe5tWFD6tYLdGpZfhWvbYxk/vz5JCUlcfjwYbp06WJvT09PZ8yYMbz//vs0b96cWrVqER0dzYoVK/jggw/sbc7Iyiv78VzgRq5DgBp+vi8V1y4jUOWUUkorpVY7PK+olDqllProGudXXSk1zOH5vdc6r9KYU1PJzs5mSWIyebl5pB48YO9bHreYaTNjmB4zh4S4Jfb2qdEziY1P5MGH/wHA+jWreXn2XJYmr+TjD953eb6c7GyWJq0gNzeXQw754pcsZvqs2cyYM4f4JYvt7e+/+w6NmxT+wJgWPYslCYk8ZGR2l3RzGjk52cxdvIy83FwOpx6y9z30yKOMmVL02P70g/dp0KixW3Nt/mYLjRo0YGVcLHtT9nP6zJlC/fHJK1m2YD4jh4aRuHotAK8tS2D6hPEkxi7kiZ49yMvL4413N7AyLpYXhoSStOZ1l2b8cssWGjW4jRWxC9m7/wCnz5wtnHHVGuLmzWbkkFAS171OXl4+b274gOTXXiU8dCDJr68HYM1b75Dw6nwSXp3PmrfecVk+c1oqOTnZLE5IIjc3l9SDB+19CUuXMDV6FtNj5pBoHCvJy5cx6PmhLFwaT99BoVdtc4dv9qbQ4OZA4iaNIeXwEc78cb5Qf0Sfp4mbGEmfHg/xxsb/2rK9/wmLxkUwbeggEt770G3ZSmPJzmHUyvc59MsJj2UA41jOzmbekmXk5uVidjiWu/d8lLFTCx/LoeEvMDc2jqiXo3lj9cobkjEtLY2cnBwSEhLIzc3loMM+uXTpUqKjo4mJiSEuLg6A/v37k5SUxNSpU4mPj3d7vvK0DqWAKr8ygZZKKV/j+d8B5782XlYdGFbqVC5wYP93tO/YCYD2nTpxcP9+e58lI4N6gYHUqVsPq9UCgFKKGVMmMTbiBY7//hsAjRo3IdNqJffiRXx9fYsu5DrsT/mO9p1s+Tp06szBlBR7X8Z5W766dethtVoBbCeR/ftp1aaNfTqlYPrkSUSOHMHvv/3m0nxXSj14gNbtOwDQun0H0hwKvho1a3HlwFtubi7mQwdpHtLKrbm+O3CQzh1s3/I6tG3DgdQ0e192Tg4+Pj74+VUhpEVzjv5wjNy8PH47fpyX58zj+RcjOfbTz5zPyKBe3TpUqFAB0+1NSHE4Wbsk48FDdGpnG5Hp0KY1B83mohmrVOGO5s04euxHzmdkULeOkadJE/YfSgWg4a23kp2TQ1Z2Nn6ljJA542BKyuVjpWMnDu532BczzhvHSl37vnj0yBFWJyXyQthgDqR8d9U2dziQfpQOzZsC0LaZidQffizUf3Od2gBUqFABL6/LO6Wvjw+1q1fj15OuG1Fy1sW8fKw5Fzy2/AKpBw7QxjiW2xR3LF8xfcWKFQHIycqm8e1lG+W+XikpKXTs2BGAjh07st/h/J2RkUFgYCB1HfbJ+vXr27NWqFDB7fnK0zqUAqp8+xToYTx+BrB/PVdK1VRKbVBKpSiltimlQoz2aUqpJKXUZqXUUaXUC8ZLYoAmSql9Sqm5Rpu/UuptpVSaUmqtUk5cAyuB1WLBz982VO/n748lI8Ped8nh8ou+ZBtHD39xNEuTV/FsvwHELngFgLvvvY+xES/Q+8nH7aNSrmK1WPDzu5wvwyGf1o75bI8/en8DD/d4pNA8RoyKZPnKVTzXfwCvvTLfpfmulGm1UMXPz57XarGUOP1/PvmIvz3U3a2ZACxWC/5+tmLC38+PDIdcGRYL/kZmgPxLl/jjj/N8f+Qok8dGEjliOAuWLKV6tWr8+ttxsrKz2blnL+czSn5vzme04l/lahkv94Ft36xerSq//X6c7Oxsdu7bx3lj+vu7deWZwUN5OjSMXk885rJ8tn3Rtp78/f2xWBz2xUuXrzMV7JcHUr6jz4CBTJsZw5KFr161zR2sWVn4GV9m/Hx9sWRmFZkm/9IlVn3wKf+8t5u97ez5DH787Tg//nbcbdn+LKyOx7Jf6ccywPSocUwcNdL+JcrdLEX2ycsZC52/r7gOGhsbS69evdyerzytQymgyrf1QC+lVGUgBNju0PcSsFdrHQJMAFY59DUFHgI6AlOVUpWA8cARrXVrrfUYY7o2QATQHGgM3MUVlFJDlFK7lFK7ViUllim0f0AAmca3k6zMTPwDAux9Xl6XdzllfEutWq0aAK3atOHM6dMALFm4gCWJyazf8AGfffwROdnZZVp2mfNlFuSzEuCQTynHfF7k5eWxfesW7uzatdA8qtkzt+XMmdMuy1YcP/8AsjIzjbyZ+JVwH0l+Xh57dmyjQ+cuV53meiWvfZ2B4SP54utvsBofoplZmQT4X16PVQMCsBqZwbbdAwL8adywATVrVOf2xo344/x5KlSoQNiAfgyPHMdXW7bR4LZbXJJxxfo3GDRyFJu/2YI1qyBjFgH+l9dd1QB/ex/YRkIrVKjAkH59CB8/ka+3bqfBLbY8ixNX8M6KBN5bmUT8ytW4im1ftK2nzMzMwvuiwyhOwX5562230bBRY2rWqmUf5SmuzZXWfrKJ8Fnz+XrPd2Qax2FWTg7+VYqODL/2+tt0v6szt9SrA8Dwp59gypIEVn/8GXcENXF5tj8bf8djOSsTf//S7wmbMms2C5YlsGJZnLvjARBQwj5Z6Pzt8H173bp1NG7cmNatW7s9X3lah1JAlWNa6xSgIbbRp0+u6O4KrDam+x9QSylVzej7WGt9QWt9GjgJ1LvKInZorX/Rtq+3+4xlXZkhXmvdXmvdvu/AQWXK3fKOVuzeuQOAndu30eKOyzfuBlStyskTJzh96iT+xgduQbH107Fj9oPV9oEbQKVKlVBKkZd3bTdfF+eOkFbs2rHdyLedFiGXbwatWs2W79TJk/j7+3P27BlOHj9BxPChfPbJxyx9bREZGRn2zD8eO1aoQHSHZi1a8t3uXQDs27WTps1bXnXac+fOcurkSSaPjuCLTRtZsSyu0KiGKwx49hmSYhcyeUwk23ftBmw3Y7ds1tQ+jW/lyly4cIGsrCz2H0qlccMGVPbxoYqvL9k5OZw4dco+QnXf3XeRvHgRf+vWtciN6Neqf6+nSVz4CpNGR7Bjzx4Adu7dR4umwcVkzGZ/ahqNGzYA4N67upC48BXu69rFftO5t3clKvtUxrdyZXJduC+2CAlh9w7bsbJrx3aat3TYF6tWczhWbB8St97WgNOnT5GdnU1+fv5V21zp2X88SGzUaMb0f5Zdh2yXQPekmmnWuGGh6T788lsU8HDXzva2lrc3JjZqFP16PkzDm52/efevplnLluwzjuW9O3fStMXVj2WAixcvAuBbpQqVXXwrw9WEhISww9gnd+zYQcuWlzNWrVqVEydOcOrUKfs+uW3bNlJSUhg0qGyfD9erPK3Dii6dm3CHD4B5wL2A4483FPdVs2BM1fFifz5X385lnc4pwc2a4e3tzbBBA7jdZKJe4E2sTFxOv0GDGfT8UKZGjQM0o8ZFAfDSpAlYMjJQShEZNRGAZ/sPYOTQMLy8vOjc5S6XFinBzZrh4+3D8wP7c7vJRGDgTaxIWE7/0MEMfn4Yk8ePRWuIjIqibt16JK1dB0DC0jhCWrehatWqRI4cYc88ZsJEl2Urzu3BTank7c2Y4WE0uj2IOvUCWb8qmV59B7Dxow/4+L13sFgysFosDB81hoXLkwFYk7ScFiGtCAio6pZc93TtwuczvqTf0HC6du5Endq1SDv8PYfMh3miZw8G93uOIRGj8fH2ZsakCQAM6d+X51+MJD8/n6gXRwIw65VXSf/hGDcH1mNi5CiXZuzW5U4+j/ma/uEjbRlr1SLt+3RSD3/P4z0eJrRPb56PHIu3tzcvR40FIGbhaxw59iM31avHhBdtV8D/9WhP+o+w5X3ykR5XXZ6zgps2w9vHm+GhA7k9yES9wEBWJSbQd1AoA8OeZ9qE8Wh9+VgZGPY8L02I4sKFCwwYPOSqbe7QtXUIM3atYOiMuXRu1ZLa1atx+MefMR/7iZ733MX8Va/TrHFDwmfNp3WwidAnerLyg0/YeTCNav5+jB3wrNuylaaClxdz+vSkSb1azOnzKAn/3Ubqrzf+hvLbg5vi7e1N5LAwGhvH8usrk3mmn+1Y/uhdh2N59Bhipk4i02olPz+f/mFDb0jGpk2b4uPjQ2hoKEFBQQQGBpKYmMigQYMICwtjwoQJaK0ZN24cAHPmzMHf35+wsDAaNGjAxInuPx+Wl3WorryOKcoHpZRVa+2vlLoFeFJrvVApdS8QqbV+RCm1CDiltX7ZaF+gtW6jlJoGWLXW84z5HAAeASzAHq11A6PdPi/jeSywS2u94mqZTlmzy/XOUsENly9c7ZzVdZci3eUWL8/fbFuSS7m5no5QKou/cz/KfaN5Hdxe+kQe9tRnrv2BAldLGu65grCs6lQu/2Mkp3JcN6LrLo3r1Cj2w6X8r93/57TWvwALi+maBiQrpVKALKBfKfM5o5T61iioPgU+dnVWIYQQ4v8LKaDKKa11kTvjtNabgc3G47PAP4uZZtoVz1s6PO59xeSbHfrCryOuEEII8f+K3EQuhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSUpr7ekM4k/CYrGU653FJ++CpyOUKjfjD09HKJWqUNHTEUp06WKOpyOU6mRAPU9HKNFNeVZPRyjV7xX9PR2hRAMXr/V0hFJ9OLqfpyOUKiPP0wlKV79GgCquXUaghBBCCCGcJAWUEEIIIYSTpIASQgghhHCSFFBCCCGEEE6SAkoIIYQQwklSQAkhhBBCOEkKKCGEEEIIJ0kBJYQQQgjhJCmghBBCCCGcJAWUEEIIIYSTpIASQgghhHCSFFBCCCGEEE6SAkoIIYQQwklSQAkhhBBCOEkKKCGEEEIIJ0kBJYQQQgjhJCmgnKSU2qyUeuiKtgilVJJS6u1SXrvF+L+hUqq3k8u9Qym1z/h3Vin1g/H4c+ffhfvNnz+f0NBQ5s2bV6g9PT2dQYMGMXDgQL7//nsAEhMT6d69O0uWLLFPt23bNvr3709YWBjHjh1zeb68vDyipk2n75ChJKxaXaR/+67dPBs6hIHDwjl+8qS9/eSpU7Trdh8//fwLAAOGhtN/6HAGDA1n+67dLs8JkJefz6TZ8xj44hiS179ZpH/WosX87V/P8N6nG+1tIye/xMBRYwkbO4ETp067JVeRjLPmMGDkKJJef6NI/8yFr3HfE//m3Y8/LbHNnfkmz13AwNHjSX6z6GE6K3Yp9z/9HO99tsneNmTsRAaPmcCQsRPZsS/F7RmXLXqVyGFhLH31lULtr69M5tl/PsLK+KX2tkVzYhg9dDCjhw7hh/Tv3ZorLy+fidEx9A8fSdLa14v079izl77DRhAaMZoTJ08BsPbtd+kzNJy+w0bw3cFDAAwaOYpBI0fxdGgYEROnuCWrM+tw6auvMCZ8KBGDB3Iw5Tu35HFGrQA/4sP+zaZJz1PBS3k0izPn73nz5jFkyBD69evHvn373Jpr8avzGRkWSuwrhXP9cCSdF4YMYsTggRwxcu3avo3hg/ozalgYPxmfId/t2c2wgf0YPqg/H7xb4sd1mUgB5bzXgV5XtPUCkrXWT5X0Qq11F+NhQ8CpAgpI1Vq31lq3Bj4AxhjPH3ByPm6XlpZGTk4OCQkJ5ObmcvDgQXvf0qVLiY6OJiYmhri4OAAee+wxZsyYUWgeCQkJxMXFER0dzbJly1yecfPX39C4YQNWxcex97sUTp85U6h/WVIy8QsXEDFsKAkrLxdYa9a/SUiLFoWzvraQ5LhYOrVv5/KcAF9u3Uaj224lacFc9h48xOmzZwv1hz77DBGDBxZqGzssjKRX5jDg6X+x9t0NbslVKOOWrTS67VaSF77Cvv0HimQc3Kc3Lw4JLbXNXb7atoNGt95C0vwY9h1M5fTZc4X6Q3v/m5Gh/Yu8Li7mZeLnRNOxdYhb86Wb08jJzmbekmXk5uViTj1k7+ve81HGTn2p0PT/7tOX+XHLGTVhEmuTE92a7cstW2jU4DZWxC5k7/4DnD5TeNvGr1pD3LzZjBwSSuI6W4H1wWcbWbV4EXNfmsoKo6BOXPgKiQtf4ZGH/k63Ozu7PKez6zA0/AXmxsYR9XI0b6xe6fI8zrJk5zBq5fsc+uWER3M4e/6OiIggPj6emJgYkpOT3ZbrcFoaOdk5LFxmy5V26HKupPilTHo5minRMSTH23KtTkpgXmwcE6dHs2K57TPkzXVrmDpzNq8tT+Kzjz687kxSQDnvbeARpZQP2EaTgJuBX5RSB4y2FkqpHcYIUYpSKshotxrziAHuNvpfVEpVVkolK6X2K6X2KqXuM6bvr5R6Syn1IbCJq1BKva6U6uHw/A2l1D+UUqFKqfeUUhuVUmal1CSHafo5ZFyilHLZvpCSkkLHjh0B6NixI/v377f3ZWRkEBgYSN26dbFabaujVq1axc7H19eX2rVr88svv7gqmt13Bw7QuUMHW8a2bTlwKNXel52Tg4+PD35+foS0bMHRH34A4Oy5c2RmZXHzTYH2ab28FINHRDBm0hTOn89weU6AlENpdGrTGoAOrUI4aC484lCnVs0ir6lvZKxQwQsvL/cf5imHUunUrg0A7Vu34mDa4SsyFt3GxbW5S0pqGh3btAKgfcgdHDp8xTqsWXQdKi/FsKgpRM2ay3mLxa35Ug8coE172/7Ypn0H0g4esPfVqFmLK8cjAm++GYCKFSvi5VXBrdm+O3iITu3aAtChTWsOms32PvuxUqUKdzRvxtFjPwJwa/36XMzNxWK1Ur1q1ULz+/LbrdzbtQuu5uw6rFixIgA5Wdk0vj3I5XmcdTEvH2vOBU/HcPr8XbAes7KyMJlMbst16EAK7TrYcrXr0JFDBy7nsmRkULdeIHXq1iXTarW3+/r6Uqt2bX771fYZ0rBxEzKtVnIvXqSyr+91Z5ICykla6zPADqC70dQLeAPQDpM9Dyw0RovaA1dWAOOBr40RpAXAcGPedwDPACuVUpWNae8E+mmt/1ZCrARgAIBSqgbQASi4ntPRyNgW6K2Uaq2Uagk8DnQxMlak6KgaxvyGKKV2KaV2lfXbhcViwc/PDwB/f38sDh8+ly5dsj/WWhd5raMzZ85w7NgxfjAKGFeyWKwOGf3IcMiYkWHB3+gDyDcyr1n/Js/868lC83llZjTJcbHce3dXliWvcHlOAGtmJn5Vqtiy+vlhcThBlCQ/P5/E19/gyR4PuyWXI4vVWihjhtW9BYezLIXWYRUyyrAO50wcR/ycaLp17kji60UvnbqS1WqhirHP+fn5Yy1jwZa8NI5Hn/qXO6NhsVrxd9y2jseK5XIfXD6+O7Vtw+N9BzB0zDieefJxe//Zc+dQSlGzenWX57yWdTg9ahwTR42ktVF4iWs7f0dGRhIeHm4vvNzBanHYvv6Ft692yHXJIdfZM2f46dgxfjpm+wzp2u1eJkRG0K/XUzzwUHeulxRQ18bxMl4v47mjrcAEpdQ4oIHWOruU+XUFVgNordOAH4GCUv4/WuuzV3uh4X9Ac6VULeBZ4E2tdb7Rt1FrfU5rnQlsMJb1ALYia5dSah9wD9CkuBlrreO11u211u0HDBhQSgybgIAAMjMzAcjMzCQgIMDe5zgaotTVr/O/8MILTJgwgRUrVtCqVasyLbcsktesZcDQcP731df2jNbMLAL8/e3TVK0agNXoA/BSXmRYLBw/eZLbGzcuNL9q1Wzfru+/5x7Sjx51WU6AlW++w+DI8XyxZSuZWVkAZGZmEeDvV8orbV6JT+SRB+7n1ptvcmmuQhnfeIvQUWP44luHjFmF16cnrXr7XYaMncjmLdsd8mWXaR1WM/bb+7p0Jv3YT27N6e8fQJaxz2VlZeJfhvX33pvrua1RI1q2au2WTCvWv8GgkaPY/M0WrFfZtlUD/O19YDumrZmZbPj0Mz5Ys5I1S2JZGJ9g7//i2y3ce5frR5/g2tbhlFmzWbAsgRXL4tyS6c/oWs7f8+bNIzk5mcWLF7stl3/A5e2bmZmJv//lXMohl5eRa0j4C8yYPIHXV62gRYjtM2Rp7EIWLUtg9VvvsenTT8jJybmuTFJAXZsNwP1KqbaAr9Z6j2On1nod8CiQDWxUSpU0egQUGV12lFlCX8HyNLAW231VAwDHoaIrh3m0sbykgnuqtNbBWuuXS1tOWYWEhLBjxw4AduzYQcuWLe19VatW5cSJE5w6darEE1xISAjLli1j4MCBNGrUyFXRGNDnWZLjYpkybgzbd+0CYOfuPbRs3sw+jW/lyly4cIGsrCz2HzxEk0YNOfbjT/z48888HzGKbTt3Mn32XAB7obU3JYVb69d3WU6Afv9+kuXzYpg4Mpwde203ue78LoXmZRgm3/DpRhTwyN/vd2mmIhmf/hcJr8xl0osvsH2P7QbSXfu+o0Ww+4byndH3qSeInxPNxBeG2W8E3/XdfpqbSr9kY820FQbfHUrlFofLtu7QrGVL9u227Y97d+6kaYuWJU6/e8d2Uvfv55l+ZftScy3693qaxIWvMGl0BDv22E5xO/fuo0XTYPs0l4+VbPanptG4YQO8lBeVfXyoVKkS/v5+hT6kvvjmW+7repdb8jq7Di9evGh7D1WquORyzl+Fs+fvgvXo5+eHrxvXY/OWIezZZcu1Z+cOmjnkCqhalVMnT3D61Cn8jFwt7gjhlSXLeLb/QBo0tH2GVPDywt8/gEqVKuGlFHl5edeVqeJ1vfr/Ka21VSm1GUii6OgTSqnGwFGt9SLjcQi2UaICFiDA4flX2EaO/qeUMgG3AWZsl93KKhnYBvystTY7tD+olKoOXAT+aSwnH3hbKbVQa33aGLny01q75Gt206ZN8fHxITQ0lKCgIAIDA0lMTGTQoEGEhYUxYcIEtNaMGzcOgA0bNvD222+TkZGBxWJh3LhxJCYmsmPHDqpVq8aECRNcEauQe+7uyn+mz6DvkKHc3aUzdWrXJu3wYQ6lmXni0Z4M7t+PwS9E4OPtTfSUSdwUGMjahHgAJk6fQdiA/gAMGj6Cyj4+eHt7M2PypBKWeO26de7EtK8XMPDFMdzVsT11atXEfOQIqYfTeezhh0hYt57PvvgSrTWnzpxhSJ/ezIpdQovgYAZHjqdtSEuG9u3jlmz2jHd25r9fz2PAyFF07diBOrVqYU4/wqHD3/P4P7qTsHYdn/73C7SGU2fOENa3T7Ft7nJ3pw7895stDBw9nq4d2lGnZk3MR46Smn6Exx76O4mvv8lnm78y1uFZhjzbi+ejJuHj7YOPdyWmjRrptmwAtwc3xdvbm8hhYTS+PYg69QJ5fWUyz/QbwMaPPuCjd9/BYsnAarEwfPQY4hbMp4qfH+NGDOOW2xrwwtjxbsvWrcudfB7zNf3DR9K1cyfq1KpF2vfppB7+nsd7PExon948HzkWb29vXo4aS5UqvtzZoT19h40g/9Ilwvo+B9i+bFismdwcWM8tOZ1dhzFTJ5FptZKfn0//sKFuyeSMCl5ezOnTkyb1ajGnz6Mk/Hcbqb/e+BvKnT1/R0VFYTXWY3h4uNtymZo2xdvbh5FhoTQOCqJeYCBrkhPpM2AQ/UPDeHmSLdfIMbZca5IT2bNzB1WrVePF8bbPkF7P9SNyxDC8vLzoeGeXMo1SlkSVdh+KKJ5S6nHgXaCZ1jrNuJn8I611S6VUFNAHyAWOA7211meVUlattb9SqhLwGVAbWAHEAUuBdkAeMEpr/YVSqj/QXmsdfsWyVxjLevuK9s+B9VrrBON5KHA/UA3bJbrVWusZRl9vYCy2Uchc4Hmt9c6S3rPFYinXO4tPnudvwCxNbsYfno5QKlWhfH+vunTx+obdb4STAe4pElzlpryy3UfnSb9XLB+Xga9m4OK1no5Qqg9H9/N0hFJlXN8g0A1Rv0ZAsVeJpID6i1BK+QH7gVZaa4vRFgq01FpHuGIZUkBdPymgrp8UUNdPCqjrJwWUa/yZCyi5B+ovwPjFnqnAgoLiSQghhBDuU76/aooy0VpvxHbf1JXtCcVMLoQQQojrJCNQQgghhBBOkgJKCCGEEMJJUkAJIYQQQjhJCighhBBCCCdJASWEEEII4SQpoIQQQgghnCQFlBBCCCGEk6SAEkIIIYRwkhRQQgghhBBOkgJKCCGEEMJJUkAJIYQQQjhJCighiMjBYwAAIABJREFUhBBCCCcprbWnM4g/CYvFIjvL/wM+eRc8HaFEOj/P0xFKddHHz9MRhKDn/JWejlCqD0f383SEUgUEBKji2mUESgghhBDCSVJACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTqro6QDir2n+/PmkpqbStGlTIiMj7e3p6enMmjULrTVRUVEEBQWRmJjIW2+9xaOPPsqwYcMAmDZtGj/88AM+Pj488cQTdO/e3WP5oqOjOXLkCEopxo8fT1BQEABaa3r37s3TTz/NY489dsNzFdcWFRXFmTNnyM3N5cKFC6xbt47k5GS2bNnChQsXGDBgAPfdd59Lsubl5TF5xkx+/e13unXtQmjf5wr1b9+1m0VLl+Hj7c3MaVMIrFsXgJOnTvHwk//mvbWrue3WWxgwNBwAa6aVmwIDWTQnxiX5CjJOmRnDr78fp1uXOxn03LOFM+7ew2vLluPj7U30lIkE1q3Lb8ePM3P+q2RnZ9PjoQd5omcPIidP5czZc1y6dIlp48fSqMFtLst4vcfK1fZPV8nLy+Oll17it99+4+6776Z///6F+nfu3MmSJUvw9vZm+vTp1KtXj5iYGD7//HPCw8Ptx8aQIUMAsFqt3HTTTcyfP9+lOcE1x7W7OZNx3rx5HD58mAsXLvDiiy/SunXrG5KxOLUC/JjVuwcN69Tk4ZnLyL+kb3iG692+ZrOZ2bNn4+XlxfDhw2nTps115ZERKDdRSi1QSkU4PN+olEpweD5fKTXqGuZrdVVGh3k2VEodcNX80tLSyMnJISEhgdzcXA4ePGjvW7p0KdHR0cTExBAXFwfAY489xowZM4rMZ8aMGcTHx7u8eHI2X//+/UlKSmLq1KnEx8fbp/3yyy+pUaOGx3IV1zZr1izi4+Pp27cvXbt2BeC5555j+fLlLF26lJUrV7os7+avv6Fxwwasio9j73cpnD5zplD/sqRk4hcuIGLYUBJWrra3r1n/JiEtWtifJ8fFkhwXS8+Hu3PPXV1clg9g8zdbaNSgASvjYtmbsr9IxvjklSxbMJ+RQ8NIXL0WgNeWJTB9wngSYxfyRM8eAMyaOpnkxYsYMSSUdW+/47J8rjhWrrZ/uspXX31Fo0aNSExMZN++fZw+fbpQf0JCArGxsYwYMYIVK1YAEBoaysiRIwtNFx8fT3x8PD169ODuu+92eU5XHdfu5GzGiIgI4uPjiYmJITk5+YZkvBpLdg6jVr7PoV9OeGT5rti+S5cuZdasWcTGxpKUlHTdmaSAcp8tQBcApZQXUBto4dDfBfjWA7ncLiUlhY4dOwLQsWNH9u/fb+/LyMggMDCQunXrYrXaasFatWoVmYdSiqlTp/Liiy/y+++/ezRf/fr1AahYsSIVKlSwT7tx40YefPBBj+Uqrq3AF198wd/+9jd7boALFy7QpEkTl+X97sABOnfoYMvbti0HDqXa+7JzcvDx8cHPz4+Qli04+sMPAJw9d47MrCxuvimwyPw2f/0N93Vz7QfrdwcO0rlDewA6tG3DgdS0YjJWIaRFc47+cIzcvDx+O36cl+fM4/kXIzn2088AVDLWYVZ2NiYXrkNXHCtX2z9dZf/+/faM7du359ChQ/a+HIft3LJlS44ePQpA7dq1rzq/r776invuucflOV11XLuTsxkLjt2srCxMJtMNyXg1F/PyseZc8NjyXbF9LRYL9erVo3LlymRnZ5OTk3NdmaSAcp9vMQoobIXTAcCilKqhlPIBmgF7lVJjlFI7lVIpSqmXCl6slOqjlNqhlNqnlFqmlCp0hCulaiultiqlehjPi8zHGFlKVUotV0odVEptUkr5Gn3tlFLfKaW2AsNd+cYtFgt+fn4A+Pv7Y7FY7H2XLl2yP9b66kPAERERJCUl0a9fPxYsWODKeNecLzY2ll69egGwdetW2rVr59ITr7O5rpY1Ly+P9PR0mjZtam+LiYmhV69edDAKHtfktTrk9SPDIW9GhgV/ow8g38i6Zv2bPPOvJ4vM68zZcyilqOnCET0Ai9WCv18VW0a/KzJaimb844/zfH/kKJPHRhI5YjgLliwFIDc3l35Dw5m1YCEhLZq7Lp8LjpUCjvunK1ksFvz9/e0ZMzIy7H0ZGRn2/FA4c3HOnj2LUsqlI7eOOa/3uHa3a8kYGRlJeHi4vXj4/8oV27dGjRqkp6dz7tw5jhw5UuSLp7OkgHITrfVvQJ5S6jZshdRWYDtwJ9AeSAHuBYKAjkBroJ1SqptSqhnwNHCX1ro1kA/Yb95QStUDPgamaK0/Vko9WNx8jMmDgMVa6xbAH0DBp1cy8ILW+s6S3odSaohSapdSaldZh5ADAgLIzMwEIDMzk4CAAHufl9flXU4pddV5VKtWDYDWrVtz5orLLtfrWvKtW7eOxo0b2+9B2LBhAz179vRorqtl3bVrF+3atSs07/Hjx/POO++4ZNg6ec1aBgwN539ffW3Pa83MIsD4kAWoWjUAq9EH4KW8yLBYOH7yJLc3blxknl989bVLR5+S177OwPCRfPH1N1gzswDIzMokwP/yOq0acEVGLy8CAvxp3LABNWtU5/bGjfjj/HkAKlWqxMq4WOa//BKLE65/HRZwxbECRfdPV1i1ahVDhgxh8+bN9g8aq9VaKGPVqlXt+cuS88svv3TL6BO45rh2t2vJOG/ePJKTk1m8ePENyVheuWL7hoeHs2DBAmbOnElQUBDVq1e/rkxSQLlXwShUQQG11eH5FuBB499eYA/QFFvBcz/QDtiplNpnPC/41KkE/BcYq7X+j9F2tfkA/KC13mc83g00VEpVA6prrb802i/foHIFrXW81rq91rr9gAEDyvSmQ0JC2LFjBwA7duygZcuW9r6qVaty4sQJTp06Zf9WW5yCE/axY8cKHSiu4Gy+bdu2kZKSwqBBg+zT/fzzz0RGRrJmzRrWrVvHsWPHbniuq63LzZs3F7pR/OLFiwD2Sy3Xa0CfZ0mOi2XKuDFs37ULgJ2799CyeTP7NL6VK3PhwgWysrLYf/AQTRo15NiPP/Hjzz/zfMQotu3cyfTZc+3T/++rr/hbt25XLuraMz77DEmxC5k8JpLtu3YDsGPPXlo2uzwqVyjjoVQaN2xAZR8fqvj6kp2Tw4lTp/D380NrTW5eHgB+fn5U9vFxWU5XHCvF7Z+u0LdvX+Lj45kwYQI7d+4EYPfu3bRwuH+tssM6PHDgAI2LKY4dbd68mXvvvdelOQu44rh2N2czFhy7fn5++Pr63rCc5ZErtm+DBg1YvHgxEydOpF69evZLpNdKfgrPvQrug7oD2yW8n4HRQAaQhG0EapbWepnji5RSI4CVWuuoYuaZh60QeggoKIDUVebTEHC8aJ0P+BrTu+1HKJo2bYqPjw+hoaEEBQURGBhIYmIigwYNIiwsjAkTJqC1Zty4cYBtNOftt98mIyMDi8XCuHHjmDx5MhkZGfafoPBkvjlz5uDv709YWBgNGjRg4sSJrFu3DoAPP/yQ/Px8GjZseMNzFdemtSYlJYWxY8fa5ztv3jyOHTtGXl4ezz33XLHLvhb33N2V/0yfQd8hQ7m7S2fq1K5N2uHDHEoz88SjPRncvx+DX4gwfsJtEjcFBrI2wXYz58TpMwgb0B8Aa2YmFou12Puirjtj1y58PuNL+g0Np2vnTtSpXYu0w99zyHyYJ3r2YHC/5xgSMRofb29mTJoAwJD+fXn+xUjy8/OJenEkFy9eZOjosSgApZg4OqLEZTrDFcdKcfunK3Xr1o1p06YxaNAg7rrrLmrXro3ZbCY1NZXHHnuMgQMHMnz4cLy9vXnpJdtdCImJiWzcuBGtNadOnWLw4MFYrVYsFgs33XSTS/MVcMVx7W7OZoyKisJqtZKfn094eLjb85WkgpcXc/r0pEm9Wszp8ygJ/91G6q837oZyV2zfDRs28Omnn1K5cuVC58hrpcpybV1cG6VUa+Bd4KjW+gGjbTdQH2gJtAVeBu7XWluVUvWBXGw3nL+P7RLeSaVUTSBAa/2j8VN41YC3gB1a6xjjEl5x86kCfKS1bmksOxLw11pPU0qlAMO01t8opWYDPQqmuxqLxSI7y/8DPnmeu1G0LHR+nqcjlOqiz/WP9AlxvXrOd91P3brLh6P7eTpCqQICAoq9Ni0jUO61H1sxtO6KNn+t9Wlgk3G/01bjuq0V6KO1PqSUmmT0e2ErhoYDPwJorfOVUr2AD5VSGVrrJcXNB9uI09UMAJKUUlnARte9ZSGEEOKvT0agRJnJCNT/DzICdf1kBEqUBzIC5RpXG4GSm8iFEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SWmtPZ1B/En8es5SrneWihXK//eB89k5no5QqpszT3k6Qonys7M8HaFUb53I9XSEEvVpcYunI5TqN68qno5QIp+KFT0doVRVy39Ees5f6ekIpdo8LVwV117+P3GEEEIIIcoZKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SQooIYQQQggnSQElhBBCCOEkKaCEEEIIIZwkBZQQQgghhJOkgBJCCCGEcJIUUEIIIYQQTpICSgghhBDCSVJACSGEEEI4SQooIYQQQggnVfR0APHXtPjV+RxOTSUouCnhoyLt7T8cSWfB7FlorYkYG0WToCB2bd9GcvxSfHx8iBgbxW0NGwKgtWbwc715/F9P0+Ofj7kt62uvzMOceoig4GaMjBxjbz+ans78mJlorRk9PoomQSamTRjP2TNnyM29yIWcCyStW++2XAWWv7aQdHMaTYJMDBn5or39jVUr+OS9d3ngHz14bnAYAAtmzuCXH4/h7ePDQz3/yb1/f9Dt+QDy8vN56ZVF/Hr8BHd3as+Afz9VqH9W7FI+//pbwgc8x+PdbZmGjJ2I1hqlFKG9n6Zj6xC35ns5dim/nThF1/Zt6PfEPwv1j5o5F0tmJpUqVmTqiKHUq12LpLff4+1PN9Hz/nsZ2vtpt2Ur8L+31nL8xx+od1tD7v93n0J9uRcvEj95ND36h9GwWUuOpR7gmw/eoaK3N39/ph+1Am92W668vHymxMzm19+P0+3Ozgzq07tQ//bde4hNSMLb25uZE6OoV7cOa996h08+/y9eXl5EDh9Kq5YtGDF+AharbR3PmDCeenXruDzrskWv8n1aKrebgnk+YpS9/fWVyXz07js82OMR+g15HoClr77CkfTvyb1wgcEjRtIipJXL8xS43vPhd3t2syx2EUopHurxCI8+8VQJS7s28+fPJzU1laZNmxIZeTljeno6s2bZMkZFRREUFER0dDRHjhxBKcX48eMJCgrCbDYze/ZsvLy8GD58OG3atHF5xtLUCvBjVu8eNKxTk4dnLiP/knbr8mQE6gZQSuUrpfYppQ4opT5USlX3dCZ3OpyWRk52DguXJZCbm0vaoYP2vqT4pUx6OZop0TEkx8cBsDopgXmxcUycHs2K5cvs0275+ktq1Kjh1qzmtFRysrOJXZ5EXl4uqQcvZ01cFseUGTN5adZsEpbask6bGcOiZct55rl+3Hn33W7NBpBuNnMhJ5vZsXHk5eVxOPWQve/BRx5l9OSpRV4zevI0Zi1afMOKJ4Cvtu2g0a23kDQ/hn0HUzl99lyh/tDe/2ZkaP8ir4uLeZn4OdFuLZ4Avt65m4b167N85jT2pZo5fe6PQv2jB/VjefQ0+j3+KK9/+AkA/3zgPqZHhLs1V4ETPx0j9+IFekdOIj8vj9+PHS3Un/LNF9S+qb79+ZaPN/DviPE8MnAo3370rluzbf72Wxo3aMDKxYvYu/8Ap8+cLdQfv2oNS+fPYWTYYBLXrAPg/c82sjoulnnTp5L8uu1LxrgXRrAidiEDn32G1W++7fKc6eY0crKzmbdkGbl5uZgdjpXuPR9l7NSXCk0fGv4Cc2PjiHo5mjdWr3R5ngKuOB++uW4NU2fO5rXlSXz20Ycuz5iWlkZOTg4JCbaMBx3Og0uXLiU6OpqYmBji4mwZ+/fvT1JSElOnTiU+Pt4+3axZs4iNjSUpKcnlGcvCkp3DqJXvc+iXEzdkeVJA3RjZWuvWWuuWwFlguKcDudOhAym069ARgHYdOnLowH57nyUjg7r1AqlTty6ZVqu93dfXl1q1a/Pbr7/Y2/63aSP3PuDeIuBgSgrtOnayZe3YqVDWjPPnqRdYNCvA15u/oNt9f3NrNgDzwQO0atcBgFbt2mN2OPnWqFkTpVSh6ZWCBdHTmT5+DCeP/+72fAVSUtPo2Mb2Db59yB0cOvx9of46NWsWeY3yUgyLmkLUrLmct1jcmm+/+Xs6tmppy9eyOanpRwr1169XF4AKFSrgVcF2WqxVvXqR9esuvx5Np0HTFgA0aNqC339It/fl5+Xx+w9HuOV2U6HXePv44F+tOn+cOunWbCkHD9GpXVsAOrRpzYG0NHtfdk4Olb298atShZDmzTjy448A3Fb/Zi7m5mKxWqletRoAt9x8E2BbxxUquP6jJ/XAAdq0tx0rbdp3IO3gAXtfjZq1uHJLVqxouwCTk5VN49uDXJ6ngCvOhw0bNyHTaiX34kUq+/q6PGNKSgodO9oyduzYkf37Hc6DGRkEBgZSt25drEbG+vVtxXzFihWpUKGC7b1YLNSrV4/KlSuTnZ1NTk6Oy3OW5mJePtacCzdseVJA3XhbAftXSaXUGKXUTqVUilLqJaNttlJqmMM005RSo0uYvqFSKlUptVwpdVAptUkp5Wv0bVZKtTce11ZKHTMeV1BKzXWYV5ir3qDVYqGKnx8Afv7+WB0+HPWlS/bHl/Tl4dWzZ87w07Fj/HTsBwB2bttKSJt29oPTXaxWC35GVn8/fywZGZezaseslx/n5eVxND2d4KbN3JqtIN/V1mVxBg1/gblx8TzZuw+Ji19ze74ClsxM/KpUAcDfrwoZVxScxZkzcRzxc6Lp1rkjia+/6f58xgePX5UqZFgzi0yTn3+J5Hc28MSDD7g1S3EuZGfhU9mWz8fXl5ysLHvf/q1f0bzTXUVek5lxnjPHf+Ps8d/cms1iteJv7IMB/n5YLJe3bYbl8vEDcCk/H4CO7dryWJ9+PD96LL2fetzen5+fT8LqtTz16CMuz1noWPEr/VgBmB41jomjRtLaKLzcwRXnw67d7mVCZAT9ej3FAw91d3lGi8N29Pf3x+KQ8ZJDRq0LXxKLjY2lV69eANSoUYP09HTOnTvHkSNH7MXWX5rWWv65+R9gNf6vALwFdDeePwjEAwpbMfsR0A1oA3zp8PpDwG0lTN8QyANaG9O/CfQxHm8G2huPawPHjMdDgEnGYx9gF9ComOxDjL5dwJCyvF+TyTTcZDL923j8hMlkesGh70uHx5uBISaT6U6TyfSFyWRaYTKZNhl9b5lMpsomk6m/yWQKdde2KS1rwXs2mUybHdr/bjKZFtyIfaekfEbbvfXr1//4Kq/95gbkG2MymTabTKZTJpOpo9E2ymQyPeq4DxntxW5Lk8nkW7DdPZGvIKPJZHrVZDL1vXL9mkymGZ7aziaTqaLJZHpXa039+vU/NJlMDxjtRY4ZT6w7k8lUxWQyfeKwDjebTKaqJpNph8lk8jaZTLcU9BvTF1nH7l6HjtuyuGPFZDLdajKZtt3obWs8L+v58Csjp7fJZPrSZDJV8WRG43GEyWSa7NBnMplMm0wm0ztGxkruWqcl/TPybdZaV3T3smQE6sbwVUrtA84ANYH/GO0PGv/2AnuApkCQ1novUFcpdbNSqhVwTmv909WmN+b1g9Z6n/F4N7aiqiQPAn2NXNuBWg7zstNax2ut2xv/4sv4frcC9xuPHwC2OfSdDQ4OviU4OPhm4DwwxGw2bzWbzfcB0UCqMV0QsAEYDUQEBwc3LeOynVVi1kqVKg13yFrgceA9N+VxJh8AFy9etN+tGRwcXNX4Pxj448ppXc1sNs81m833AmEOOe8DdjpMNqS41xZkBe4CjhQ3zQ3KR/Xq1ScA2mw2r3JHjjK42nauB9waHBz8mbe399+BWcHBwTWucsy4VFnWndlszgJ8g4OD/X19fSOwfdm7BGSZzeaL2I4bP4Dg4OBBuHcdO3us+BgPLUDRIckbk6us58N84A9jnV4CKnkyY3Bw8INAF2BGwURms/mw2Wx+ENv+8pPZbM51ccayKvZ84w5SQN0Y2Vrr1kADwJvL90ApYJa23R/VWmt9u9Y60eh7G3gKeBpYX4bpHS/85nP5JyzzuLydKztMo4ARDvNqpLXe5Io3azab9wA5wcHBX2M72H8KDg6eaHRPNd7PW8ZjgoODJwYHB38BzAKmG/NobTabuwPzgVfNZnMablBa1ptuuqnxFVkVcCfwjTvyOJPP+ECa7+/vXys4OHix8ZK1wcHB3wAJwPgbkdHwIdDSWPZWs9n8e3BwcGsjI0bmMcCo4ODgKcZr/me8r3E4nIg9ka9OnTq3Ae2Dg4M3BwcHv2RkHoRt/3vWYf26xdW2s9ls/tVsNncwm83drVbrWSDKbDafK+6YcaMS1x22D/r/1K5d+xYgxmw2W4FNwcHBW4H/AnON6ZZwxTp2pWs4Vt4w1uGHGMe3O7jifAjMBj431ukXZrPZ8QvdDc8IvAY0Ar4IDg5eZuQeZOReCUzBMyp9++23JqAVsBHo5M6FKWPIS7iRUsqqtfY3HrcB3geaYPs29zJwv9baqpSqD+RqrU8qpVoAy7FddrtHa/27UurB4qYHqgAfadtN6iilIgF/rfU0pVQCsFtrHaeUigAitNYNlVJDgH8A/9Ja5yqlTMCvWmt3fhMrQim1S2vd/kYu0xnlPR+U/4zlPR9IRlco7/mg/Gcs7/mg/Ge8kfnk90DdYFrrvUqp74BeWuvVSqlmwFbjp32sQB/gpNb6oFIqAFtR87vx2k1XmT6/hEXOA95USj0H/M+hPQHbZb49yjazU4D7ftnS1ZX1sqCnlPd8UP4zlvd8IBldobzng/Kfsbzng/Kf8YblkxEoIYQQQggnyT1QQgghhBBOkgJKCCGEEKgb9Ztj/yKkgBJCCCEEQMEPO0ltUAaykoQQ18zxG6tSytW/m+a6XZHPp6RpRenK4zaGy9v5zzKCUh5zKqVuA7YopTpprS+VxyLqyvXm6fVY7laQEDeapw/CPyullNLGT6EopfoBI40/EVQu1ucV+YYCvZRSVTwcqwiHD//Agj/BVB4ppZoDjxqP3fs3lpykL/80VD2PBrkKh218i1LKGyhX29k4Vn4ClgKxSqmQ8lZEXXE811JKeWsP/xRcuVk54q/D4WTRSinVVCl1q2N7eeGQx6/ECT3EYT1WLm1aT3A4mY0EwoEPtNb52H5Jq8cvAzjkGwYMBL7SWmeV/Kobq+BDQSn1KLZfNHmLpzOV4E4gCsDYzh6nlGqrlLrbeDwCeF8pFauUurM8FXnGNu6J7ZdMzgWilFLlYls7FibAAeA4kKSUal2eiiiH43k0tl/kuV4p9Ygnv3SUixUj/jocPhD+BmzC9tuwlxjDwro8FVFGnn8Anyqlpiil/u7pTI6MfN2BeKXUBOP3gpUrSqlA4O/Gv1NKqV7ASqVUI+3415g9k00ppWpg+9M74Ua+fkqpqUop1/8122tgbONuwEvANK3190opn/K0rZVSFQGMv3qwTynV32j36LFsXE7sBExXtl8efB8wGNtfX/gX8EB5KaKU7RcjvwT0wvb7FzsA5z29DqFQYfICtl/U/Ca2v326QinVrjwVUUqpJ4F/aK17Y/uy8Q+tdban8pSLlSL+OowPhA7A3dh+MedQ4BNgVnkropRSbYFBwDJsf+bmYaXUU55NdZlSqjMwDdufJOgOTFRKNfFwpkLbTmt9HPgR2wn3VWwfDBeARQUfvJ7Kp23OAV9j+1Mxq4HOQHUjp0c5ZG2O7Y9+5xqXGt8EEgpGbj3JOEYiC4ombH/CqAkUumzmEVrrXGx/YiQB+CewV2udAkwGzgEPAf/wZBHlsI1rAu9g+0PxbeH/2jvvcD2qqov/dgoJndBLQui9E0IvoYaEUEQQQofQQu9dEEVAauhNOgpIL8HQexVQQKmCfArSpXeyvj/WfrmTa6JEycxNPOt55sk7887NXXfKOfvssjZDJX0ELBQdJ69sXuAQSZcAQ4FL8eJ38aYWQxUvfMtWmS457Qq8B+yZ30/XBL9iQBV8b8j8l064b9PmwDu5OrgUTwqnRMSyTQ+8ADk5XQn8XtKlwGm4oe1yEbFpo+SAiJgNe++uknQZ7os4M7B9RMzVEKdqDsLgiNghItYH9sW9rw6UtA++ru+SobyG+G0YETunAXAT8AtgV0k7A08DS4ZzUWpHZVJteZkew9VPV+FrdgE2SnvUz+6f8Dm+lxtHxGnYezIkItZuilBETJueWYC+eCK9DhgcESunYXI8NuSXY9QeoHVxbN3jlvH2EvbSngFsKOnl9H4fSdtz0AS/1n4nvLDYCCANplbT+5PSK1r7wrcyVyyS/76JG8wPBAZK+jIi9gd+0oShXAyogv8alRdronzx1sZdxFv5Eh8Bv8IegEbDOi1I+iuesHaJiHklvY6bZf4NWD5DU01iCjwxrBsRC0l6C9gbmAvYuYm4f8U42Rt77r7Bq/31Jf0q+zXuBRwDnJgegib4DcWD7Nd4EphZ0m3A6xGxDV61HiDpyzr5VXlGxEDg9Ig4AhvGB+Cel2dg42kQNRugMMpFgyPiAAAgAElEQVSKf6VwXtb0ks6VNAB4Hic/d8KGSVN5bl8BW0TEg/i63Y+9yCcD+6UR9Rk2Tk6ou78nfHuP1wB+ke/LTNiQvx5YLyJWw82CL5D0Xp3c2i001gv3WJ0S2AsvIFuNnufD0YNNJH3R1MI3vUu35Ht9Ezai7gVWDrcoGwyc3khenqSyle0/3mhrB7Q6NpKOwaG7zjgH6pzKuZ07AM+FgXVwH8BuwO7A48B8+f0MQO8G+c0LLIgHtJ7YszMMWCC/nxZYtCFugQ27C3P/AODmvNeT4RXs4cBCTd1jYPp8DicHtgLuaD13wCzAwa1r2eCzuCxO1p0HT/4X5bPYHYe+XwTWaZDfAOBPeGJ6F4ebqt8PSv6zNshxx+R2XuXYDHn8XmDFDnCPHwOGANfiUPzQvHZXYo/3OnluNMRxc+AF4MbksxLQC4fjL8cGc6PvSoVr33wmt8gx8Agc2bisqfFGUumFV/Cfod0qZjn8MB+EB7G+ePA4HXgIeErSdk1xbSETh08AnsQ5CSPwYPYDnKu1vqTnGuDVSU7UXBs4BV+7xZLTp3hC6wmcLOmPNXObWrlCjogF5SbX1wDC4ZyNZDf65jg09pRqHFQiYhpsIL0VEQvjMOyeQB9cXbl2XtvdsLH3qhqqIIuIzpK+iYgdgA+wp+kUfA1fjYiZ8fvTRdJjDXGcDU9K22Bj/iic83SEpOMq5/0KuELS9TXxqlaKEREzYYP4Z8ALknbP44tiw/Rh2ctcO/IangLcLumU9KBsDswiad88p6ukr9r/XTVy/BE25oZg431r7N2+FHgUL0C6SXq7bm4Vjv2xt+mpfG+WAK4ADpN0eZ4ziZqsrG3asizb+LfhkMNqtK3sfwgclJ8nxpP/5TjhbxJguYZ49qh8njw59c39tXCu1vq5fxiwfM38pq58ngG4E1g294fgCX8BYHZcHVP7ahCv+E4EdgEewF6eHYA/kqt8PPj+iZo9ErSFks7Oe3lZXsfDseHe8tptjD0mczTxHLZ/HnEuzIPA73F4sfUO/RSHwWt9BoE5gIVavzuft6VwfiDYgB+Jc8jAXop7gblr4hiVz9tjb8n2uT8ncHs+o+vlszB5g/d4ZlzwcSWZcJ/HJ8ee7sa8s+32f45Doa13pBewB86/W6tJjkCn/HcYDtktghcV4DSGz4HtmrrHo3BumkDZxr8tB6oFcDhnUpzQ9zIwe+Wcq2nQjQ5MhF3nPXO/E3BDaxLIY3sB1zbErxtwMdCrcuzXQL/K/pHAb/LzFA1ey2eA91v3NyfcPbD7/1zseVqwIW4TY0PzbextAuebXIRX09c3OXFVnr2ZgY+BlfPeX47DifNgb9kfqDlsl+/wfXn97gWeIo3gfKdPzc8r5X1eK/e7NmGkYEP4MWBT7L07sHK/r8+/pZHnMHnMkO/wHNg79gvgJJxLNFte3zkb4FU1QGfGi6BO2Ih6uDJG9sZe7xka5jhH5fMhwDXAYrm/UfKeq6n7PArvpgmUbfzZqOQw4ZXrpcBmaazsg2PpC+f2ODXn6oyG7xQ5QR2Q+xthT8Vqud8H58tM1hC/yXOwPST3j8N5RTPl/io4ObJuXu1XgjvhXKILqXhI8tou1BqA6+ZX2d8z7+uVtHkYu+KV65I4dNLE/e3cbn8HbOgtDsyNPU73YANmvdH9beOQ27zYS7ctzmHrgsUJX8hncmFcZHEizoXpU30mGriWy+VEulaF/2Ok5zuPTdkEt3Y8T8Ke0MAaVefnNb2WyuKoIW574oXtuTikOEU+g/eSeZ+kp6dmXlXjaTfgOeCXwA/y2EF57y/Bi7ledXMcI/emCZRt/NjwSn9JbCwtj1evQ3OA+GEOuvvkoHx76+FviGfL3dsTmBUr6+4ITJMGygjs/XmhNXHVzK9Thd/CuGJxW5w4/husa3My9uzUza86mC0HLFzZvwm4Mj9vAQxq4P5W+a2IDbhJcv8Q7Inolc/oRg09g5NXPi8ArEBbuHtr4BNgydyfigzlUp/xND0u8d8m97tWvjsFeCY/r45zodZs8j7n/ob53p5OW9hzbiwPsF8D/OYHjs3PvUmPSL7DpwLT5v6S2Eg5jjTw6rrP7fiuj7XGOmOv02l5vAvOC701PzeS0J5cBuXYNz9OGRgGbJXfLYU9kPM2xW90W0kiL/hOiIjpsXt3TmBVHC55OiKG4MnqJklXhxu2dpX0cRMJkpkovgpe2R8laZGIWAwbJsdgg28u7J14WdKTdfLMkuG1sJG5FR4oZsO5G6dhj1g/7N15UtL9DV3HfXCo9j1c7XSspBci4iacDzMvNpJrTWqv8NsVG3H34Hy8fpI+jIiD8UA7ETY+X6yZ1+RYcfoFSWdFxM9x1dBFwCOSvo6In+Hw3YqSHqiTX4XnNTjktJKcoNtd0uf53T3A/pIeqZxf5ztSLVBZBPgI+D/s0dkEVyleKenNsLDsSEmv1MEtOc2LF2BnSrowIi7DeTkf4aT2y4Dhkobl+cvisOM/gJ+pBnmP0STdb4ON5knxgnddSV9ExKyS/i8iplOzCeOz4bzAayTtGhGT4uKePvh+n6OGZEf+JZq24Mo2/my4keg/gDOp5ORg78lVeOKq3QU8Gp4P4VX+apVji+JE5wM6AL+78WC7cuXYknhlWPtqejT81gN+m5+PxZ6602hLiO1LegFq5tVa8K2DE9onwe79t4G/AlNV+PVu6NpNiQsAzsSr/i7YoBoGrJDnLIv1ddaomVsvPOm39n+Nc69aHtHu+e+NpIes4edw1+R3FfAINvhWx16y/bFGVd2c5sWVnsdXjs2IC2YuBw7FhsATwDyVc5YCpquRZ8vjuRr2OPbDaRV3Vc7ZJ69lo2G7yrHNgL+Tnm28CBqCF75TNf08jvbvaJpA2Tr2xqghk+44CfZMrJI9f+W77Wg+56k1we6ABRR/TYZ38vhCeDUzx+he4Jo4dsYu89/mgFsNn/TJiWIuasw1wSGImWirwloEV2INSZ69aMvVWaqBazZnTp6T5/7MOWltC4zIY7dgQ6pH3fxGw3dqnGNyPm2aaEfi3JPTc3JdtPrM1sSrC5bwuLByrGVEdc395XFuUe1JuoxaNdsnebTCdcfihdEkeCF3TN33GodjH8WhxGOwJ757u3N6A1viRdLaDVzDfqThhpXsH8x3Z0acV3QorqrcIp/D2osr2s0pm2Jv7ABshK6HUxeqRlRjBTT/9m9pmkDZOu5WMUhWz0l/MA5HzI1DEnvhPKjraDCBs8Jzvtxa+1cB1+fnFckKqAb59Ww3SVwMXJef58Tl7ZPWzK0/zsG6AieKT5bHu+DVaUvs79C859PWzG9ATvr35qS1YOW7Y2jL49kFuIuGpAoq97hliEyEjagLsBHVKd+jA2mgTJw2j0SXnNwvqXz3a+zRWxEb8LXm3SWHNbGA4xq53zPfjykr51wG7JWfa60CxJWTp9OWk3MkXkj2o7IIqpz/A+xl7F4Xx/y9BwOf0SYMfD9t+Vh9cZL2lXltG6tMTT5Dk9/mwF+ALSvX7m80YICO9d/QNIGydewtJ7Cn8iG/JQex+XPC/2kOEj/sADzXxSuq3+RE39I3uQrLF7zUxMRQ4Tco+V2Pq9k643yEy/FK+2lglZo5rYG9D/2wm/9s4PDK93tg4+pIvJKdvWZ+rWs2J/aMnEomhuf1OwD3FjsBexxrL79OLi3jqX/ezwNxbhH53pwHbD66n6mZX8vD2BUbm5dWzrkK57YNrJtf/r7dcB7Rrdjg7I7zBAdXzjkI2LPhezxx5Roens/ft0YUbeHQgVhWYeI6+eXnA7EAZU+s2D1L5bspqs9CUxuuADw3x8Ct8l53ps3QX5eGddu+09/RNIGyddwNh0p+kRNY/5xsjySNqDyn5bFosnqjTw5W0+JQ4uvAWaS3AldBLdwUz/z9T+f13Bb3aLuoMlhsQQpo1shpUizceUblWL+cFKYhPXU4r+0cahbxzMH0KpxI3zq2ITbyemKPzqQ4zHgKlWrBmq9hq+KzPxbGXAOXWz9Cm0jrNtgTVavcQ/7u1sS/Gs552Sz3u2CP40WVcxes/kzNPKfFEgB7Y4/2yjhk9iSuYDs+P8/fALfWNeyHPSatsGInbKCcir2LXSvH+9XFdXT3Czf4/gobpafgUPxV2OCrxaj7F3wXwh7aI7A3dETlu6HA0k3yG6u/pWkCZeuYWw5gg3E+zhyk4Br2Pj2NV9odId9kdhy2WzIHsd/hhPFr0kBYqWF+s+L8oiVwk+UHk/MfcE5R7SHFCrc+eMW/S+6fB7yDV4O/x30CGxOswyX+5+HKHHAi9ktYJ+YGbOzVnsyeXKbEuTAto6Mlirk2zpPZARv1LY9Ok0n3/XEBxSBcUXk0KfeR7/Xl7c6vS05hEWCR/NwJ5zmdh6tUb8LJ9jPj5OKDqCRlN3At58WGXUsPq2osHY0XbFM3wKvqedoEe2VbYffBeLG2Cs6Hmp8G+xcmpzlwLtZMeV/voS1suynWeeoQIpnf6e9pmkDZOt6GjaaLK5PDIrgsF6xbdCEdQI8DG01P4QTJwHk6O+V3W6eBUku7iTHwWwkbdD2xR+UMYNv8bnec0D5fzZza6+sshT0Rt+Y2NU7U3TUNlLrbs0xPm5Do0jnQXorL2G+sPI8r4XBoIwZU8jgdr+q75HWbEa+oF8jn8W7siaq1WgzLYrTyXqbFofcFcdLzMzjceVry7kIDK37s5RyZ9/WH+Rx2yWu6VhoDN5JevCY3HE7cGVd67lI53vI+dmpynEkOe+bztj1e/Gydx/fG8gW1jjMVXu3Hm645Lh+e78jhOCdrOA13DPhPti4UFFSQTTrvwYPsswCSnoqI6SLiDhzO20XS8w3SJCIWx2KT+0t6I4/9GTgqIrri1c2+qlkHqMJvHhwy2UPS3/LYc8CyEdEZe8t+pBqbF7fT11kVC4z+GYeYzsdG8nt5+mkR0UXS13XxS8wBHBERz+TnbXCC+Dl48EXSU3nuvTVzAyAiJpI1aU7C+SazSXopInphD8+b2OP4EnCcpLdqprghcE9EvC/pndQA6oE9JQtjD+izwFtYK+2RMf9X4waS3o2I1bG3cxHsHdkLeA2X+18aERMDm+S483Hr2a0DrXclIrpjA+QsbCgtGRGDJN0oa3q13pFGxpnk2gMbSKtExFD8/P0qIrpJOjEivsLNv2tHZbyZN/efj4idcVhxPux1nBJXCr7RwLvyX6FT0wQKOg4iYiZJf8fu8pWwBwAASUthPZt1JN3cEL+o7H6OPWWbVo5di+PqS+Pu8ffXx+6f+PXBA8SgyrFHcfhpE+BcSU/USK86mO2JV36b4Oq7L7Eh0D8iDkwxSOo0niJihYiYUdLDWGtsR+B8SR9gzawdgU8i4vbKz8To/7dxxrFXRHRVm6DfX7FhshuApL9i/bELyeKFJhYakk5Ibo9GRM9cYEya3MCG6C3YYP6mbn4tSLoT54xthb2z9+BKsbUjYiJ8DbeX9FGdxlNyU0Ssi4tS7qUt0fkRYGBEbJDn1b3AGN1z/3Eevxsnr6+dz+gWEbGEpFPrfg6rHCOiJ15MnhwRW+P3+WmcI/a5pDclPTW+GU9ACeGVzRvONTifthDTdlhAsXbdn3/Dczlg9/y8IK5gO6LdOa38hKYSxltd6zfGk+m27c5pCRY2wW9h2rSTTsG9sVqhiCVxWKz23DZc8dcLJ5f+EPgxDn8uXTlnOux1bCQZG1cN/RFX1i2ex3vRTvMHG/atxstNVNtNk/+egeUJZsZl+Gdgw+k5YPW6r+G/4D0AT6itgpRaqz3HwGlxnLTeF3uLb8UFC1PicNn5NCDk2Y5jX5zvOQXwI1xZ2WqovSXOe6v9WlafebyInBSnMfTFeYEHY02vF2k4J+u/3UoIrwAASa9HxMPAMhHxlaRfRoSA6yNiAzXg5h8DvgR2j4hvJJ0eEVsBZ0TEsZIOAFC2SlC+wTXjY+DAjACcHhGdgH7pTj8zeX1eF7/RtOD4EngpIg7A4dgN5VDEIEk3RsTGkr4Y17wq/DpJGilpWETMh5Pr+0u6KiI+BM7K1f6cOEl7HzXgNclruH1EbIeriH4cEcOw12QE9kS1zn2p3c/VxjEiBgGbRsQ+koZGxMnYSN4Ae5aXAT6U9FBdvP4dJA1Ph8VjEbG8si1Lze1jvg1Xp/ekO27H82geewcXLvwRt1u6RjV7TCJiWknv5Of9ca7Y28CHuGjmJuCgiNgWJ71vpBpb3LTQumcRsR82Pt/HxtJh2NCbHefprQE05gH9XtC0BVe2ZjbaVqtLkB6T3N8Kr64G5/4O1KxPNAaePWlrutoHJ0rukfsL42qi2hM5K/wmp02WYFGcY9LyRG1BA2XsjLoSXAevALtgL8TzFe5DsBel9iqiCr9WwcIwHCbpnft74sH393SQRNg8tgY2TE7GidAv0rBuDTaOngKWaXf8xDw+SxO8xoL/elj3q9Porvk4/L0T5e9eJMfD3bGn+1c4N6sl6XEkMKChazNn3sfe+R63inqG0SbGOzn2Ni4CzFgzv/bJ4v2AW/PzlXktO7c7p1EP3vfydzdNoGw133BXWHVqPbzYfT4CGFo55wgcvtuycqxJnaceWPV3dzK8hMNN7wAH536tCt7t+M2GV399KkbU4jiJeMfcn6lBfq2eYnPhUNSauJT4cmC/NE5qrX5hVOPu2yrP3D8Wh+965/5CNCdX0DIyB+EQ2MW0aYpNj0MUF2Il5T5N3ePksx1wYn7uwqhtgo4ne/F15I0M49X4+1rh9JXyHfkrbT0fT8lxZ2ucW/QyNeu1VXjOg8OIm+CKzxNwUcAtlb+hX4P3rVu7/f7Y47lfcmwZoYu1N6TG561xAmWr8WZ7NfUbvHJ+HNgpjZPWarqlB7RYGgS1iidWeM6OPWH7AJvmsXVx6fWOtJVoH4OryGpdWWNtpzVxdVhL8fwgrBOzGG05WOfg5pi9Grzn8+Ik2BkrxybC8gAH4rLnRjw7FT6d8Sp10cqx44BX6AA5EliE8hHcwuiOnBC6tjun9nvMP6/6N8CJ99BmyC8PLNf0NeyIG85nugd7buYEXsW5OS0V+U7YA3oszg1sorfd7GQ7mxyn38Ue7TOxqGdrQbkT1phrIn9xTVzAczhOCQAvLO7Isac1Hu6GpSkmqZvjuNpKDtT/CLKM9DKsmvsEXj0Pw6uZc/Fkf1RErIA9ArtJ+lMDPOfD1TfX4sFj5YjYSdLKEfElXglOHxFPY3f2JpJeq5Hf/HiyvxmLjS4XEe/hEuzdsPfu5IjohkuH+8nVWXXxa5838g5WZu+alU1fSfoycz6OqYtXhd+SeEB9OCKGYE8iOMl5toh4TtIXkvaLiDehuTGqci0Xxfd3AZwbs62kryJiUkmfwLcVeLVzi4jVgFnws3Y1sG9E7AvcHhGTYMX7LevkNr5A0gcRsQkuBAhsRA3AOYzTy3l41+Kq0C+VuYt1IaVQrgCej4g9Jd0WEftgg/51XEDzi4h4FXumfiTpHzVz7I+rsy/Gc0r/iHgCe/IewMbpATlGDsFK+J/WyXGcomkLrmzjfsOGyPNkzzra+jUtht3Wu+X+rFhKv5EVK56gHgI2zv1W+GQ4cF9+XhW7rh8hFXdr5Dc/liJo8euM8xHOJVuiYOG6s7Bg4bo186uGxXrgyb4bXvVtU/luMBYsrHUliFW6H6dNfXoh7FncC1dhvYEn/Jup5OU1tZG99bAXYjjWRpsjj20IHEVWMDbEby2c27Q6VpxudQ64DE9od9T9jowvW2sMzM/bY1HPZXJ/axwu+2ne80ZEg/Hi4VJsjFyEjbtNgEPyvVkGpzXsQQMq7XjxPRIYlPs9k+eyuT8pDuUdi6MFjUQ0xuk1aJpA2Wq4yV7lv092gWfUpo3LYeNqtoY5dsrB4sXKsW6Vz9cDQyr7U+W/dbWdmBivBke0O94ZqyhfDCyZx7rTFmasi1/VeNoHG6Ln5OQ6Bzb8zsHG3R+ouXdcDqQPAGvm/rSM2uR0J+zZmwaLoDad8zQP7mm3YnJ9FotOgvNlnm39LU1wxIbxpVjKY/W8v70rz2TQZgA2lr/YEbfKPV6RNmN+qxwjW0bUAOwFrz1pHC9k58nP02IPz09x+sLx2PN0CQ3KoVS4DsSVia0mxcNxQcopOMWh1nGw7q0IaU7gSFf/43jFfGZEbC6XgSus2N2qIHrvX/0/4xqSRmJPxOMRcUke+yLDTmCOc1V+5IM8R9SDL/HK/qkUm+yWv/8bSY/hgW79PPa5sty4Ln6t3xMRS+Fk9n2xB+IgnLuzFvZEPQFsIOnpOnglp6nxwHqcpFsjYi48OVXv52NYbfpdSZdJer0uflVIUkQMBH6OjZM9k+eaOJx8KfAzrHJ/a1McZamJV7DX6TBgC0mvRsSWWAZCWGm8znekwyMiOuc9Xhsn//cAkHQR9uTcFBErSxqOPc3D6xRsjYhJ8f08JCLWz3HkZezRvhJX876FFxmn1MVrTJBFlffF4/apOL/ydGxUbY1DjFNMsM9g0xZc2cb9xqgd2V8mO7LnseVxp+7pmuaZfKbBuRyXtDu+E560muTWFa9MT8UJ2BNVvtu7el3rvrf5eTncxmH/3J84+d5GOzHPBngOxMbbIslnn3bfz4U9VFNRCa80wHNWPPjPg0O2u+CJdnEcUpmYlKOgxlV15R2eC/fc644n0ZG0FTIsjkOhHb7aroH7OnPl8yzYC9u3ct1Wzs9bYsX2qWkoPJv3dzPc1mZn2op8Wh7uGbCRNWfT17XCefV8FmeoHOtEeqAm1K31UhZM4GiXdHoufjHfwF6VgyXd0CjBCiJiGhxu+kzS5hGxGA6f7SDpnoa5dcUD2trA3yX9PCIWSX7bSXqwIV6zSHotIk7E3df7yUmyEyXXbfCK8AM19NJnwulw/Lwdk96Ab9Ib8CnwO2VSdlOIiAVwJdsyud8b+AUOmZ0u6bYGua2B34sHsIE3CN/TH2IpiiWAn3Skd7kjINzf85e4g8FLeWwY9mJPhz20nYFfSzonInqp5qKA0SEilsD5Q7fhhe6UeIz5c0uAtlGC7ZDv8fHAqpLebJpPHSgG1P8Q2hlRvwa+wr2mhtep+tuO02gHgjSiTscega64XUvtPfhGx69iRPXDlSeLAodJurFufslnCRx+uFjSHRFxOhYXXU/SP9KI6tq0cQLfGgGn4lyT98O9sYbiCqJXGuDTeidaDYKJiItweOxYSZ9FxI74Hr+DE8e/rONdyYm/u6RXchExGLhe0gMRsTe+54vR9o58JekPTb3LHRER0QU/X1MCZwMXSBoYERtij+0IOay8E7CEpB0qhn3j1zHcR64/9pLtjPMbh5GR3Ca5jQ4RsR6WM+jT0Qy8cYFiQE3AGN0AUJkwlsVJ2nfXPVBExJw4D+f43B+TETUtruo4T9K1NfKbGSePnjcmfmlE9ceD85mSbmjQCO0GHICNuasl3ZX5CCtjTZv36+b0r5Ar1V9gYcrBwE6S/tgAj9a7MBCHIL5IXovi0Of8OFn3EJz3tCWwlaR3a+A2H27PcSTucTYCG0nrAq9IGhkRxwKfSvrJuOYzPiPcFPhnuI3Ifnjc26zy/dLYuDpA0ohmWI4ZOdZ0x+KZJ6iBBtVjg4iYTNLHTfOoA8WAmoBQmRB64x5JnSV9NKbz2n+ukeesWADzCElH5bExGVFdZc2dWnhmwujKwKHYGDlzTPxydTulpHdbiaY1G6ID81cOTy/T3ljL5nxJD0XE8cCpkl6ti9N3RUSsgw2ExRs2ngZgWYzNcJ7Jk3iiegPYFpgMe2u75vFWYu+45DYbFrI9SdIv89j0WN7haqV+V0QMBeaStPe45DMhIEPbXXA12wnA5JI2iIheuB3PRSX0WTC2KAbUBIZwM9F9cJLklzhv4y/tzmm5qLvj1dgHNfJr/e7ZcP+6MyQdmd+NYqREpcFn7tfZXHRjLPx2mVyhMzp+rUm4NuOuYvh2xWXN6+LqttvSE3UZTkI9SNJ945rTf4OImEQ1i+pFxIy4ndHncgPtM3Dvx2lpa2E0I05yfzp/ZnVsZG0v6fc1cNwGWEzSHuFm1EvgxOcFsO7PDVi2YDvg6KZCxx0VuUBbEHhJ0ot5bHnc13M34HOcBzpthvOmlfRORwjZFYxfKDIGExDCKtlH4KTSkTgU8V4Owq1zWgbMVMCduOqtNuTv7pxG3QrA0Ij4cX43ssU1z/k6InpExOm5X5fxNADLPnwFbBcRu46BnyJiSqxc3HUcc6oaT1Pi1fRZ2GDaNSLWkkvbb8J92Tq0mx+gAeNpPmx8nA38LCLWkDQUe5sOAlaTtCUO3e0eEVPkj36Nc7TGufGUeBnoExFrAefhBdEx2Bv2AtYv2gBXfd5Yfb8LACeGbwecExE7Z0jpAXz9jpZz3XYBPoqIxVWz5EjBhIPSymXCQifs5l8Kq9RuJunDiFgsIp7FSaYt4+lK7KV4uQ5iFW/NMsB0EfGmpEcjYjnggYgYKelnaaR0SeOpxfNoWbuqDp7TYW/DlriMuC+wfUR8IumC0fC7DleVfTUueVWMp32wkOOsyfMpPMGfEhH3YA2ojSS9NS75jG8IV9ddhsOcL2LNro0i4gFsKH8NLBYR72IR0nMlfQgg6e6a6T6Ge1YeixtSD8M6QLPjarELcHf7rYHDRxf6/l+GpMcjYnOcJP5TYNGIeAnrFZ0YEfNKej4iNqtrXCmYQKEOoKVQtv9uw679PfDK6x486M6S3/XHAootRdgeWGBxxQZ4ro09I7sAH9PWEmV2rL3yk8q5PXA7hVp40hbOnhFPoC2l86loM1R2rpw/FS4vru06YjHHR4EpcELsWbisGezN2xuYu+nnsSNueX1GVvbnxfpnM+KF5B64SfALWIiyI3Ceut3+KriFUWAZg4eYwHV2vodrOCNu/zQ8352PsRxK49zKNv5vxQM1niOTl+fBAnvvYI/NwsAmEfEMrio6VG2Jr9vjlhS15sdExOy4oot5OA4AABLJSURBVGgQbgL8NnBsWKX2vIhYGBtSrfyeK+vgWQmNzRIRf5f0RnpyzoyIIXKp/ZM4ifjR/Jnu2AMwTvmFpRxGqq1B6PTA87Jn5Ipwg87TIuIBSffj7uwFo4Gk+yNiQES8LGkOvOjohKvYvo6I83FSew9JTzVKNiHpPRhFNuNo7O0U8EJErKhKjmDBP0PSGzhEe2dErI89j7U1Hy+YsFGSyMdjREQ3ud3JdDj35Rzck20V7N5/C7hd0s0dIUEy3MJjGpw4vmREbIbLxAdLujzPafX56iHp7zXxGoBzYO7DejHnAlvgarxzsOt/S0kP5/mzYl2lP49jTkfgfKaXJB0c1nvaExcGPJLn/RJr2xTj6Tsgr+uVwHNY4uHTfOZo+v0YHdJ46ov7oQ2Tc56+bS3SETl3NFSLP6IDaTwVjP8oBtR4hJy4Z5D0WETMgxMlr5NL1pfCmkSHSnotzx9lsKi5iq31O+cAJlaWqoer2wZI2jrzoQ4Dfi4nedaOsIr4JXhluiuu3tkY+Ai3j+kCvKAa9WHCit2HAscBr+Ik4m1xns5PcDuRd/HK+jDchuJvdfEb3xERq2LR0Z65P0q1Z0dDGlHTpHe0TPwFBR0EJYQ3fqEv8OeImBiYCXgduCCsPP0Jbgjci3RRKxMkWwNunQNvGk8DsZr48znu98fJsBtGxDnA0lhE8aG6JobR/J5OuIy9N86TGaxMvAfOVs0JutHWeHdDSddHRF8s8ngKNuqOwQroK+MWFOsW42nsIOnOiBgSEW8B81ZCpB0ScoHCG/m5GE8FBR0ExQM1HqCdC3oKLPh3vKQR6XlaEWvFDMZJpisC3zQ52EbE3FjS/0RJT0TEXcAnktaJiMVxmPEZ1dhbLHOXBki6Jr0Qc+HQ5z24ummJzHlaHWtA7apxLJo4Bp4DsXLy1ri31IO4l9fVwNOSts3zusnSBQX/ATKc96nqr7IrKCiYAFA8UB0cETEJMB/wRE76z2Itm72y9P824LH0Sv0FuKvJcERq0vTA5cOzAC0vWL+IuDMi7pS0KlZ8rlUcU9LnKelwNBbT20kWUzwOe3kGRsTbOPH+8CaMp+R5c0R8g6/RwWpTnu4HXB8R00l6GwulFvyHkDQcmlHjLygoGP9RPFAdHOF+cEfhxOpVcOn/o+HmlxviypwHJX1e+Zkm2rOM8jvTy3QgcC9ws1INPSLuxyrPjzTBL9wS4w6sibVEftcTi47uCvwVuEHSTU1PrOHGu6cBS6dnbBtcRbmWRtOip6CgoKCgPhQDajxAuG/YxcDlsnJy6/jOuFrscFxt18jNrBgna+Bk7A+BK0htJ1zdNkI1iXb+C36TAJ/hSrujseL0BpL+EZZT+LD9zzTBt4pw493jcOPdTYChkp5pllVBQUFBQWkB0EFRLVXGXpwNgVkjYv/0SiE3uj0N+LDJyT6Nk1Vwvs4TWOPpDizseSTWsFk7Irq3+7vGOSrG0wB8rQ4GJpW0Mxb1/E1E/BCHSOfuaCXtkm7BnryTsZBnMZ4KCgoKOgCKB6oDojLpr4Zzcx7Eqsm9sSfiJlxxNwRYW9JnDXCcEVhO0jW5vyvucH507g/EuUTL4erBtyT9oW6eyWUN4ETcnuV8nCt2oqT7IuIQfF1vkHRTE/y+C6KBxrsFBQUFBWNGSSLvgEjjqT9wAjZCDsZtPE4Adgb2xwnaZzRkPAWwCPCniJhG0rs4NLZS65xMhF4fC2LWVmmX/HphNfY7cVPltYHNgRly/y+4ATCSjoo2QdIOEbYbHYrxVFBQUNCxUEJ4HRARMTPutr4e8Dfcd+1rbDh9I2lHrFd0ZQMhsU4ybsXaND+NiCHAhcBsEXFOREweEcsDywKT1MkvsSSWAegvd14/BPg7sB8WndwLayhtmhVtX0DHCdsVFBQUFHR8FA9UB0OWqnfFgomdsCGwPDArcC3wZUQcLukDqHfSj4huwFLA/RExLzAH8DAWoHwveV6FxTMXAA6Q9Ke6+LUg6bqImAjYI2UVrgMmw21kFouI13H+0zkpB1BQUFBQUDBWKAZUB0JELITzdE6V9EpE9ME20nvhfnePA2e1jKcGMA2wQETsi1uerAi8D3wBDMRk14uILsD0qbFUe/uY5HFlRHyG+9iFpKsj4gyc6D45Nu5+XwevgoKCgoIJD8WA6iCIiKmwgXR5Knd3kvS7iPgkIh7AFW17SnquKY5pEH0NrANcIXc6JyJuxblF60fEjJLOiIi/58/U3XtvVdzm5EXcEuVrYP+I+ApLQfwWmKzJ61hQUFBQMP6jVOE1hIiYFLcOuS+r7V7B1WoXA6tJuq9y7lrAGw1WsX3r2cmQ2CAsPDk57hD/t4iYHVfcPdlE2C65DcQK6CfhNiiPS9o/ZQr2B46T9JsmuBUUFBQUTFgoHqjmMBLYLSIOxPlNO0q6PFuy3BwRA1tGlKQRTZGseHbWxGG7jyWdGxGvAFsBQyPiSdwY+PiWV6oBnl1xLtag5DkFcCqApKsiYiRuvlxQUFBQUPBfo3igGkCG50ZGxDI4Mfw+SRtXjm8FXIA9UXc1y/ZbJfSfYlXx/YCXsIdnPmAzYCNgP0k31MhpFmBq4GtJz2Y14ok4eX0SYDNJ/5deqU6SbqyLW0FBQUHBhI8iY1Az0qMzMhPGJwcGYIXxEyWNBJB0EbAuMFGDVAGIiKmBwdhI6oI9Zz1wtd1zkg4FVpR0Q12SClkBeAvWx/pNRGyUIcZfJber03haESt4fzjm/62goKCgoGDsUTxQNaISDhuEe7FtnYni0wE3AvcAd2FPz2qSPm5C3HE0OU8zYy2qS4D+WBLgQeAJSWu3PGc1cZsL+A3OZ/pVRGyM29xshxcEA4A9cLhuHuCgjqwwXlBQUFAwfqLkQNWAiJhM0sdpPC2Mw2EbSHoxIubAXp1VgbOBnYBjJX0MzYg7Js/lcCjsXUlPRsQ0wIOS3oyIeYAzgRF5fi3GU+JHuLJueO7fgr11vYGvMo/stzi81zmvcYdVGC8oKCgoGD9RPFDjGBExGfBrYBtJ72S12uHA09iTs0qeOgz3uJtM0vtNTvoRsTRwM3AZTg4/ERtL1wPPAD/ASuh318ip6hUbhmUdhuIcrGOx565Pcvy9pLPq4lZQUFBQ8L+HYkDVgIiYFotQLoRVsbfBE/9J2CBZHSc6NzbpV8KLMwCLAV9IujsiBmADZTfgD1i+4EtJD9bIbRJghhQXnQ/3sjseX88pgXUkvRYR8wNrAPdLeqIufgUFBQUF/3soIbxxiJZRkp6nlYFzgE8lnRcRF0v6MiL6ArtixexGeYYbGB+GPWNXRsRjkoZnbvgFOJ/o8gYo9gb2jIg3gCHA8pJ2jYijcZ7TJxHRPavxXpD0TQMcCwoKCgr+h1Cq8MYRKkbJ9BExsaSrcen/GRGxQRpPS+Lw2GFyc95GkDyXwflF++Mk7d7ACsl9OPZAvdYQv2eBPwMHYeHOv+Txg4A3gV8mX4rxVFBQUFBQB0oIbxwi9ZMOBAI4WNI9GRI7GTgA5+v0kvRqk9V2EdEd5zh9KWmN/O4A3Cz4JuB2SZ9Vf6ZmfgtiA2kOYG3gLOBuSR/leScCF6v0tisoKCgoqAklhDeOEBFLYK/NUFxht3dETJl6SV2B03CuzqtQf7VdxThZF+c1HYTDdntJOknSsRHxY5ww/jDwWd08K5IPewIHZjjxo9z/KCsDlwf2rbkSsKCgoKDgfxzFgPqekNV1fSVdEREzYi2ibyQ9BTwVER8D20ZEV0lXR8SDkt5uim8aJ0vgsN1Zkh6MiI2AUyPiG0mnSDoyIuZoimdqPh2B29z8LnlflDlZOwJz4fYxxXgqKCgoKKgVJQfq+8NkwMsRMZXcD+42YLKI2AFA0nnArcB2ETF9k8YTQERMjo2QvmrrufcQ9pjtHBH75LGXm2PJNMA7LeMpIiZKThcB2wPrpsFaiwJ6QUFBQUFBCyUH6ntEuBHww8C5kk6LiE2A1YCHJf0yz+kl6a8N8RslfylFPU/ELVl2qxxfFssqPNAATSJiSUmPp3zB+cC5uF/gl1nN2A84WtIXTfArKCgoKCgoBtR/idEYJasAxwFnp1zBxsB6wD2SzmlKIDMiOkv6JiJWB/pi7+MvgZ7AtsDHkvarm1c7jq28rAdwQnu/iNgdmBP4BHgEX9uhkm5vkmtBQUFBwf82igH1PSAiVsCijn+Q9FBELA+cCpwm6fyIGAw8LenpBrhNI+nd/LwScCFwKLAV8BRuifIebsz7uqS9GuDYSW6w3F3S53nsHuBNSRtHxGrAOkBX4GZJt9TNsaCgoKCgoIpiQP2XSCHMa4GLcf7QdpKuSl2li4ETJJ3dELf5gN8CK6dUwpHAR5KOi4hu2GjqKWm7iFgMGJlJ73Xxmx63rnk5IhbFFXW3SHolv38YeE3ShrnfrYTtCgoKCgo6AkoV3n+ASqhpJhwK20HSzRl6OiUiSCNqG6wB1RT6A1cAU6SX7HlgtYiYRdJrwOER8WBE9K5bQykNuG2AeSPiJ0B33Iblm4j4bco7bAi8EhHXSPoB8FWdHAsKCgoKCsaEYkCNJSrG05rA2cAbwFsR8YCkmyJiJHBRRHRpqO1JFc8CA4ANcJ7TI8BK2Ih6EBt/3YCv6yYm6YuIuB4YDOwNHAL8HGs8ERHXAj1wztMd+TNFrqCgoKCgoEOgyBh8R0REZ/hWP2kxXF23JfBj4GXcq22KbHuyLW4x0jSeB6YDXgI+kfQScANuFnw2TiL/eXqjakNEtJ67mYDZcVXdUcAruCqwLxYaHQ7cKunOIlVQUFBQUNCRUHKgvgNSGHMgcCfuB/dM/rsWIGAVHC77GhskH+TPNdmeZVLgU2BmXAW4FHCppDtST2k6oKukvzTEcxHgOmAjYAFgfqyl9eM8pRfQraUBVVBQUFBQ0JFQPFDfDXMD62KDaSTOzZkOK2R/hUNMtwMTA9O3fqgJuYI0ntYGrsbVdp0lnQH8EfhRftdZ0mtqa8pbG8+KJ2kG4AlJj0u6BPcFnA84Fphe0tPFeCooKCgo6KgoBtS/QXpn7gNuxI1s90s5gsHA7hGxS+bmjAB+IunFhnh2yX/7AnsBFwHzJMc+ko4H/oI9PpM2wK9lOHXOf38PzBgRmwFIegTnbAUlN6+goKCgoIOjTFT/BhWPzqY4p2hIRHwqaVhEbAiMSJHKU4B36+aXPfjek/RBfj4BGCHp1xFxF25ovEly/HlEzC7pnZo5tsKKqwLrR8TfgMeBYcBKETEHbnOzFLCTpD/Vya+goKCgoGBsUTxQ/wYRMTU2Qo6SNBTYGVg+Ig5IzaT+WJCyKcwJvBoRUwH/BzwBbB4Ri2ZPvpPxfR6cSe6v1E0wjaeVgAuAJ4FZsTdvbuAybDjtCRxXpw5VQUFBQUHBf4qSRN4OETEvsDhwv6S/5bHzgKexsvg3EbEl9p78OI+pqRYtya8/Vj7vk56oQ4A+wOGSnoqI6YCpJT1fM69vr0lE7Ixzr06LiGmAlXEl4144+b6bpM+avI4FBQUFBQXfFcUDVUHm6ewAXAL8IiJOzoa2v8NCjyvkqQ8D9wF3tSb7Jid9Sb8F9gAej4gpsZ7SI8Dx6Yl6u27jKXkpIvpHxHrAl8CWETFztpa5DVfezS1ppKTPWj9TN8+CgoKCgoKxRcmBqiAn/BHA0sBhOJ/oEFxx1wnoGRE7AQsDe0l6pjGy7SBpeOZp/w57n47FveNqv8eVnKelgM2x5tRNwGzAvhFxEhbwnJSiLl5QUFBQMB6ihPBGg4i4Dnhc0k+zHcvPgPdxX7kuwAV1tz75rshw3gXAfC09qhp/94zAJFh/6l3gXuADSf3z+6WAHwBr5jknS7q6To4FBQUFBQXfB4oBVUFEdJI0MqUABgFX4STnk7FK9vLAtSlj0GEREQOx8vjdNf7O+XDz5A9wMvuNwDv4Gv5E0pmVc2cCvpb0dsl5KigoKCgYH1EMqNEgIqbHhtMKwJ6Szs7jk0j6tFFyY4G6jJOIWABfr72BF7Hy+aLArvgangCcIenccc2loKCgoKCgDpQk8tFA0ls49+kPuHdcyzs13hhPUGtC9tTAopLuysrFO7BUQQ9JdwK7A/tFxI418SkoKCgoKBinKAbUmPEkbn+yYiu01zShjgpJ9wMDIuLlPDQ/frZalXX34erGPzbDsKCgoKCg4PtFCeH9C2QuVBdJDzbNZXxARAwArgSeA1aS9GnJcSooKCgomBBRDKiC7xXZruViST1zv4ukrxumVVBQUFBQ8L2ihPAKvldkztOQiHgrInoU46mgoKCgYEJE8UAVjBNkOO/TOqUUCgoKCgoK6kIxoArGKUoOVEFBQUHBhIhiQBUUFBQUFBQUjCVKDlRBQUFBQUFBwViiGFAFBQUFBQUFBWOJYkAVFBQUFBQUFIwligFVUFBQUFBQUDCWKAZUQUFBQUFBQcFY4v8BnrtdTxo0ZEIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot     = True,\n",
    "    cbar      = False,\n",
    "    annot_kws = {\"size\": 8},\n",
    "    vmin      = -1,\n",
    "    vmax      = 1,\n",
    "    center    = 0,\n",
    "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
    "    square    = True,\n",
    "    ax        = ax\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation = 45,\n",
    "    horizontalalignment = 'right',\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train : (8631, 17)\n",
      "Shape of y_train : (8631,)\n",
      "Shape of x_test : (3699, 17)\n",
      "Shape of y_test : (3699,)\n"
     ]
    }
   ],
   "source": [
    "### Splitting the data \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# checking the shapes\n",
    "\n",
    "print(\"Shape of x_train :\", x_train.shape)\n",
    "print(\"Shape of y_train :\", y_train.shape)\n",
    "print(\"Shape of x_test :\", x_test.shape)\n",
    "print(\"Shape of y_test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative             1.960357\n",
       "Administrative_Duration    5.615719\n",
       "Informational              4.036464\n",
       "Informational_Duration     7.579185\n",
       "ProductRelated             4.341516\n",
       "ProductRelated_Duration    7.263228\n",
       "BounceRates                2.947855\n",
       "ExitRates                  2.148789\n",
       "PageValues                 6.382964\n",
       "SpecialDay                 3.302667\n",
       "Month                      0.061312\n",
       "OperatingSystems           2.066285\n",
       "Browser                    3.242350\n",
       "Region                     0.983549\n",
       "TrafficType                1.962987\n",
       "VisitorType                2.326762\n",
       "Weekend                    1.265962\n",
       "Revenue                    1.909509\n",
       "dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:   28.2s finished\n",
      "\n",
      "[2021-01-21 02:36:08] Features: 1/8 -- score: 0.8771863833813277[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:   18.0s finished\n",
      "\n",
      "[2021-01-21 02:36:26] Features: 2/8 -- score: 0.8806641325472677[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   18.3s finished\n",
      "\n",
      "[2021-01-21 02:36:45] Features: 3/8 -- score: 0.8894683678213586[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:   22.9s finished\n",
      "\n",
      "[2021-01-21 02:37:08] Features: 4/8 -- score: 0.8929438085413128[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:   27.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:   27.1s finished\n",
      "\n",
      "[2021-01-21 02:37:35] Features: 5/8 -- score: 0.8983894713545446[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   27.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   27.6s finished\n",
      "\n",
      "[2021-01-21 02:38:03] Features: 6/8 -- score: 0.9005913353069461[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:   21.8s finished\n",
      "\n",
      "[2021-01-21 02:38:24] Features: 7/8 -- score: 0.9008237290771057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   19.0s finished\n",
      "\n",
      "[2021-01-21 02:38:43] Features: 8/8 -- score: 0.9005926233519935"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=101)\n",
    "\n",
    "sfs1 = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1), \n",
    "           k_features=8, \n",
    "           forward=True, # if forward = True then SFS otherwise SBS\n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv = 4,\n",
    "           n_jobs=-1\n",
    "           ).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Administrative_Duration',\n",
       " 'ProductRelated',\n",
       " 'ProductRelated_Duration',\n",
       " 'ExitRates',\n",
       " 'PageValues',\n",
       " 'Month',\n",
       " 'Region',\n",
       " 'VisitorType')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs1.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data.drop('Administrative_Duration', axis = 1)\n",
    "data_new = data_new.drop('ProductRelated', axis = 1)\n",
    "data_new = data_new.drop('ProductRelated_Duration', axis = 1)\n",
    "data_new = data_new.drop('ExitRates', axis = 1)\n",
    "data_new = data_new.drop('PageValues', axis = 1)\n",
    "data_new = data_new.drop('Month', axis = 1)\n",
    "data_new = data_new.drop('Region', axis = 1)\n",
    "data_new = data_new.drop('VisitorType', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12300</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12301</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12302</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12303</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12304</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12305</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12306</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12307</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>368.25</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12308</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12309</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12310</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12311</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>211.25</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12312</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.011149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12313</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>86.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12314</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12315</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12316</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12317</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12318</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12322</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12323</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12324</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Administrative  Informational  Informational_Duration  BounceRates  \\\n",
       "0                   0              0                    0.00     0.200000   \n",
       "1                   0              0                    0.00     0.000000   \n",
       "2                   0              0                    0.00     0.200000   \n",
       "3                   0              0                    0.00     0.050000   \n",
       "4                   0              0                    0.00     0.020000   \n",
       "5                   0              0                    0.00     0.015789   \n",
       "6                   0              0                    0.00     0.200000   \n",
       "7                   1              0                    0.00     0.200000   \n",
       "8                   0              0                    0.00     0.000000   \n",
       "9                   0              0                    0.00     0.000000   \n",
       "10                  0              0                    0.00     0.000000   \n",
       "11                  0              0                    0.00     0.018750   \n",
       "12                  0              0                    0.00     0.000000   \n",
       "13                  0              0                    0.00     0.000000   \n",
       "14                  0              0                    0.00     0.000000   \n",
       "15                  2              0                    0.00     0.008333   \n",
       "16                  0              0                    0.00     0.200000   \n",
       "17                  0              0                    0.00     0.000000   \n",
       "18                  0              0                    0.00     0.000000   \n",
       "19                  0              0                    0.00     0.000000   \n",
       "20                  0              0                    0.00     0.000000   \n",
       "21                  0              0                    0.00     0.200000   \n",
       "22                  0              0                    0.00     0.000000   \n",
       "23                  0              0                    0.00     0.000000   \n",
       "24                  0              0                    0.00     0.200000   \n",
       "25                  0              0                    0.00     0.000000   \n",
       "26                  4              0                    0.00     0.002857   \n",
       "27                  0              0                    0.00     0.050000   \n",
       "28                  0              0                    0.00     0.000000   \n",
       "29                  1              1                    0.00     0.043478   \n",
       "...               ...            ...                     ...          ...   \n",
       "12300               0              0                    0.00     0.022222   \n",
       "12301               0              0                    0.00     0.200000   \n",
       "12302               0              0                    0.00     0.021212   \n",
       "12303               0              0                    0.00     0.000000   \n",
       "12304               0              0                    0.00     0.006667   \n",
       "12305               0              0                    0.00     0.000000   \n",
       "12306               0              0                    0.00     0.018182   \n",
       "12307               2              3                  368.25     0.020000   \n",
       "12308               1              0                    0.00     0.009091   \n",
       "12309               3              0                    0.00     0.008333   \n",
       "12310               1              0                    0.00     0.015789   \n",
       "12311               1              2                  211.25     0.001361   \n",
       "12312               7              1                    9.00     0.011149   \n",
       "12313               3              3                   86.00     0.000000   \n",
       "12314               0              0                    0.00     0.028571   \n",
       "12315               0              0                    0.00     0.000000   \n",
       "12316               0              0                    0.00     0.013636   \n",
       "12317               3              0                    0.00     0.002105   \n",
       "12318               0              0                    0.00     0.000000   \n",
       "12319               0              0                    0.00     0.000000   \n",
       "12320               0              0                    0.00     0.014286   \n",
       "12321               0              0                    0.00     0.200000   \n",
       "12322               6              0                    0.00     0.000000   \n",
       "12323               2              0                    0.00     0.000000   \n",
       "12324               0              1                    0.00     0.000000   \n",
       "12325               3              0                    0.00     0.007143   \n",
       "12326               0              0                    0.00     0.000000   \n",
       "12327               0              0                    0.00     0.083333   \n",
       "12328               4              0                    0.00     0.000000   \n",
       "12329               0              0                    0.00     0.000000   \n",
       "\n",
       "       SpecialDay  OperatingSystems  Browser  TrafficType  Weekend  Revenue  \n",
       "0             0.0                 1        1            1        0        0  \n",
       "1             0.0                 2        2            2        0        0  \n",
       "2             0.0                 4        1            3        0        0  \n",
       "3             0.0                 3        2            4        0        0  \n",
       "4             0.0                 3        3            4        1        0  \n",
       "5             0.0                 2        2            3        0        0  \n",
       "6             0.4                 2        4            3        0        0  \n",
       "7             0.0                 1        2            5        1        0  \n",
       "8             0.8                 2        2            3        0        0  \n",
       "9             0.4                 2        4            2        0        0  \n",
       "10            0.0                 1        1            3        0        0  \n",
       "11            0.4                 1        1            3        0        0  \n",
       "12            0.0                 1        1            3        0        0  \n",
       "13            0.0                 2        5            3        0        0  \n",
       "14            0.0                 3        2            3        0        0  \n",
       "15            0.0                 1        1            3        0        0  \n",
       "16            0.0                 1        1            3        0        0  \n",
       "17            0.0                 1        1            4        1        0  \n",
       "18            0.0                 2        2            3        0        0  \n",
       "19            0.0                 2        4            4        0        0  \n",
       "20            1.0                 2        2            1        1        0  \n",
       "21            0.0                 3        3            3        0        0  \n",
       "22            0.0                 3        2            5        0        0  \n",
       "23            0.8                 2        4            3        0        0  \n",
       "24            0.0                 2        2            1        1        0  \n",
       "25            0.0                 1        1            3        0        0  \n",
       "26            0.0                 2        2            3        0        0  \n",
       "27            0.0                 1        1            3        0        0  \n",
       "28            0.2                 2        6            3        0        0  \n",
       "29            0.4                 3        2            1        0        0  \n",
       "...           ...               ...      ...          ...      ...      ...  \n",
       "12300         0.0                 2        4            2        0        0  \n",
       "12301         0.0                 1        1            1        0        0  \n",
       "12302         0.0                 2        7            1        1        0  \n",
       "12303         0.0                 2       13           20        0        0  \n",
       "12304         0.0                 3        2            2        1        0  \n",
       "12305         0.0                 1        1            3        0        0  \n",
       "12306         0.0                 2        2            2        0        0  \n",
       "12307         0.0                 3        2            2        0        0  \n",
       "12308         0.0                 2        2            1        1        0  \n",
       "12309         0.0                 2        2            2        0        0  \n",
       "12310         0.0                 3        2            3        0        0  \n",
       "12311         0.0                 2        2            2        0        1  \n",
       "12312         0.0                 2        5            2        1        1  \n",
       "12313         0.0                 2        2            2        0        1  \n",
       "12314         0.0                 5       11            1        1        0  \n",
       "12315         0.0                 3        2           11        1        0  \n",
       "12316         0.0                 4        1            1        0        0  \n",
       "12317         0.0                 2        2            2        0        0  \n",
       "12318         0.0                 1        2           10        1        0  \n",
       "12319         0.0                 2        2            2        0        0  \n",
       "12320         0.0                 2        2            1        0        0  \n",
       "12321         0.0                 1        8            1        0        0  \n",
       "12322         0.0                 2        2            2        0        0  \n",
       "12323         0.0                 2        2           10        0        0  \n",
       "12324         0.0                 2        2            1        0        0  \n",
       "12325         0.0                 4        6            1        1        0  \n",
       "12326         0.0                 3        2            8        1        0  \n",
       "12327         0.0                 3        2           13        1        0  \n",
       "12328         0.0                 2        2           11        0        0  \n",
       "12329         0.0                 3        2            2        1        0  \n",
       "\n",
       "[12330 rows x 10 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = data_new['Revenue']\n",
    "X_new = data_new.drop('Revenue', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    7.6s remaining:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'adaboostclassifier__n_estimators': [3],\n",
    "    'adaboostclassifier__learning_rate': [0.1],\n",
    "    'adaboostclassifier__algorithm': ['SAMME'],\n",
    "    'adaboostclassifier__random_state': [0]\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), AdaBoostClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X_new, y_new)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123752</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                             params  \\\n",
       "0       0.123752  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0            0.5                0.0                    0.0              1   \n",
       "\n",
       "   rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  1                      1             0.5   \n",
       "\n",
       "   mean_train_F_score  mean_train_Sensitivity  \n",
       "0                 0.0                     0.0  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_abc= gs.cv_results_\n",
    "d = pandas.DataFrame(results_abc)\n",
    "table_abc_new = make_table(d)\n",
    "table_abc_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    6.2s remaining:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [20],\n",
    "    'mlpclassifier__max_iter': [250],\n",
    "    'mlpclassifier__activation': ['logistic'],\n",
    "    'mlpclassifier__solver': ['adam'],\n",
    "    'mlpclassifier__alpha': [0.001],\n",
    "    'mlpclassifier__learning_rate': ['adaptive'], \n",
    "    'mlpclassifier__random_state':[0]\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), MLPClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='AUC', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X_new, y_new)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.076736</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.655887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                             params  \\\n",
       "0       6.076736  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0       0.655887                0.0                    0.0              1   \n",
       "\n",
       "   rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  1                      1        0.688429   \n",
       "\n",
       "   mean_train_F_score  mean_train_Sensitivity  \n",
       "0                 0.0                     0.0  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mlp= gs.cv_results_\n",
    "d = pandas.DataFrame(results_mlp)\n",
    "table_mlp_new = make_table(d)\n",
    "table_mlp_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    1.6s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('gradientboostingclassifier',\n",
       "                                        GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                                   init=None,\n",
       "                                                                   learning_rate=0.1,\n",
       "                                                                   loss='deviance',\n",
       "                                                                   max_...\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'gradientboostingclassifier__learning_rate': [0.1],\n",
       "                         'gradientboostingclassifier__loss': ['exponential'],\n",
       "                         'gradientboostingclassifier__n_estimators': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'gradientboostingclassifier__loss': ['exponential'], \n",
    "              'gradientboostingclassifier__learning_rate': [0.1], \n",
    "              'gradientboostingclassifier__n_estimators':[100],\n",
    "             }\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), GradientBoostingClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.399495</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.658719</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752513</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>0.013452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                             params  \\\n",
       "0       1.399495  {'gradientboostingclassifier__learning_rate': ...   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0       0.658719           0.002941               0.001572              1   \n",
       "\n",
       "   rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  1                      1        0.752513   \n",
       "\n",
       "   mean_train_F_score  mean_train_Sensitivity  \n",
       "0            0.026421                0.013452  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gbt= gs.cv_results_\n",
    "d = pandas.DataFrame(results_gbt)\n",
    "table_gbt_new = make_table(d)\n",
    "table_gbt_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
