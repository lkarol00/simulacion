{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de muestras:  12330\n",
      "Número de características:  18\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "filename = 'online_shoppers_intention.csv'\n",
    "data = pandas.read_csv(filename, header=0)\n",
    "\n",
    "print(\"Número de muestras: \", data.shape[0])\n",
    "print(\"Número de características: \", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>154.216667</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "5               0                      0.0              0   \n",
       "6               0                      0.0              0   \n",
       "7               1                      0.0              0   \n",
       "8               0                      0.0              0   \n",
       "9               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "5                     0.0              19               154.216667   \n",
       "6                     0.0               1                 0.000000   \n",
       "7                     0.0               0                 0.000000   \n",
       "8                     0.0               2                37.000000   \n",
       "9                     0.0               3               738.000000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0     0.200000   0.200000         0.0         0.0   Feb                 1   \n",
       "1     0.000000   0.100000         0.0         0.0   Feb                 2   \n",
       "2     0.200000   0.200000         0.0         0.0   Feb                 4   \n",
       "3     0.050000   0.140000         0.0         0.0   Feb                 3   \n",
       "4     0.020000   0.050000         0.0         0.0   Feb                 3   \n",
       "5     0.015789   0.024561         0.0         0.0   Feb                 2   \n",
       "6     0.200000   0.200000         0.0         0.4   Feb                 2   \n",
       "7     0.200000   0.200000         0.0         0.0   Feb                 1   \n",
       "8     0.000000   0.100000         0.0         0.8   Feb                 2   \n",
       "9     0.000000   0.022222         0.0         0.4   Feb                 2   \n",
       "\n",
       "   Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0        1       1            1  Returning_Visitor    False    False  \n",
       "1        2       1            2  Returning_Visitor    False    False  \n",
       "2        1       9            3  Returning_Visitor    False    False  \n",
       "3        2       2            4  Returning_Visitor    False    False  \n",
       "4        3       1            4  Returning_Visitor     True    False  \n",
       "5        2       1            3  Returning_Visitor    False    False  \n",
       "6        4       3            3  Returning_Visitor    False    False  \n",
       "7        2       1            5  Returning_Visitor     True    False  \n",
       "8        2       2            3  Returning_Visitor    False    False  \n",
       "9        4       1            2  Returning_Visitor    False    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative               int64\n",
       "Administrative_Duration    float64\n",
       "Informational                int64\n",
       "Informational_Duration     float64\n",
       "ProductRelated               int64\n",
       "ProductRelated_Duration    float64\n",
       "BounceRates                float64\n",
       "ExitRates                  float64\n",
       "PageValues                 float64\n",
       "SpecialDay                 float64\n",
       "Month                       object\n",
       "OperatingSystems             int64\n",
       "Browser                      int64\n",
       "Region                       int64\n",
       "TrafficType                  int64\n",
       "VisitorType                 object\n",
       "Weekend                       bool\n",
       "Revenue                       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    9462\n",
       "True     2868\n",
       "Name: Weekend, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Weekend'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    10422\n",
       "True      1908\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "May     3364\n",
       "Nov     2998\n",
       "Mar     1907\n",
       "Dec     1727\n",
       "Oct      549\n",
       "Sep      448\n",
       "Aug      433\n",
       "Jul      432\n",
       "June     288\n",
       "Feb      184\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Returning_Visitor    10551\n",
       "New_Visitor           1694\n",
       "Other                   85\n",
       "Name: VisitorType, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['VisitorType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative             0\n",
       "Administrative_Duration    0\n",
       "Informational              0\n",
       "Informational_Duration     0\n",
       "ProductRelated             0\n",
       "ProductRelated_Duration    0\n",
       "BounceRates                0\n",
       "ExitRates                  0\n",
       "PageValues                 0\n",
       "SpecialDay                 0\n",
       "Month                      0\n",
       "OperatingSystems           0\n",
       "Browser                    0\n",
       "Region                     0\n",
       "TrafficType                0\n",
       "VisitorType                0\n",
       "Weekend                    0\n",
       "Revenue                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Weekend\"] = data[\"Weekend\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9462\n",
       "1    2868\n",
       "Name: Weekend, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Weekend'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Revenue'] = data[\"Revenue\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10422\n",
       "1     1908\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Revenue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = data['Month'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = np.where(data['Month']==\"Feb\", 0, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Mar\", 1, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"May\", 2, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"June\", 3, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Jul\", 4, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Aug\", 5, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Sep\", 6, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Oct\", 7, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Nov\", 8, data['Month'])\n",
    "data['Month'] = np.where(data['Month']==\"Dec\", 9, data['Month'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3364\n",
       "8    2998\n",
       "1    1907\n",
       "9    1727\n",
       "7     549\n",
       "6     448\n",
       "5     433\n",
       "4     432\n",
       "3     288\n",
       "0     184\n",
       "Name: Month, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VisitorType'] = np.where(data['VisitorType']==\"Returning_Visitor\", 0, data['VisitorType'])\n",
    "data['VisitorType'] = np.where(data['VisitorType']==\"New_Visitor\", 1, data['VisitorType'])\n",
    "data['VisitorType'] = np.where(data['VisitorType']==\"Other\", 2, data['VisitorType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10551\n",
       "1     1694\n",
       "2       85\n",
       "Name: VisitorType, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['VisitorType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>154.216667</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "5               0                      0.0              0   \n",
       "6               0                      0.0              0   \n",
       "7               1                      0.0              0   \n",
       "8               0                      0.0              0   \n",
       "9               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "5                     0.0              19               154.216667   \n",
       "6                     0.0               1                 0.000000   \n",
       "7                     0.0               0                 0.000000   \n",
       "8                     0.0               2                37.000000   \n",
       "9                     0.0               3               738.000000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0     0.200000   0.200000         0.0         0.0     0                 1   \n",
       "1     0.000000   0.100000         0.0         0.0     0                 2   \n",
       "2     0.200000   0.200000         0.0         0.0     0                 4   \n",
       "3     0.050000   0.140000         0.0         0.0     0                 3   \n",
       "4     0.020000   0.050000         0.0         0.0     0                 3   \n",
       "5     0.015789   0.024561         0.0         0.0     0                 2   \n",
       "6     0.200000   0.200000         0.0         0.4     0                 2   \n",
       "7     0.200000   0.200000         0.0         0.0     0                 1   \n",
       "8     0.000000   0.100000         0.0         0.8     0                 2   \n",
       "9     0.000000   0.022222         0.0         0.4     0                 2   \n",
       "\n",
       "   Browser  Region  TrafficType VisitorType  Weekend  Revenue  \n",
       "0        1       1            1           0        0        0  \n",
       "1        2       1            2           0        0        0  \n",
       "2        1       9            3           0        0        0  \n",
       "3        2       2            4           0        0        0  \n",
       "4        3       1            4           0        1        0  \n",
       "5        2       1            3           0        0        0  \n",
       "6        4       3            3           0        0        0  \n",
       "7        2       1            5           0        1        0  \n",
       "8        2       2            3           0        0        0  \n",
       "9        4       1            2           0        0        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = data['Month'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VisitorType'] = data['VisitorType'].astype('str').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative               int64\n",
       "Administrative_Duration    float64\n",
       "Informational                int64\n",
       "Informational_Duration     float64\n",
       "ProductRelated               int64\n",
       "ProductRelated_Duration    float64\n",
       "BounceRates                float64\n",
       "ExitRates                  float64\n",
       "PageValues                 float64\n",
       "SpecialDay                 float64\n",
       "Month                        int32\n",
       "OperatingSystems             int64\n",
       "Browser                      int64\n",
       "Region                       int64\n",
       "TrafficType                  int64\n",
       "VisitorType                  int32\n",
       "Weekend                      int32\n",
       "Revenue                      int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12330, 17) (12330,)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('Revenue', axis=1)\n",
    "Y = data[\"Revenue\"]\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(0,np.sum(Y==0)/Y.shape[0])\n",
    "plt.bar(1,np.sum(Y==1)/Y.shape[0])\n",
    "plt.title('Distribución de clases original')\n",
    "for i in range(2):\n",
    "    plt.text(i, np.sum(Y==i)/Y.shape[0], str(round(np.sum(Y==i)/Y.shape[0],3)), color='black', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculateAUC(clf):\n",
    "    prob_y_0 = clf.predict_proba(X_test)\n",
    "    \n",
    "    prob_y_0 = [p[1] for p in prob_y_0]\n",
    "\n",
    "    print( roc_auc_score(y_test, prob_y_0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8798976997517861\n"
     ]
    }
   ],
   "source": [
    "#calculateAUC(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table(data):\n",
    "    df = pandas.DataFrame()\n",
    "    df = df.assign(mean_fit_time = data[\"mean_fit_time\"])\n",
    "\n",
    "    df = df.assign(params = data[\"params\"])\n",
    "\n",
    "    df = df.assign(mean_test_AUC = data[\"mean_test_AUC\"])\n",
    "    df = df.assign(mean_test_F_score = data[\"mean_test_F-score\"])\n",
    "    df = df.assign(mean_test_Sensitivity = data[\"mean_test_Sensitivity\"])\n",
    "    df = df.assign(rank_test_AUC = data[\"rank_test_AUC\"])\n",
    "    df = df.assign(rank_test_F_score = data[\"rank_test_F-score\"])\n",
    "    df = df.assign(rank_test_Sensitivity = data[\"rank_test_Sensitivity\"])\n",
    "\n",
    "    df = df.assign(mean_train_AUC = data[\"mean_train_AUC\"])\n",
    "    df = df.assign(mean_train_F_score = data[\"mean_train_F-score\"])\n",
    "    df = df.assign(mean_train_Sensitivity = data[\"mean_train_Sensitivity\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tiene como medida de desempeño AUC y el accuracy, sólo puse el accuracy como para ver algo, pero en sí se usa sin el accuracy, sólo con el AUC, esa es la prueba que está después de esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  48 | elapsed: 16.9min remaining:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 17.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('svc',\n",
       "                                        SVC(C=1.0, cache_size=200,\n",
       "                                            class_weight=None, coef0=0.0,\n",
       "                                            decision_function_shape='ovr',\n",
       "                                            degree=3, gamma='auto_deprecated',\n",
       "                                            kernel=...\n",
       "                                            probability=True, random_state=None,\n",
       "                                            shrinking=True, tol=0.001,\n",
       "                                            verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'svc__C': (0.001, 0.01, 1, 10),\n",
       "                         'svc__gamma': ['scale'],\n",
       "                         'svc__kernel': ('linear', 'poly', 'sigmoid')},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'svc__kernel':('linear', 'poly', 'sigmoid'),\n",
    "              'svc__gamma':['scale'],\n",
    "              'svc__C':(0.001, 0.01, 1, 10)}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), SVC(probability = True))\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_svc__C</th>\n",
       "      <th>param_svc__gamma</th>\n",
       "      <th>param_svc__kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.011196</td>\n",
       "      <td>1.434184</td>\n",
       "      <td>1.213589</td>\n",
       "      <td>0.198066</td>\n",
       "      <td>0.001</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.970697</td>\n",
       "      <td>0.839589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253669</td>\n",
       "      <td>0.268344</td>\n",
       "      <td>0.009720</td>\n",
       "      <td>8</td>\n",
       "      <td>0.208945</td>\n",
       "      <td>0.274633</td>\n",
       "      <td>0.334032</td>\n",
       "      <td>0.315164</td>\n",
       "      <td>0.283194</td>\n",
       "      <td>0.047940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.010588</td>\n",
       "      <td>0.406227</td>\n",
       "      <td>1.262536</td>\n",
       "      <td>0.079116</td>\n",
       "      <td>0.001</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.903156</td>\n",
       "      <td>0.851529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029350</td>\n",
       "      <td>0.022011</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>11</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.020615</td>\n",
       "      <td>0.002495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.948824</td>\n",
       "      <td>1.096042</td>\n",
       "      <td>2.343567</td>\n",
       "      <td>0.225114</td>\n",
       "      <td>0.001</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.952383</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.745382</td>\n",
       "      <td>2.080913</td>\n",
       "      <td>1.057702</td>\n",
       "      <td>0.241812</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.966250</td>\n",
       "      <td>0.835507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327044</td>\n",
       "      <td>0.364260</td>\n",
       "      <td>0.027589</td>\n",
       "      <td>6</td>\n",
       "      <td>0.308176</td>\n",
       "      <td>0.375961</td>\n",
       "      <td>0.433263</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.387841</td>\n",
       "      <td>0.051667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.130707</td>\n",
       "      <td>1.595418</td>\n",
       "      <td>1.520653</td>\n",
       "      <td>0.163336</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.885291</td>\n",
       "      <td>0.848628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115304</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>0.025525</td>\n",
       "      <td>10</td>\n",
       "      <td>0.061495</td>\n",
       "      <td>0.077568</td>\n",
       "      <td>0.097834</td>\n",
       "      <td>0.088749</td>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.013555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.219475</td>\n",
       "      <td>0.815531</td>\n",
       "      <td>2.683069</td>\n",
       "      <td>0.207297</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.931649</td>\n",
       "      <td>0.847148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218029</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>9</td>\n",
       "      <td>0.160028</td>\n",
       "      <td>0.214535</td>\n",
       "      <td>0.255066</td>\n",
       "      <td>0.244584</td>\n",
       "      <td>0.218553</td>\n",
       "      <td>0.036919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.269707</td>\n",
       "      <td>22.889794</td>\n",
       "      <td>1.068824</td>\n",
       "      <td>0.191393</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.965817</td>\n",
       "      <td>0.835809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>4</td>\n",
       "      <td>0.322851</td>\n",
       "      <td>0.394829</td>\n",
       "      <td>0.464710</td>\n",
       "      <td>0.452131</td>\n",
       "      <td>0.408630</td>\n",
       "      <td>0.056093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51.326718</td>\n",
       "      <td>2.224108</td>\n",
       "      <td>1.194457</td>\n",
       "      <td>0.209341</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.852108</td>\n",
       "      <td>0.843723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425577</td>\n",
       "      <td>0.346431</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>7</td>\n",
       "      <td>0.330538</td>\n",
       "      <td>0.410203</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.415793</td>\n",
       "      <td>0.055879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.178688</td>\n",
       "      <td>2.981318</td>\n",
       "      <td>1.260144</td>\n",
       "      <td>0.039451</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.625320</td>\n",
       "      <td>0.683358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448637</td>\n",
       "      <td>0.434485</td>\n",
       "      <td>0.021038</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390636</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>0.450734</td>\n",
       "      <td>0.426974</td>\n",
       "      <td>0.024468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>389.273173</td>\n",
       "      <td>124.027194</td>\n",
       "      <td>1.003884</td>\n",
       "      <td>0.213332</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.965871</td>\n",
       "      <td>0.836233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>4</td>\n",
       "      <td>0.322851</td>\n",
       "      <td>0.394829</td>\n",
       "      <td>0.465409</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.408980</td>\n",
       "      <td>0.056404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.258530</td>\n",
       "      <td>11.832461</td>\n",
       "      <td>1.031252</td>\n",
       "      <td>0.064425</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.796524</td>\n",
       "      <td>0.818780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486373</td>\n",
       "      <td>0.410371</td>\n",
       "      <td>0.048453</td>\n",
       "      <td>3</td>\n",
       "      <td>0.445143</td>\n",
       "      <td>0.535290</td>\n",
       "      <td>0.563242</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.532495</td>\n",
       "      <td>0.053570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.129594</td>\n",
       "      <td>2.312611</td>\n",
       "      <td>1.246092</td>\n",
       "      <td>0.111782</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.825978</td>\n",
       "      <td>0.679914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.431863</td>\n",
       "      <td>0.032917</td>\n",
       "      <td>2</td>\n",
       "      <td>0.376660</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.459818</td>\n",
       "      <td>0.452131</td>\n",
       "      <td>0.428022</td>\n",
       "      <td>0.032600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_svc__C  \\\n",
       "0       40.011196      1.434184         1.213589        0.198066        0.001   \n",
       "1       46.010588      0.406227         1.262536        0.079116        0.001   \n",
       "2       48.948824      1.096042         2.343567        0.225114        0.001   \n",
       "3       39.745382      2.080913         1.057702        0.241812         0.01   \n",
       "4       50.130707      1.595418         1.520653        0.163336         0.01   \n",
       "5       51.219475      0.815531         2.683069        0.207297         0.01   \n",
       "6       96.269707     22.889794         1.068824        0.191393            1   \n",
       "7       51.326718      2.224108         1.194457        0.209341            1   \n",
       "8       24.178688      2.981318         1.260144        0.039451            1   \n",
       "9      389.273173    124.027194         1.003884        0.213332           10   \n",
       "10      86.258530     11.832461         1.031252        0.064425           10   \n",
       "11      16.129594      2.312611         1.246092        0.111782           10   \n",
       "\n",
       "   param_svc__gamma param_svc__kernel  \\\n",
       "0             scale            linear   \n",
       "1             scale              poly   \n",
       "2             scale           sigmoid   \n",
       "3             scale            linear   \n",
       "4             scale              poly   \n",
       "5             scale           sigmoid   \n",
       "6             scale            linear   \n",
       "7             scale              poly   \n",
       "8             scale           sigmoid   \n",
       "9             scale            linear   \n",
       "10            scale              poly   \n",
       "11            scale           sigmoid   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...         0.970697   \n",
       "1   {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...         0.903156   \n",
       "2   {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...         0.952383   \n",
       "3   {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...         0.966250   \n",
       "4   {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...         0.885291   \n",
       "5   {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...         0.931649   \n",
       "6   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...         0.965817   \n",
       "7   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...         0.852108   \n",
       "8   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...         0.625320   \n",
       "9   {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...         0.965871   \n",
       "10  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...         0.796524   \n",
       "11  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...         0.825978   \n",
       "\n",
       "    split1_test_AUC  ...  split3_test_Sensitivity  mean_test_Sensitivity  \\\n",
       "0          0.839589  ...                 0.253669               0.268344   \n",
       "1          0.851529  ...                 0.029350               0.022011   \n",
       "2          0.853107  ...                 0.000000               0.000000   \n",
       "3          0.835507  ...                 0.327044               0.364260   \n",
       "4          0.848628  ...                 0.115304               0.074420   \n",
       "5          0.847148  ...                 0.218029               0.219601   \n",
       "6          0.835809  ...                 0.339623               0.391514   \n",
       "7          0.843723  ...                 0.425577               0.346431   \n",
       "8          0.683358  ...                 0.448637               0.434485   \n",
       "9          0.836233  ...                 0.339623               0.391514   \n",
       "10         0.818780  ...                 0.486373               0.410371   \n",
       "11         0.679914  ...                 0.452830               0.431863   \n",
       "\n",
       "    std_test_Sensitivity  rank_test_Sensitivity  split0_train_Sensitivity  \\\n",
       "0               0.009720                      8                  0.208945   \n",
       "1               0.007338                     11                  0.017470   \n",
       "2               0.000000                     12                  0.000000   \n",
       "3               0.027589                      6                  0.308176   \n",
       "4               0.025525                     10                  0.061495   \n",
       "5               0.007756                      9                  0.160028   \n",
       "6               0.033126                      4                  0.322851   \n",
       "7               0.047721                      7                  0.330538   \n",
       "8               0.021038                      1                  0.390636   \n",
       "9               0.033126                      4                  0.322851   \n",
       "10              0.048453                      3                  0.445143   \n",
       "11              0.032917                      2                  0.376660   \n",
       "\n",
       "    split1_train_Sensitivity  split2_train_Sensitivity  \\\n",
       "0                   0.274633                  0.334032   \n",
       "1                   0.020266                  0.024458   \n",
       "2                   0.000000                  0.000000   \n",
       "3                   0.375961                  0.433263   \n",
       "4                   0.077568                  0.097834   \n",
       "5                   0.214535                  0.255066   \n",
       "6                   0.394829                  0.464710   \n",
       "7                   0.410203                  0.438155   \n",
       "8                   0.418588                  0.447939   \n",
       "9                   0.394829                  0.465409   \n",
       "10                  0.535290                  0.563242   \n",
       "11                  0.423480                  0.459818   \n",
       "\n",
       "    split3_train_Sensitivity  mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                   0.315164                0.283194               0.047940  \n",
       "1                   0.020266                0.020615               0.002495  \n",
       "2                   0.000000                0.000000               0.000000  \n",
       "3                   0.433962                0.387841               0.051667  \n",
       "4                   0.088749                0.081412               0.013555  \n",
       "5                   0.244584                0.218553               0.036919  \n",
       "6                   0.452131                0.408630               0.056093  \n",
       "7                   0.484277                0.415793               0.055879  \n",
       "8                   0.450734                0.426974               0.024468  \n",
       "9                   0.452830                0.408980               0.056404  \n",
       "10                  0.586303                0.532495               0.053570  \n",
       "11                  0.452131                0.428022               0.032600  \n",
       "\n",
       "[12 rows x 47 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_svc = gs.cv_results_\n",
    "data = pandas.DataFrame(results_svc)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.011196</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.849134</td>\n",
       "      <td>0.399287</td>\n",
       "      <td>0.268344</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.873948</td>\n",
       "      <td>0.414885</td>\n",
       "      <td>0.283194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.010588</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.839221</td>\n",
       "      <td>0.042729</td>\n",
       "      <td>0.022011</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.040283</td>\n",
       "      <td>0.020615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.948824</td>\n",
       "      <td>{'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...</td>\n",
       "      <td>0.842535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.853601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.745382</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.844399</td>\n",
       "      <td>0.487738</td>\n",
       "      <td>0.364260</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.868839</td>\n",
       "      <td>0.510337</td>\n",
       "      <td>0.387841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.130707</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.832056</td>\n",
       "      <td>0.135121</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.884842</td>\n",
       "      <td>0.149338</td>\n",
       "      <td>0.081412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.219475</td>\n",
       "      <td>{'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...</td>\n",
       "      <td>0.831313</td>\n",
       "      <td>0.342604</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.839334</td>\n",
       "      <td>0.339956</td>\n",
       "      <td>0.218553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.269707</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.844004</td>\n",
       "      <td>0.510040</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.867881</td>\n",
       "      <td>0.525761</td>\n",
       "      <td>0.408630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51.326718</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.812497</td>\n",
       "      <td>0.459162</td>\n",
       "      <td>0.346431</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.906362</td>\n",
       "      <td>0.559482</td>\n",
       "      <td>0.415793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.178688</td>\n",
       "      <td>{'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...</td>\n",
       "      <td>0.644190</td>\n",
       "      <td>0.448139</td>\n",
       "      <td>0.434485</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.642793</td>\n",
       "      <td>0.453644</td>\n",
       "      <td>0.426974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>389.273173</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.843949</td>\n",
       "      <td>0.510040</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.867898</td>\n",
       "      <td>0.526133</td>\n",
       "      <td>0.408980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.258530</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.782387</td>\n",
       "      <td>0.496160</td>\n",
       "      <td>0.410371</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.917496</td>\n",
       "      <td>0.658020</td>\n",
       "      <td>0.532495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.129594</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.688934</td>\n",
       "      <td>0.426242</td>\n",
       "      <td>0.431863</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653024</td>\n",
       "      <td>0.430286</td>\n",
       "      <td>0.428022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0       40.011196  {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...   \n",
       "1       46.010588  {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...   \n",
       "2       48.948824  {'svc__C': 0.001, 'svc__gamma': 'scale', 'svc_...   \n",
       "3       39.745382  {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...   \n",
       "4       50.130707  {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...   \n",
       "5       51.219475  {'svc__C': 0.01, 'svc__gamma': 'scale', 'svc__...   \n",
       "6       96.269707  {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...   \n",
       "7       51.326718  {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...   \n",
       "8       24.178688  {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...   \n",
       "9      389.273173  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "10      86.258530  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "11      16.129594  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.849134           0.399287               0.268344              1   \n",
       "1        0.839221           0.042729               0.022011              6   \n",
       "2        0.842535           0.000000               0.000000              5   \n",
       "3        0.844399           0.487738               0.364260              2   \n",
       "4        0.832056           0.135121               0.074420              7   \n",
       "5        0.831313           0.342604               0.219601              8   \n",
       "6        0.844004           0.510040               0.391514              3   \n",
       "7        0.812497           0.459162               0.346431              9   \n",
       "8        0.644190           0.448139               0.434485             12   \n",
       "9        0.843949           0.510040               0.391514              4   \n",
       "10       0.782387           0.496160               0.410371             10   \n",
       "11       0.688934           0.426242               0.431863             11   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   8                      8        0.873948   \n",
       "1                  11                     11        0.883681   \n",
       "2                  12                     12        0.853601   \n",
       "3                   4                      6        0.868839   \n",
       "4                  10                     10        0.884842   \n",
       "5                   9                      9        0.839334   \n",
       "6                   1                      4        0.867881   \n",
       "7                   5                      7        0.906362   \n",
       "8                   6                      1        0.642793   \n",
       "9                   1                      4        0.867898   \n",
       "10                  3                      3        0.917496   \n",
       "11                  7                      2        0.653024   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.414885                0.283194  \n",
       "1             0.040283                0.020615  \n",
       "2             0.000000                0.000000  \n",
       "3             0.510337                0.387841  \n",
       "4             0.149338                0.081412  \n",
       "5             0.339956                0.218553  \n",
       "6             0.525761                0.408630  \n",
       "7             0.559482                0.415793  \n",
       "8             0.453644                0.426974  \n",
       "9             0.526133                0.408980  \n",
       "10            0.658020                0.532495  \n",
       "11            0.430286                0.428022  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_svc = make_table(data)\n",
    "table_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 1, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 32 candidates, totalling 128 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 128 out of 128 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('gradientboostingclassifier',\n",
       "                                        GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                                   init=None,\n",
       "                                                                   learning_rate=0.1,\n",
       "                                                                   loss='deviance',\n",
       "                                                                   max_...\n",
       "             param_grid={'gradientboostingclassifier__learning_rate': [0.3, 0.2,\n",
       "                                                                       0.1,\n",
       "                                                                       0.01],\n",
       "                         'gradientboostingclassifier__loss': ('deviance',\n",
       "                                                              'exponential'),\n",
       "                         'gradientboostingclassifier__n_estimators': [50, 100,\n",
       "                                                                      200,\n",
       "                                                                      300]},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'gradientboostingclassifier__loss':('deviance', 'exponential'), \n",
    "              'gradientboostingclassifier__learning_rate':([0.3, 0.2, 0.1, 0.01]), \n",
    "              'gradientboostingclassifier__n_estimators':([50, 100, 200, 300]),\n",
    "             }\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), GradientBoostingClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gradientboostingclassifier__learning_rate</th>\n",
       "      <th>param_gradientboostingclassifier__loss</th>\n",
       "      <th>param_gradientboostingclassifier__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018934</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.051197</td>\n",
       "      <td>7.218690e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980061</td>\n",
       "      <td>0.893875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626834</td>\n",
       "      <td>0.522001</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>16</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.711391</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.735150</td>\n",
       "      <td>0.704577</td>\n",
       "      <td>0.044747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.908934</td>\n",
       "      <td>0.034056</td>\n",
       "      <td>0.058595</td>\n",
       "      <td>6.763725e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978085</td>\n",
       "      <td>0.888735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.517809</td>\n",
       "      <td>0.071062</td>\n",
       "      <td>22</td>\n",
       "      <td>0.697414</td>\n",
       "      <td>0.780573</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.763627</td>\n",
       "      <td>0.038407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.236166</td>\n",
       "      <td>0.119706</td>\n",
       "      <td>0.074007</td>\n",
       "      <td>1.589358e-03</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.975819</td>\n",
       "      <td>0.890047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.514666</td>\n",
       "      <td>0.063907</td>\n",
       "      <td>23</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.835779</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.834906</td>\n",
       "      <td>0.037428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.784479</td>\n",
       "      <td>0.107923</td>\n",
       "      <td>0.109123</td>\n",
       "      <td>1.981880e-02</td>\n",
       "      <td>0.3</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.972247</td>\n",
       "      <td>0.890363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.511520</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>25</td>\n",
       "      <td>0.835779</td>\n",
       "      <td>0.891684</td>\n",
       "      <td>0.916841</td>\n",
       "      <td>0.907757</td>\n",
       "      <td>0.888015</td>\n",
       "      <td>0.031475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.942293</td>\n",
       "      <td>0.037418</td>\n",
       "      <td>0.046877</td>\n",
       "      <td>6.344746e-06</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981410</td>\n",
       "      <td>0.920846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.537726</td>\n",
       "      <td>0.063562</td>\n",
       "      <td>3</td>\n",
       "      <td>0.606569</td>\n",
       "      <td>0.685535</td>\n",
       "      <td>0.714885</td>\n",
       "      <td>0.689727</td>\n",
       "      <td>0.674179</td>\n",
       "      <td>0.040616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.797441</td>\n",
       "      <td>0.053532</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>2.030439e-02</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981878</td>\n",
       "      <td>0.919125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597484</td>\n",
       "      <td>0.527771</td>\n",
       "      <td>0.047975</td>\n",
       "      <td>12</td>\n",
       "      <td>0.635919</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.732355</td>\n",
       "      <td>0.712788</td>\n",
       "      <td>0.046351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.030110</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>0.062490</td>\n",
       "      <td>1.030919e-05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981495</td>\n",
       "      <td>0.913181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593291</td>\n",
       "      <td>0.520433</td>\n",
       "      <td>0.048657</td>\n",
       "      <td>19</td>\n",
       "      <td>0.696716</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.819008</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>0.773410</td>\n",
       "      <td>0.046801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.160142</td>\n",
       "      <td>0.111802</td>\n",
       "      <td>0.093746</td>\n",
       "      <td>2.787433e-06</td>\n",
       "      <td>0.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>0.902096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.510474</td>\n",
       "      <td>0.055337</td>\n",
       "      <td>26</td>\n",
       "      <td>0.747030</td>\n",
       "      <td>0.822502</td>\n",
       "      <td>0.865129</td>\n",
       "      <td>0.850454</td>\n",
       "      <td>0.821279</td>\n",
       "      <td>0.045520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.949221</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.054682</td>\n",
       "      <td>1.354142e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.979664</td>\n",
       "      <td>0.895185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.528290</td>\n",
       "      <td>0.072939</td>\n",
       "      <td>11</td>\n",
       "      <td>0.607966</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.725367</td>\n",
       "      <td>0.701607</td>\n",
       "      <td>0.680294</td>\n",
       "      <td>0.044024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.831514</td>\n",
       "      <td>0.033565</td>\n",
       "      <td>0.058592</td>\n",
       "      <td>2.029400e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.979098</td>\n",
       "      <td>0.896087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>0.063265</td>\n",
       "      <td>17</td>\n",
       "      <td>0.655486</td>\n",
       "      <td>0.730259</td>\n",
       "      <td>0.767994</td>\n",
       "      <td>0.741440</td>\n",
       "      <td>0.723795</td>\n",
       "      <td>0.041752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.472657</td>\n",
       "      <td>0.043318</td>\n",
       "      <td>0.066403</td>\n",
       "      <td>6.763003e-03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978551</td>\n",
       "      <td>0.896703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.523576</td>\n",
       "      <td>0.065315</td>\n",
       "      <td>14</td>\n",
       "      <td>0.716282</td>\n",
       "      <td>0.796646</td>\n",
       "      <td>0.823201</td>\n",
       "      <td>0.809224</td>\n",
       "      <td>0.786338</td>\n",
       "      <td>0.041523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.641785</td>\n",
       "      <td>0.118673</td>\n",
       "      <td>0.089843</td>\n",
       "      <td>6.767442e-03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978074</td>\n",
       "      <td>0.894272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.519907</td>\n",
       "      <td>0.064565</td>\n",
       "      <td>21</td>\n",
       "      <td>0.766597</td>\n",
       "      <td>0.832285</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.854647</td>\n",
       "      <td>0.828966</td>\n",
       "      <td>0.037663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.964845</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.046870</td>\n",
       "      <td>1.104779e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980877</td>\n",
       "      <td>0.919508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.533008</td>\n",
       "      <td>0.071561</td>\n",
       "      <td>7</td>\n",
       "      <td>0.591894</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.682041</td>\n",
       "      <td>0.662648</td>\n",
       "      <td>0.041784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.789047</td>\n",
       "      <td>0.056867</td>\n",
       "      <td>0.054681</td>\n",
       "      <td>7.812859e-03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981155</td>\n",
       "      <td>0.918554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633124</td>\n",
       "      <td>0.536677</td>\n",
       "      <td>0.068431</td>\n",
       "      <td>4</td>\n",
       "      <td>0.614256</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.721873</td>\n",
       "      <td>0.704403</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.041749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.590663</td>\n",
       "      <td>0.104142</td>\n",
       "      <td>0.089025</td>\n",
       "      <td>3.004021e-02</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.909749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.525148</td>\n",
       "      <td>0.065251</td>\n",
       "      <td>13</td>\n",
       "      <td>0.662474</td>\n",
       "      <td>0.747030</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.738994</td>\n",
       "      <td>0.046466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.865595</td>\n",
       "      <td>0.084518</td>\n",
       "      <td>0.093749</td>\n",
       "      <td>3.576279e-07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981014</td>\n",
       "      <td>0.905370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614256</td>\n",
       "      <td>0.522527</td>\n",
       "      <td>0.064724</td>\n",
       "      <td>15</td>\n",
       "      <td>0.700210</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.824598</td>\n",
       "      <td>0.803634</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.047091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.010746</td>\n",
       "      <td>0.017026</td>\n",
       "      <td>0.046872</td>\n",
       "      <td>8.066691e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980584</td>\n",
       "      <td>0.898009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654088</td>\n",
       "      <td>0.539296</td>\n",
       "      <td>0.080232</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.662474</td>\n",
       "      <td>0.696017</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.650594</td>\n",
       "      <td>0.045002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.466631</td>\n",
       "      <td>0.041158</td>\n",
       "      <td>0.064034</td>\n",
       "      <td>1.625492e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980691</td>\n",
       "      <td>0.901120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631027</td>\n",
       "      <td>0.534579</td>\n",
       "      <td>0.076037</td>\n",
       "      <td>5</td>\n",
       "      <td>0.607966</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.710692</td>\n",
       "      <td>0.693920</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.039944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.073749</td>\n",
       "      <td>0.107715</td>\n",
       "      <td>0.066402</td>\n",
       "      <td>6.753197e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980072</td>\n",
       "      <td>0.888999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>0.060334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.756813</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.714361</td>\n",
       "      <td>0.042985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.876342</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>0.104995</td>\n",
       "      <td>2.080029e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980126</td>\n",
       "      <td>0.890811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.520432</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>20</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.756115</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.768693</td>\n",
       "      <td>0.749301</td>\n",
       "      <td>0.042583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.144039</td>\n",
       "      <td>0.017098</td>\n",
       "      <td>0.042977</td>\n",
       "      <td>6.761799e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981005</td>\n",
       "      <td>0.922730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.534055</td>\n",
       "      <td>0.079531</td>\n",
       "      <td>6</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.654088</td>\n",
       "      <td>0.680643</td>\n",
       "      <td>0.657582</td>\n",
       "      <td>0.642383</td>\n",
       "      <td>0.038981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.836223</td>\n",
       "      <td>0.027213</td>\n",
       "      <td>0.058596</td>\n",
       "      <td>6.764688e-03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981278</td>\n",
       "      <td>0.919517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>0.076013</td>\n",
       "      <td>2</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.701607</td>\n",
       "      <td>0.679944</td>\n",
       "      <td>0.662648</td>\n",
       "      <td>0.041579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.563814</td>\n",
       "      <td>0.087699</td>\n",
       "      <td>0.078122</td>\n",
       "      <td>2.016012e-06</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981661</td>\n",
       "      <td>0.916248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.529864</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>9</td>\n",
       "      <td>0.615653</td>\n",
       "      <td>0.697414</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.707897</td>\n",
       "      <td>0.688854</td>\n",
       "      <td>0.044366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.525891</td>\n",
       "      <td>0.050021</td>\n",
       "      <td>0.109373</td>\n",
       "      <td>1.913932e-02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981686</td>\n",
       "      <td>0.913524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631027</td>\n",
       "      <td>0.530913</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>8</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.719776</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.733753</td>\n",
       "      <td>0.713312</td>\n",
       "      <td>0.044613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.140621</td>\n",
       "      <td>0.067201</td>\n",
       "      <td>0.050788</td>\n",
       "      <td>6.766239e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978728</td>\n",
       "      <td>0.876318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.957033</td>\n",
       "      <td>0.072777</td>\n",
       "      <td>0.062491</td>\n",
       "      <td>1.105006e-02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980022</td>\n",
       "      <td>0.883821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>0.288263</td>\n",
       "      <td>0.037811</td>\n",
       "      <td>29</td>\n",
       "      <td>0.252271</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.387841</td>\n",
       "      <td>0.352376</td>\n",
       "      <td>0.068272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.874999</td>\n",
       "      <td>0.094398</td>\n",
       "      <td>0.078121</td>\n",
       "      <td>2.227013e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980583</td>\n",
       "      <td>0.906263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607966</td>\n",
       "      <td>0.479030</td>\n",
       "      <td>0.076458</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439553</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.596785</td>\n",
       "      <td>0.623340</td>\n",
       "      <td>0.551712</td>\n",
       "      <td>0.070290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.025265</td>\n",
       "      <td>0.139256</td>\n",
       "      <td>0.109373</td>\n",
       "      <td>3.573794e-06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>deviance</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981053</td>\n",
       "      <td>0.913476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637317</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>0.072187</td>\n",
       "      <td>18</td>\n",
       "      <td>0.539483</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.664570</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.624389</td>\n",
       "      <td>0.051014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.214842</td>\n",
       "      <td>0.090005</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>6.762728e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>50</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.978594</td>\n",
       "      <td>0.875114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.228589</td>\n",
       "      <td>0.120521</td>\n",
       "      <td>0.056160</td>\n",
       "      <td>5.716732e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.979367</td>\n",
       "      <td>0.920586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.077044</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>30</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>0.069881</td>\n",
       "      <td>0.154437</td>\n",
       "      <td>0.101328</td>\n",
       "      <td>0.097834</td>\n",
       "      <td>0.035464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.855266</td>\n",
       "      <td>0.109932</td>\n",
       "      <td>0.070322</td>\n",
       "      <td>7.806734e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.980562</td>\n",
       "      <td>0.928645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.448633</td>\n",
       "      <td>0.066407</td>\n",
       "      <td>28</td>\n",
       "      <td>0.386443</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.506639</td>\n",
       "      <td>0.079327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.734383</td>\n",
       "      <td>0.147604</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>6.766168e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>exponential</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.981024</td>\n",
       "      <td>0.926110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.512571</td>\n",
       "      <td>0.076128</td>\n",
       "      <td>24</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.609364</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.599057</td>\n",
       "      <td>0.053299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        1.018934      0.034350         0.051197    7.218690e-03   \n",
       "1        1.908934      0.034056         0.058595    6.763725e-03   \n",
       "2        4.236166      0.119706         0.074007    1.589358e-03   \n",
       "3        6.784479      0.107923         0.109123    1.981880e-02   \n",
       "4        0.942293      0.037418         0.046877    6.344746e-06   \n",
       "5        1.797441      0.053532         0.058597    2.030439e-02   \n",
       "6        4.030110      0.064890         0.062490    1.030919e-05   \n",
       "7        5.160142      0.111802         0.093746    2.787433e-06   \n",
       "8        0.949221      0.020303         0.054682    1.354142e-02   \n",
       "9        1.831514      0.033565         0.058592    2.029400e-02   \n",
       "10       3.472657      0.043318         0.066403    6.763003e-03   \n",
       "11       5.641785      0.118673         0.089843    6.767442e-03   \n",
       "12       0.964845      0.023105         0.046870    1.104779e-02   \n",
       "13       1.789047      0.056867         0.054681    7.812859e-03   \n",
       "14       3.590663      0.104142         0.089025    3.004021e-02   \n",
       "15       5.865595      0.084518         0.093749    3.576279e-07   \n",
       "16       1.010746      0.017026         0.046872    8.066691e-06   \n",
       "17       2.466631      0.041158         0.064034    1.625492e-02   \n",
       "18       4.073749      0.107715         0.066402    6.753197e-03   \n",
       "19       6.876342      0.161335         0.104995    2.080029e-02   \n",
       "20       1.144039      0.017098         0.042977    6.761799e-03   \n",
       "21       1.836223      0.027213         0.058596    6.764688e-03   \n",
       "22       3.563814      0.087699         0.078122    2.016012e-06   \n",
       "23       5.525891      0.050021         0.109373    1.913932e-02   \n",
       "24       1.140621      0.067201         0.050788    6.766239e-03   \n",
       "25       1.957033      0.072777         0.062491    1.105006e-02   \n",
       "26       3.874999      0.094398         0.078121    2.227013e-06   \n",
       "27       6.025265      0.139256         0.109373    3.573794e-06   \n",
       "28       1.214842      0.090005         0.058594    6.762728e-03   \n",
       "29       2.228589      0.120521         0.056160    5.716732e-03   \n",
       "30       3.855266      0.109932         0.070322    7.806734e-03   \n",
       "31       5.734383      0.147604         0.105469    6.766168e-03   \n",
       "\n",
       "   param_gradientboostingclassifier__learning_rate  \\\n",
       "0                                              0.3   \n",
       "1                                              0.3   \n",
       "2                                              0.3   \n",
       "3                                              0.3   \n",
       "4                                              0.3   \n",
       "5                                              0.3   \n",
       "6                                              0.3   \n",
       "7                                              0.3   \n",
       "8                                              0.2   \n",
       "9                                              0.2   \n",
       "10                                             0.2   \n",
       "11                                             0.2   \n",
       "12                                             0.2   \n",
       "13                                             0.2   \n",
       "14                                             0.2   \n",
       "15                                             0.2   \n",
       "16                                             0.1   \n",
       "17                                             0.1   \n",
       "18                                             0.1   \n",
       "19                                             0.1   \n",
       "20                                             0.1   \n",
       "21                                             0.1   \n",
       "22                                             0.1   \n",
       "23                                             0.1   \n",
       "24                                            0.01   \n",
       "25                                            0.01   \n",
       "26                                            0.01   \n",
       "27                                            0.01   \n",
       "28                                            0.01   \n",
       "29                                            0.01   \n",
       "30                                            0.01   \n",
       "31                                            0.01   \n",
       "\n",
       "   param_gradientboostingclassifier__loss  \\\n",
       "0                                deviance   \n",
       "1                                deviance   \n",
       "2                                deviance   \n",
       "3                                deviance   \n",
       "4                             exponential   \n",
       "5                             exponential   \n",
       "6                             exponential   \n",
       "7                             exponential   \n",
       "8                                deviance   \n",
       "9                                deviance   \n",
       "10                               deviance   \n",
       "11                               deviance   \n",
       "12                            exponential   \n",
       "13                            exponential   \n",
       "14                            exponential   \n",
       "15                            exponential   \n",
       "16                               deviance   \n",
       "17                               deviance   \n",
       "18                               deviance   \n",
       "19                               deviance   \n",
       "20                            exponential   \n",
       "21                            exponential   \n",
       "22                            exponential   \n",
       "23                            exponential   \n",
       "24                               deviance   \n",
       "25                               deviance   \n",
       "26                               deviance   \n",
       "27                               deviance   \n",
       "28                            exponential   \n",
       "29                            exponential   \n",
       "30                            exponential   \n",
       "31                            exponential   \n",
       "\n",
       "   param_gradientboostingclassifier__n_estimators  \\\n",
       "0                                              50   \n",
       "1                                             100   \n",
       "2                                             200   \n",
       "3                                             300   \n",
       "4                                              50   \n",
       "5                                             100   \n",
       "6                                             200   \n",
       "7                                             300   \n",
       "8                                              50   \n",
       "9                                             100   \n",
       "10                                            200   \n",
       "11                                            300   \n",
       "12                                             50   \n",
       "13                                            100   \n",
       "14                                            200   \n",
       "15                                            300   \n",
       "16                                             50   \n",
       "17                                            100   \n",
       "18                                            200   \n",
       "19                                            300   \n",
       "20                                             50   \n",
       "21                                            100   \n",
       "22                                            200   \n",
       "23                                            300   \n",
       "24                                             50   \n",
       "25                                            100   \n",
       "26                                            200   \n",
       "27                                            300   \n",
       "28                                             50   \n",
       "29                                            100   \n",
       "30                                            200   \n",
       "31                                            300   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'gradientboostingclassifier__learning_rate': ...         0.980061   \n",
       "1   {'gradientboostingclassifier__learning_rate': ...         0.978085   \n",
       "2   {'gradientboostingclassifier__learning_rate': ...         0.975819   \n",
       "3   {'gradientboostingclassifier__learning_rate': ...         0.972247   \n",
       "4   {'gradientboostingclassifier__learning_rate': ...         0.981410   \n",
       "5   {'gradientboostingclassifier__learning_rate': ...         0.981878   \n",
       "6   {'gradientboostingclassifier__learning_rate': ...         0.981495   \n",
       "7   {'gradientboostingclassifier__learning_rate': ...         0.980849   \n",
       "8   {'gradientboostingclassifier__learning_rate': ...         0.979664   \n",
       "9   {'gradientboostingclassifier__learning_rate': ...         0.979098   \n",
       "10  {'gradientboostingclassifier__learning_rate': ...         0.978551   \n",
       "11  {'gradientboostingclassifier__learning_rate': ...         0.978074   \n",
       "12  {'gradientboostingclassifier__learning_rate': ...         0.980877   \n",
       "13  {'gradientboostingclassifier__learning_rate': ...         0.981155   \n",
       "14  {'gradientboostingclassifier__learning_rate': ...         0.981408   \n",
       "15  {'gradientboostingclassifier__learning_rate': ...         0.981014   \n",
       "16  {'gradientboostingclassifier__learning_rate': ...         0.980584   \n",
       "17  {'gradientboostingclassifier__learning_rate': ...         0.980691   \n",
       "18  {'gradientboostingclassifier__learning_rate': ...         0.980072   \n",
       "19  {'gradientboostingclassifier__learning_rate': ...         0.980126   \n",
       "20  {'gradientboostingclassifier__learning_rate': ...         0.981005   \n",
       "21  {'gradientboostingclassifier__learning_rate': ...         0.981278   \n",
       "22  {'gradientboostingclassifier__learning_rate': ...         0.981661   \n",
       "23  {'gradientboostingclassifier__learning_rate': ...         0.981686   \n",
       "24  {'gradientboostingclassifier__learning_rate': ...         0.978728   \n",
       "25  {'gradientboostingclassifier__learning_rate': ...         0.980022   \n",
       "26  {'gradientboostingclassifier__learning_rate': ...         0.980583   \n",
       "27  {'gradientboostingclassifier__learning_rate': ...         0.981053   \n",
       "28  {'gradientboostingclassifier__learning_rate': ...         0.978594   \n",
       "29  {'gradientboostingclassifier__learning_rate': ...         0.979367   \n",
       "30  {'gradientboostingclassifier__learning_rate': ...         0.980562   \n",
       "31  {'gradientboostingclassifier__learning_rate': ...         0.981024   \n",
       "\n",
       "    split1_test_AUC  ...  split3_test_Sensitivity  mean_test_Sensitivity  \\\n",
       "0          0.893875  ...                 0.626834               0.522001   \n",
       "1          0.888735  ...                 0.612159               0.517809   \n",
       "2          0.890047  ...                 0.603774               0.514666   \n",
       "3          0.890363  ...                 0.612159               0.511520   \n",
       "4          0.920846  ...                 0.624738               0.537726   \n",
       "5          0.919125  ...                 0.597484               0.527771   \n",
       "6          0.913181  ...                 0.593291               0.520433   \n",
       "7          0.902096  ...                 0.587002               0.510474   \n",
       "8          0.895185  ...                 0.624738               0.528290   \n",
       "9          0.896087  ...                 0.612159               0.521479   \n",
       "10         0.896703  ...                 0.622642               0.523576   \n",
       "11         0.894272  ...                 0.616352               0.519907   \n",
       "12         0.919508  ...                 0.624738               0.533008   \n",
       "13         0.918554  ...                 0.633124               0.536677   \n",
       "14         0.909749  ...                 0.616352               0.525148   \n",
       "15         0.905370  ...                 0.614256               0.522527   \n",
       "16         0.898009  ...                 0.654088               0.539296   \n",
       "17         0.901120  ...                 0.631027               0.534579   \n",
       "18         0.888999  ...                 0.612159               0.529341   \n",
       "19         0.890811  ...                 0.605870               0.520432   \n",
       "20         0.922730  ...                 0.645702               0.534055   \n",
       "21         0.919517  ...                 0.641509               0.538248   \n",
       "22         0.916248  ...                 0.622642               0.529864   \n",
       "23         0.913524  ...                 0.631027               0.530913   \n",
       "24         0.876318  ...                 0.000000               0.000000   \n",
       "25         0.883821  ...                 0.266247               0.288263   \n",
       "26         0.906263  ...                 0.607966               0.479030   \n",
       "27         0.913476  ...                 0.637317               0.521479   \n",
       "28         0.875114  ...                 0.000000               0.000000   \n",
       "29         0.920586  ...                 0.064990               0.077044   \n",
       "30         0.928645  ...                 0.555556               0.448633   \n",
       "31         0.926110  ...                 0.643606               0.512571   \n",
       "\n",
       "    std_test_Sensitivity  rank_test_Sensitivity  split0_train_Sensitivity  \\\n",
       "0               0.080801                     16                  0.629630   \n",
       "1               0.071062                     22                  0.697414   \n",
       "2               0.063907                     23                  0.773585   \n",
       "3               0.071401                     25                  0.835779   \n",
       "4               0.063562                      3                  0.606569   \n",
       "5               0.047975                     12                  0.635919   \n",
       "6               0.048657                     19                  0.696716   \n",
       "7               0.055337                     26                  0.747030   \n",
       "8               0.072939                     11                  0.607966   \n",
       "9               0.063265                     17                  0.655486   \n",
       "10              0.065315                     14                  0.716282   \n",
       "11              0.064565                     21                  0.766597   \n",
       "12              0.071561                      7                  0.591894   \n",
       "13              0.068431                      4                  0.614256   \n",
       "14              0.065251                     13                  0.662474   \n",
       "15              0.064724                     15                  0.700210   \n",
       "16              0.080232                      1                  0.575821   \n",
       "17              0.076037                      5                  0.607966   \n",
       "18              0.060334                     10                  0.642907   \n",
       "19              0.057093                     20                  0.679245   \n",
       "20              0.079531                      6                  0.577219   \n",
       "21              0.076013                      2                  0.592593   \n",
       "22              0.065332                      9                  0.615653   \n",
       "23              0.065332                      8                  0.640112   \n",
       "24              0.000000                     31                  0.000000   \n",
       "25              0.037811                     29                  0.252271   \n",
       "26              0.076458                     27                  0.439553   \n",
       "27              0.072187                     18                  0.539483   \n",
       "28              0.000000                     31                  0.000000   \n",
       "29              0.009182                     30                  0.065688   \n",
       "30              0.066407                     28                  0.386443   \n",
       "31              0.076128                     24                  0.509434   \n",
       "\n",
       "    split1_train_Sensitivity  split2_train_Sensitivity  \\\n",
       "0                   0.711391                  0.742138   \n",
       "1                   0.780573                  0.791055   \n",
       "2                   0.835779                  0.867925   \n",
       "3                   0.891684                  0.916841   \n",
       "4                   0.685535                  0.714885   \n",
       "5                   0.723270                  0.759609   \n",
       "6                   0.776380                  0.819008   \n",
       "7                   0.822502                  0.865129   \n",
       "8                   0.686233                  0.725367   \n",
       "9                   0.730259                  0.767994   \n",
       "10                  0.796646                  0.823201   \n",
       "11                  0.832285                  0.862334   \n",
       "12                  0.676450                  0.700210   \n",
       "13                  0.700210                  0.721873   \n",
       "14                  0.747030                  0.786862   \n",
       "15                  0.777079                  0.824598   \n",
       "16                  0.662474                  0.696017   \n",
       "17                  0.691125                  0.710692   \n",
       "18                  0.723270                  0.756813   \n",
       "19                  0.756115                  0.793152   \n",
       "20                  0.654088                  0.680643   \n",
       "21                  0.676450                  0.701607   \n",
       "22                  0.697414                  0.734451   \n",
       "23                  0.719776                  0.759609   \n",
       "24                  0.000000                  0.000000   \n",
       "25                  0.333333                  0.436059   \n",
       "26                  0.547170                  0.596785   \n",
       "27                  0.629630                  0.664570   \n",
       "28                  0.000000                  0.000000   \n",
       "29                  0.069881                  0.154437   \n",
       "30                  0.484277                  0.572327   \n",
       "31                  0.609364                  0.632425   \n",
       "\n",
       "    split3_train_Sensitivity  mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                   0.735150                0.704577               0.044747  \n",
       "1                   0.785465                0.763627               0.038407  \n",
       "2                   0.862334                0.834906               0.037428  \n",
       "3                   0.907757                0.888015               0.031475  \n",
       "4                   0.689727                0.674179               0.040616  \n",
       "5                   0.732355                0.712788               0.046351  \n",
       "6                   0.801537                0.773410               0.046801  \n",
       "7                   0.850454                0.821279               0.045520  \n",
       "8                   0.701607                0.680294               0.044024  \n",
       "9                   0.741440                0.723795               0.041752  \n",
       "10                  0.809224                0.786338               0.041523  \n",
       "11                  0.854647                0.828966               0.037663  \n",
       "12                  0.682041                0.662648               0.041784  \n",
       "13                  0.704403                0.685185               0.041749  \n",
       "14                  0.759609                0.738994               0.046466  \n",
       "15                  0.803634                0.776380               0.047091  \n",
       "16                  0.668064                0.650594               0.045002  \n",
       "17                  0.693920                0.675926               0.039944  \n",
       "18                  0.734451                0.714361               0.042985  \n",
       "19                  0.768693                0.749301               0.042583  \n",
       "20                  0.657582                0.642383               0.038981  \n",
       "21                  0.679944                0.662648               0.041579  \n",
       "22                  0.707897                0.688854               0.044366  \n",
       "23                  0.733753                0.713312               0.044613  \n",
       "24                  0.000000                0.000000               0.000000  \n",
       "25                  0.387841                0.352376               0.068272  \n",
       "26                  0.623340                0.551712               0.070290  \n",
       "27                  0.663871                0.624389               0.051014  \n",
       "28                  0.000000                0.000000               0.000000  \n",
       "29                  0.101328                0.097834               0.035464  \n",
       "30                  0.583508                0.506639               0.079327  \n",
       "31                  0.645003                0.599057               0.053299  \n",
       "\n",
       "[32 rows x 47 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gbt = gs.cv_results_\n",
    "data = pandas.DataFrame(results_gbt)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018934</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.872946</td>\n",
       "      <td>0.544303</td>\n",
       "      <td>0.522001</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>0.957320</td>\n",
       "      <td>0.758080</td>\n",
       "      <td>0.704577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.908934</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.870756</td>\n",
       "      <td>0.540353</td>\n",
       "      <td>0.517809</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969131</td>\n",
       "      <td>0.815212</td>\n",
       "      <td>0.763627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.236166</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.868728</td>\n",
       "      <td>0.534473</td>\n",
       "      <td>0.514666</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>0.983424</td>\n",
       "      <td>0.880818</td>\n",
       "      <td>0.834906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.784479</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.865349</td>\n",
       "      <td>0.528994</td>\n",
       "      <td>0.511520</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>0.990853</td>\n",
       "      <td>0.926336</td>\n",
       "      <td>0.888015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.942293</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888167</td>\n",
       "      <td>0.564358</td>\n",
       "      <td>0.537726</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.957226</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.674179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.797441</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.884061</td>\n",
       "      <td>0.556206</td>\n",
       "      <td>0.527771</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.968761</td>\n",
       "      <td>0.769530</td>\n",
       "      <td>0.712788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.030110</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878462</td>\n",
       "      <td>0.546526</td>\n",
       "      <td>0.520433</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>0.982480</td>\n",
       "      <td>0.827539</td>\n",
       "      <td>0.773410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.160142</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.873521</td>\n",
       "      <td>0.537298</td>\n",
       "      <td>0.510474</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>0.990113</td>\n",
       "      <td>0.872008</td>\n",
       "      <td>0.821279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.949221</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.876137</td>\n",
       "      <td>0.554978</td>\n",
       "      <td>0.528290</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0.952377</td>\n",
       "      <td>0.734608</td>\n",
       "      <td>0.680294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.831514</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.875376</td>\n",
       "      <td>0.548371</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>0.962328</td>\n",
       "      <td>0.778154</td>\n",
       "      <td>0.723795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.472657</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.873858</td>\n",
       "      <td>0.549199</td>\n",
       "      <td>0.523576</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>0.975941</td>\n",
       "      <td>0.837674</td>\n",
       "      <td>0.786338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.641785</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.543803</td>\n",
       "      <td>0.519907</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>0.984011</td>\n",
       "      <td>0.876723</td>\n",
       "      <td>0.828966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.964845</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888308</td>\n",
       "      <td>0.560044</td>\n",
       "      <td>0.533008</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.952037</td>\n",
       "      <td>0.716416</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.789047</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888604</td>\n",
       "      <td>0.564260</td>\n",
       "      <td>0.536677</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.961848</td>\n",
       "      <td>0.742433</td>\n",
       "      <td>0.685185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.590663</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.882644</td>\n",
       "      <td>0.553299</td>\n",
       "      <td>0.525148</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0.974691</td>\n",
       "      <td>0.794916</td>\n",
       "      <td>0.738994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.865595</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878666</td>\n",
       "      <td>0.548506</td>\n",
       "      <td>0.522527</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0.982918</td>\n",
       "      <td>0.832414</td>\n",
       "      <td>0.776380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.010746</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.879321</td>\n",
       "      <td>0.563918</td>\n",
       "      <td>0.539296</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945533</td>\n",
       "      <td>0.706109</td>\n",
       "      <td>0.650594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.466631</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878823</td>\n",
       "      <td>0.558759</td>\n",
       "      <td>0.534579</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.952546</td>\n",
       "      <td>0.731256</td>\n",
       "      <td>0.675926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.073749</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.874902</td>\n",
       "      <td>0.554668</td>\n",
       "      <td>0.529341</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962220</td>\n",
       "      <td>0.771219</td>\n",
       "      <td>0.714361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.876342</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.874559</td>\n",
       "      <td>0.547532</td>\n",
       "      <td>0.520432</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969629</td>\n",
       "      <td>0.805357</td>\n",
       "      <td>0.749301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.144039</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.887300</td>\n",
       "      <td>0.564191</td>\n",
       "      <td>0.534055</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.945660</td>\n",
       "      <td>0.700368</td>\n",
       "      <td>0.642383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.836223</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888507</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.952666</td>\n",
       "      <td>0.715934</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.563814</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.887299</td>\n",
       "      <td>0.558405</td>\n",
       "      <td>0.529864</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.688854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.525891</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.884334</td>\n",
       "      <td>0.555391</td>\n",
       "      <td>0.530913</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.969423</td>\n",
       "      <td>0.771530</td>\n",
       "      <td>0.713312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.140621</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.860386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0.925091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.957033</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.865128</td>\n",
       "      <td>0.423044</td>\n",
       "      <td>0.288263</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.931197</td>\n",
       "      <td>0.499943</td>\n",
       "      <td>0.352376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.874999</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.878733</td>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.479030</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>0.650173</td>\n",
       "      <td>0.551712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.025265</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.883566</td>\n",
       "      <td>0.558328</td>\n",
       "      <td>0.521479</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.939973</td>\n",
       "      <td>0.691046</td>\n",
       "      <td>0.624389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.214842</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.863905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0.928807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.228589</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.876798</td>\n",
       "      <td>0.141746</td>\n",
       "      <td>0.077044</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.932455</td>\n",
       "      <td>0.175823</td>\n",
       "      <td>0.097834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.855266</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.884999</td>\n",
       "      <td>0.533070</td>\n",
       "      <td>0.448633</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>0.937284</td>\n",
       "      <td>0.621062</td>\n",
       "      <td>0.506639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.734383</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.886453</td>\n",
       "      <td>0.561638</td>\n",
       "      <td>0.512571</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>0.940853</td>\n",
       "      <td>0.676696</td>\n",
       "      <td>0.599057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        1.018934  {'gradientboostingclassifier__learning_rate': ...   \n",
       "1        1.908934  {'gradientboostingclassifier__learning_rate': ...   \n",
       "2        4.236166  {'gradientboostingclassifier__learning_rate': ...   \n",
       "3        6.784479  {'gradientboostingclassifier__learning_rate': ...   \n",
       "4        0.942293  {'gradientboostingclassifier__learning_rate': ...   \n",
       "5        1.797441  {'gradientboostingclassifier__learning_rate': ...   \n",
       "6        4.030110  {'gradientboostingclassifier__learning_rate': ...   \n",
       "7        5.160142  {'gradientboostingclassifier__learning_rate': ...   \n",
       "8        0.949221  {'gradientboostingclassifier__learning_rate': ...   \n",
       "9        1.831514  {'gradientboostingclassifier__learning_rate': ...   \n",
       "10       3.472657  {'gradientboostingclassifier__learning_rate': ...   \n",
       "11       5.641785  {'gradientboostingclassifier__learning_rate': ...   \n",
       "12       0.964845  {'gradientboostingclassifier__learning_rate': ...   \n",
       "13       1.789047  {'gradientboostingclassifier__learning_rate': ...   \n",
       "14       3.590663  {'gradientboostingclassifier__learning_rate': ...   \n",
       "15       5.865595  {'gradientboostingclassifier__learning_rate': ...   \n",
       "16       1.010746  {'gradientboostingclassifier__learning_rate': ...   \n",
       "17       2.466631  {'gradientboostingclassifier__learning_rate': ...   \n",
       "18       4.073749  {'gradientboostingclassifier__learning_rate': ...   \n",
       "19       6.876342  {'gradientboostingclassifier__learning_rate': ...   \n",
       "20       1.144039  {'gradientboostingclassifier__learning_rate': ...   \n",
       "21       1.836223  {'gradientboostingclassifier__learning_rate': ...   \n",
       "22       3.563814  {'gradientboostingclassifier__learning_rate': ...   \n",
       "23       5.525891  {'gradientboostingclassifier__learning_rate': ...   \n",
       "24       1.140621  {'gradientboostingclassifier__learning_rate': ...   \n",
       "25       1.957033  {'gradientboostingclassifier__learning_rate': ...   \n",
       "26       3.874999  {'gradientboostingclassifier__learning_rate': ...   \n",
       "27       6.025265  {'gradientboostingclassifier__learning_rate': ...   \n",
       "28       1.214842  {'gradientboostingclassifier__learning_rate': ...   \n",
       "29       2.228589  {'gradientboostingclassifier__learning_rate': ...   \n",
       "30       3.855266  {'gradientboostingclassifier__learning_rate': ...   \n",
       "31       5.734383  {'gradientboostingclassifier__learning_rate': ...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.872946           0.544303               0.522001             25   \n",
       "1        0.870756           0.540353               0.517809             27   \n",
       "2        0.868728           0.534473               0.514666             28   \n",
       "3        0.865349           0.528994               0.511520             29   \n",
       "4        0.888167           0.564358               0.537726              4   \n",
       "5        0.884061           0.556206               0.527771             10   \n",
       "6        0.878462           0.546526               0.520433             17   \n",
       "7        0.873521           0.537298               0.510474             24   \n",
       "8        0.876137           0.554978               0.528290             19   \n",
       "9        0.875376           0.548371               0.521479             20   \n",
       "10       0.873858           0.549199               0.523576             23   \n",
       "11       0.871421           0.543803               0.519907             26   \n",
       "12       0.888308           0.560044               0.533008              3   \n",
       "13       0.888604           0.564260               0.536677              1   \n",
       "14       0.882644           0.553299               0.525148             12   \n",
       "15       0.878666           0.548506               0.522527             16   \n",
       "16       0.879321           0.563918               0.539296             13   \n",
       "17       0.878823           0.558759               0.534579             14   \n",
       "18       0.874902           0.554668               0.529341             21   \n",
       "19       0.874559           0.547532               0.520432             22   \n",
       "20       0.887300           0.564191               0.534055              5   \n",
       "21       0.888507           0.564609               0.538248              2   \n",
       "22       0.887299           0.558405               0.529864              6   \n",
       "23       0.884334           0.555391               0.530913              9   \n",
       "24       0.860386           0.000000               0.000000             32   \n",
       "25       0.865128           0.423044               0.288263             30   \n",
       "26       0.878733           0.542255               0.479030             15   \n",
       "27       0.883566           0.558328               0.521479             11   \n",
       "28       0.863905           0.000000               0.000000             31   \n",
       "29       0.876798           0.141746               0.077044             18   \n",
       "30       0.884999           0.533070               0.448633              8   \n",
       "31       0.886453           0.561638               0.512571              7   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  21                     16        0.957320   \n",
       "1                  24                     22        0.969131   \n",
       "2                  26                     23        0.983424   \n",
       "3                  28                     25        0.990853   \n",
       "4                   2                      3        0.957226   \n",
       "5                  11                     12        0.968761   \n",
       "6                  20                     19        0.982480   \n",
       "7                  25                     26        0.990113   \n",
       "8                  13                     11        0.952377   \n",
       "9                  18                     17        0.962328   \n",
       "10                 16                     14        0.975941   \n",
       "11                 22                     21        0.984011   \n",
       "12                  7                      7        0.952037   \n",
       "13                  3                      4        0.961848   \n",
       "14                 15                     13        0.974691   \n",
       "15                 17                     15        0.982918   \n",
       "16                  5                      1        0.945533   \n",
       "17                  8                      5        0.952546   \n",
       "18                 14                     10        0.962220   \n",
       "19                 19                     20        0.969629   \n",
       "20                  4                      6        0.945660   \n",
       "21                  1                      2        0.952666   \n",
       "22                  9                      9        0.962348   \n",
       "23                 12                      8        0.969423   \n",
       "24                 31                     31        0.925091   \n",
       "25                 29                     29        0.931197   \n",
       "26                 23                     27        0.936753   \n",
       "27                 10                     18        0.939973   \n",
       "28                 31                     31        0.928807   \n",
       "29                 30                     30        0.932455   \n",
       "30                 27                     28        0.937284   \n",
       "31                  6                     24        0.940853   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.758080                0.704577  \n",
       "1             0.815212                0.763627  \n",
       "2             0.880818                0.834906  \n",
       "3             0.926336                0.888015  \n",
       "4             0.726937                0.674179  \n",
       "5             0.769530                0.712788  \n",
       "6             0.827539                0.773410  \n",
       "7             0.872008                0.821279  \n",
       "8             0.734608                0.680294  \n",
       "9             0.778154                0.723795  \n",
       "10            0.837674                0.786338  \n",
       "11            0.876723                0.828966  \n",
       "12            0.716416                0.662648  \n",
       "13            0.742433                0.685185  \n",
       "14            0.794916                0.738994  \n",
       "15            0.832414                0.776380  \n",
       "16            0.706109                0.650594  \n",
       "17            0.731256                0.675926  \n",
       "18            0.771219                0.714361  \n",
       "19            0.805357                0.749301  \n",
       "20            0.700368                0.642383  \n",
       "21            0.715934                0.662648  \n",
       "22            0.745064                0.688854  \n",
       "23            0.771530                0.713312  \n",
       "24            0.000000                0.000000  \n",
       "25            0.499943                0.352376  \n",
       "26            0.650173                0.551712  \n",
       "27            0.691046                0.624389  \n",
       "28            0.000000                0.000000  \n",
       "29            0.175823                0.097834  \n",
       "30            0.621062                0.506639  \n",
       "31            0.676696                0.599057  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_gbt = make_table(data)\n",
    "table_gbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradientboostingclassifier__learning_rate': 0.1,\n",
       " 'gradientboostingclassifier__loss': 'exponential',\n",
       " 'gradientboostingclassifier__n_estimators': 100}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  48 | elapsed:    4.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('quadraticdiscriminantanalysis',\n",
       "                                        QuadraticDiscriminantAnalysis(priors=None,\n",
       "                                                                      reg_param=0.0,\n",
       "                                                                      store_covariance=False,\n",
       "                                                                      tol=0.0001))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'quadraticdiscriminantanalysis__reg_param': [0, 0.001,\n",
       "                                                                      0.01, 0.1,\n",
       "                                                                      0.6, 1],\n",
       "                         'quadraticdiscriminantanalysis__store_covariance': (True,\n",
       "                                                                             False)},\n",
       "             pre_dispatch='2*n_jobs', refit='F-score', return_train_score=True,\n",
       "             scoring={'AUC': 'roc_auc', 'F-score': 'f1',\n",
       "                      'Sensitivity': make_scorer(recall_score)},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {'quadraticdiscriminantanalysis__reg_param':([0, 0.001, 0.01, 0.1, 0.6, 1]), \n",
    "              'quadraticdiscriminantanalysis__store_covariance':(True,False)}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), QuadraticDiscriminantAnalysis()) \n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_quadraticdiscriminantanalysis__reg_param</th>\n",
       "      <th>param_quadraticdiscriminantanalysis__store_covariance</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.094181</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.082813</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848531</td>\n",
       "      <td>0.828068</td>\n",
       "      <td>0.800665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>0.115727</td>\n",
       "      <td>1</td>\n",
       "      <td>0.644305</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.631901</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074236</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>0.058588</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848531</td>\n",
       "      <td>0.828068</td>\n",
       "      <td>0.800665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>0.115727</td>\n",
       "      <td>1</td>\n",
       "      <td>0.644305</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.631901</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108742</td>\n",
       "      <td>0.015026</td>\n",
       "      <td>0.083789</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>0.829036</td>\n",
       "      <td>0.800740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740042</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>0.115393</td>\n",
       "      <td>3</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.589797</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.650594</td>\n",
       "      <td>0.630503</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117974</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.086623</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>0.829036</td>\n",
       "      <td>0.800740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740042</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>0.115393</td>\n",
       "      <td>3</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.589797</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.650594</td>\n",
       "      <td>0.630503</td>\n",
       "      <td>0.023885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158316</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.023579</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.834768</td>\n",
       "      <td>0.801243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>5</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.580014</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.620720</td>\n",
       "      <td>0.023762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.143254</td>\n",
       "      <td>0.022813</td>\n",
       "      <td>0.080048</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.848644</td>\n",
       "      <td>0.834768</td>\n",
       "      <td>0.801243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708595</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>5</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.580014</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.638714</td>\n",
       "      <td>0.620720</td>\n",
       "      <td>0.023762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097351</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.054680</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.850243</td>\n",
       "      <td>0.846531</td>\n",
       "      <td>0.801359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.095190</td>\n",
       "      <td>7</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.531796</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.580713</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.021127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092275</td>\n",
       "      <td>0.022673</td>\n",
       "      <td>0.064995</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.850243</td>\n",
       "      <td>0.846531</td>\n",
       "      <td>0.801359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.095190</td>\n",
       "      <td>7</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.531796</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.580713</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.021127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.097658</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.046885</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.873458</td>\n",
       "      <td>0.851085</td>\n",
       "      <td>0.805555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574423</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>0.102468</td>\n",
       "      <td>9</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.461915</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.448812</td>\n",
       "      <td>0.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.098202</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.067987</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.873458</td>\n",
       "      <td>0.851085</td>\n",
       "      <td>0.805555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574423</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>0.102468</td>\n",
       "      <td>9</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.461915</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.448812</td>\n",
       "      <td>0.013830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.103969</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.077572</td>\n",
       "      <td>0.017845</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>0.131903</td>\n",
       "      <td>11</td>\n",
       "      <td>0.381551</td>\n",
       "      <td>0.403215</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.410727</td>\n",
       "      <td>0.020891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.093759</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>0.131903</td>\n",
       "      <td>11</td>\n",
       "      <td>0.381551</td>\n",
       "      <td>0.403215</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.410727</td>\n",
       "      <td>0.020891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.094181      0.007692         0.082813        0.011657   \n",
       "1        0.074236      0.006767         0.058588        0.006745   \n",
       "2        0.108742      0.015026         0.083789        0.018004   \n",
       "3        0.117974      0.017152         0.086623        0.008798   \n",
       "4        0.158316      0.009123         0.139785        0.023579   \n",
       "5        0.143254      0.022813         0.080048        0.007533   \n",
       "6        0.097351      0.015519         0.054680        0.007826   \n",
       "7        0.092275      0.022673         0.064995        0.010410   \n",
       "8        0.097658      0.006776         0.046885        0.000008   \n",
       "9        0.098202      0.013419         0.067987        0.005269   \n",
       "10       0.103969      0.004126         0.077572        0.017845   \n",
       "11       0.093759      0.000021         0.070200        0.007679   \n",
       "\n",
       "   param_quadraticdiscriminantanalysis__reg_param  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                           0.001   \n",
       "3                                           0.001   \n",
       "4                                            0.01   \n",
       "5                                            0.01   \n",
       "6                                             0.1   \n",
       "7                                             0.1   \n",
       "8                                             0.6   \n",
       "9                                             0.6   \n",
       "10                                              1   \n",
       "11                                              1   \n",
       "\n",
       "   param_quadraticdiscriminantanalysis__store_covariance  \\\n",
       "0                                                True      \n",
       "1                                               False      \n",
       "2                                                True      \n",
       "3                                               False      \n",
       "4                                                True      \n",
       "5                                               False      \n",
       "6                                                True      \n",
       "7                                               False      \n",
       "8                                                True      \n",
       "9                                               False      \n",
       "10                                               True      \n",
       "11                                              False      \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848531   \n",
       "1   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848531   \n",
       "2   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848548   \n",
       "3   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848548   \n",
       "4   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848644   \n",
       "5   {'quadraticdiscriminantanalysis__reg_param': 0...         0.848644   \n",
       "6   {'quadraticdiscriminantanalysis__reg_param': 0...         0.850243   \n",
       "7   {'quadraticdiscriminantanalysis__reg_param': 0...         0.850243   \n",
       "8   {'quadraticdiscriminantanalysis__reg_param': 0...         0.873458   \n",
       "9   {'quadraticdiscriminantanalysis__reg_param': 0...         0.873458   \n",
       "10  {'quadraticdiscriminantanalysis__reg_param': 1...         0.919129   \n",
       "11  {'quadraticdiscriminantanalysis__reg_param': 1...         0.919129   \n",
       "\n",
       "    split1_test_AUC  split2_test_AUC  ...  split3_test_Sensitivity  \\\n",
       "0          0.828068         0.800665  ...                 0.742138   \n",
       "1          0.828068         0.800665  ...                 0.742138   \n",
       "2          0.829036         0.800740  ...                 0.740042   \n",
       "3          0.829036         0.800740  ...                 0.740042   \n",
       "4          0.834768         0.801243  ...                 0.708595   \n",
       "5          0.834768         0.801243  ...                 0.708595   \n",
       "6          0.846531         0.801359  ...                 0.645702   \n",
       "7          0.846531         0.801359  ...                 0.645702   \n",
       "8          0.851085         0.805555  ...                 0.574423   \n",
       "9          0.851085         0.805555  ...                 0.574423   \n",
       "10         0.845140         0.815510  ...                 0.572327   \n",
       "11         0.845140         0.815510  ...                 0.572327   \n",
       "\n",
       "    mean_test_Sensitivity  std_test_Sensitivity  rank_test_Sensitivity  \\\n",
       "0                0.592226              0.115727                      1   \n",
       "1                0.592226              0.115727                      1   \n",
       "2                0.591701              0.115393                      3   \n",
       "3                0.591701              0.115393                      3   \n",
       "4                0.581745              0.106326                      5   \n",
       "5                0.581745              0.106326                      5   \n",
       "6                0.521474              0.095190                      7   \n",
       "7                0.521474              0.095190                      7   \n",
       "8                0.438139              0.102468                      9   \n",
       "9                0.438139              0.102468                      9   \n",
       "10               0.397778              0.131903                     11   \n",
       "11               0.397778              0.131903                     11   \n",
       "\n",
       "    split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                   0.644305                  0.591195   \n",
       "1                   0.644305                  0.591195   \n",
       "2                   0.642907                  0.589797   \n",
       "3                   0.642907                  0.589797   \n",
       "4                   0.635220                  0.580014   \n",
       "5                   0.635220                  0.580014   \n",
       "6                   0.558351                  0.531796   \n",
       "7                   0.558351                  0.531796   \n",
       "8                   0.433962                  0.436059   \n",
       "9                   0.433962                  0.436059   \n",
       "10                  0.381551                  0.403215   \n",
       "11                  0.381551                  0.403215   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.640112                  0.651992   \n",
       "1                   0.640112                  0.651992   \n",
       "2                   0.638714                  0.650594   \n",
       "3                   0.638714                  0.650594   \n",
       "4                   0.628931                  0.638714   \n",
       "5                   0.628931                  0.638714   \n",
       "6                   0.584906                  0.580713   \n",
       "7                   0.584906                  0.580713   \n",
       "8                   0.461915                  0.463312   \n",
       "9                   0.461915                  0.463312   \n",
       "10                  0.438155                  0.419986   \n",
       "11                  0.438155                  0.419986   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.631901               0.023885  \n",
       "1                 0.631901               0.023885  \n",
       "2                 0.630503               0.023885  \n",
       "3                 0.630503               0.023885  \n",
       "4                 0.620720               0.023762  \n",
       "5                 0.620720               0.023762  \n",
       "6                 0.563941               0.021127  \n",
       "7                 0.563941               0.021127  \n",
       "8                 0.448812               0.013830  \n",
       "9                 0.448812               0.013830  \n",
       "10                0.410727               0.020891  \n",
       "11                0.410727               0.020891  \n",
       "\n",
       "[12 rows x 46 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_qda= gs.cv_results_\n",
    "data = pandas.DataFrame(results_qda)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.094181</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820525</td>\n",
       "      <td>0.504882</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845410</td>\n",
       "      <td>0.541456</td>\n",
       "      <td>0.631901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074236</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820525</td>\n",
       "      <td>0.504882</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845410</td>\n",
       "      <td>0.541456</td>\n",
       "      <td>0.631901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108742</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.505032</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.541363</td>\n",
       "      <td>0.630503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117974</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.820785</td>\n",
       "      <td>0.505032</td>\n",
       "      <td>0.591701</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.845649</td>\n",
       "      <td>0.541363</td>\n",
       "      <td>0.630503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158316</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>0.505637</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.542564</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.143254</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>0.505637</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.542564</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097351</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.846881</td>\n",
       "      <td>0.537770</td>\n",
       "      <td>0.563941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.092275</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.824929</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.846881</td>\n",
       "      <td>0.537770</td>\n",
       "      <td>0.563941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.097658</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.834557</td>\n",
       "      <td>0.463247</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.847155</td>\n",
       "      <td>0.488946</td>\n",
       "      <td>0.448812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.098202</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.834557</td>\n",
       "      <td>0.463247</td>\n",
       "      <td>0.438139</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.847155</td>\n",
       "      <td>0.488946</td>\n",
       "      <td>0.448812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.103969</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.434559</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.470089</td>\n",
       "      <td>0.410727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.093759</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 1...</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.434559</td>\n",
       "      <td>0.397778</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.470089</td>\n",
       "      <td>0.410727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        0.094181  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "1        0.074236  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "2        0.108742  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "3        0.117974  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "4        0.158316  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "5        0.143254  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "6        0.097351  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "7        0.092275  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "8        0.097658  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "9        0.098202  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "10       0.103969  {'quadraticdiscriminantanalysis__reg_param': 1...   \n",
       "11       0.093759  {'quadraticdiscriminantanalysis__reg_param': 1...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.820525           0.504882               0.592226             11   \n",
       "1        0.820525           0.504882               0.592226             11   \n",
       "2        0.820785           0.505032               0.591701              9   \n",
       "3        0.820785           0.505032               0.591701              9   \n",
       "4        0.822152           0.505637               0.581745              7   \n",
       "5        0.822152           0.505637               0.581745              7   \n",
       "6        0.824929           0.494459               0.521474              5   \n",
       "7        0.824929           0.494459               0.521474              5   \n",
       "8        0.834557           0.463247               0.438139              3   \n",
       "9        0.834557           0.463247               0.438139              3   \n",
       "10       0.851237           0.434559               0.397778              1   \n",
       "11       0.851237           0.434559               0.397778              1   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   5                      1        0.845410   \n",
       "1                   5                      1        0.845410   \n",
       "2                   3                      3        0.845649   \n",
       "3                   3                      3        0.845649   \n",
       "4                   1                      5        0.846864   \n",
       "5                   1                      5        0.846864   \n",
       "6                   7                      7        0.846881   \n",
       "7                   7                      7        0.846881   \n",
       "8                   9                      9        0.847155   \n",
       "9                   9                      9        0.847155   \n",
       "10                 11                     11        0.855633   \n",
       "11                 11                     11        0.855633   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.541456                0.631901  \n",
       "1             0.541456                0.631901  \n",
       "2             0.541363                0.630503  \n",
       "3             0.541363                0.630503  \n",
       "4             0.542564                0.620720  \n",
       "5             0.542564                0.620720  \n",
       "6             0.537770                0.563941  \n",
       "7             0.537770                0.563941  \n",
       "8             0.488946                0.448812  \n",
       "9             0.488946                0.448812  \n",
       "10            0.470089                0.410727  \n",
       "11            0.470089                0.410727  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_qda = make_table(data)\n",
    "table_qda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quadraticdiscriminantanalysis__reg_param': 0.01,\n",
       " 'quadraticdiscriminantanalysis__store_covariance': True}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class KDEClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Bayesian generative classification based on KDE\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bandwidth : float\n",
    "        the kernel bandwidth within each class\n",
    "    kernel : str\n",
    "        the kernel name, passed to KernelDensity\n",
    "    algorithm : str\n",
    "        the algorithm name, passed to KernelDensity\n",
    "    \"\"\"\n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian', algorithm = 'kd_tree'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.sort(np.unique(y))\n",
    "        training_sets = [X[y == yi] for yi in self.classes_]\n",
    "        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n",
    "                                      kernel=self.kernel, \n",
    "                                      algorithm=self.algorithm).fit(Xi)\n",
    "                        for Xi in training_sets]\n",
    "        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n",
    "                           for Xi in training_sets]\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        logprobs = np.array([model.score_samples(X)\n",
    "                             for model in self.models_]).T\n",
    "        result = np.exp(logprobs + self.logpriors_)\n",
    "        return result / result.sum(1, keepdims=True)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.29154967,  1.66810054,  2.15443469,  2.7825594 ,\n",
       "        3.59381366,  4.64158883,  5.9948425 ,  7.74263683, 10.        ])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 10 ** np.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed: 22.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 22.2min finished\n"
     ]
    }
   ],
   "source": [
    "bandwidths = 10 ** np.linspace(0, 1, 10)\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)}\n",
    "parameters = {'kdeclassifier__bandwidth': bandwidths}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), KDEClassifier()) \n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kdeclassifier__bandwidth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>split3_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123743</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>31.727459</td>\n",
       "      <td>0.226179</td>\n",
       "      <td>1</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.0}</td>\n",
       "      <td>0.894678</td>\n",
       "      <td>0.830510</td>\n",
       "      <td>0.790376</td>\n",
       "      <td>0.774951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274633</td>\n",
       "      <td>0.223790</td>\n",
       "      <td>0.031737</td>\n",
       "      <td>1</td>\n",
       "      <td>0.366876</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.441649</td>\n",
       "      <td>0.414046</td>\n",
       "      <td>0.030487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123992</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>34.466054</td>\n",
       "      <td>0.213743</td>\n",
       "      <td>1.29155</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.2915496650148839}</td>\n",
       "      <td>0.891654</td>\n",
       "      <td>0.831096</td>\n",
       "      <td>0.801189</td>\n",
       "      <td>0.795670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174004</td>\n",
       "      <td>0.145176</td>\n",
       "      <td>0.017563</td>\n",
       "      <td>2</td>\n",
       "      <td>0.212439</td>\n",
       "      <td>0.220126</td>\n",
       "      <td>0.259958</td>\n",
       "      <td>0.248777</td>\n",
       "      <td>0.235325</td>\n",
       "      <td>0.019638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131473</td>\n",
       "      <td>0.017763</td>\n",
       "      <td>37.283573</td>\n",
       "      <td>0.258985</td>\n",
       "      <td>1.6681</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.6681005372000588}</td>\n",
       "      <td>0.893810</td>\n",
       "      <td>0.833773</td>\n",
       "      <td>0.801525</td>\n",
       "      <td>0.805612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092243</td>\n",
       "      <td>0.075995</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103424</td>\n",
       "      <td>0.109713</td>\n",
       "      <td>0.132075</td>\n",
       "      <td>0.124389</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.011387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114167</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>32.555808</td>\n",
       "      <td>0.126987</td>\n",
       "      <td>2.15443</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.154434690031884}</td>\n",
       "      <td>0.901172</td>\n",
       "      <td>0.840102</td>\n",
       "      <td>0.805615</td>\n",
       "      <td>0.811767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>0.034590</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>4</td>\n",
       "      <td>0.040531</td>\n",
       "      <td>0.042628</td>\n",
       "      <td>0.053809</td>\n",
       "      <td>0.049616</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>0.005331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126461</td>\n",
       "      <td>0.019461</td>\n",
       "      <td>31.802794</td>\n",
       "      <td>0.207150</td>\n",
       "      <td>2.78256</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.7825594022071245}</td>\n",
       "      <td>0.907466</td>\n",
       "      <td>0.847083</td>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.818250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.020266</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.020790</td>\n",
       "      <td>0.002489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.125399</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>32.305125</td>\n",
       "      <td>0.182488</td>\n",
       "      <td>3.59381</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 3.5938136638046276}</td>\n",
       "      <td>0.912042</td>\n",
       "      <td>0.849152</td>\n",
       "      <td>0.813108</td>\n",
       "      <td>0.821159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.121087</td>\n",
       "      <td>0.012961</td>\n",
       "      <td>31.558014</td>\n",
       "      <td>0.112034</td>\n",
       "      <td>4.64159</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 4.641588833612778}</td>\n",
       "      <td>0.914257</td>\n",
       "      <td>0.848604</td>\n",
       "      <td>0.813908</td>\n",
       "      <td>0.821646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.117178</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>31.163359</td>\n",
       "      <td>0.218295</td>\n",
       "      <td>5.99484</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 5.994842503189409}</td>\n",
       "      <td>0.915414</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.814584</td>\n",
       "      <td>0.822821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.109367</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>31.348517</td>\n",
       "      <td>0.172236</td>\n",
       "      <td>7.74264</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 7.742636826811269}</td>\n",
       "      <td>0.916126</td>\n",
       "      <td>0.846189</td>\n",
       "      <td>0.814964</td>\n",
       "      <td>0.823610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.115231</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>31.293514</td>\n",
       "      <td>0.130393</td>\n",
       "      <td>10</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 10.0}</td>\n",
       "      <td>0.916743</td>\n",
       "      <td>0.845432</td>\n",
       "      <td>0.815206</td>\n",
       "      <td>0.824008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.123743      0.008164        31.727459        0.226179   \n",
       "1       0.123992      0.004529        34.466054        0.213743   \n",
       "2       0.131473      0.017763        37.283573        0.258985   \n",
       "3       0.114167      0.012134        32.555808        0.126987   \n",
       "4       0.126461      0.019461        31.802794        0.207150   \n",
       "5       0.125399      0.010837        32.305125        0.182488   \n",
       "6       0.121087      0.012961        31.558014        0.112034   \n",
       "7       0.117178      0.013531        31.163359        0.218295   \n",
       "8       0.109367      0.000006        31.348517        0.172236   \n",
       "9       0.115231      0.004605        31.293514        0.130393   \n",
       "\n",
       "  param_kdeclassifier__bandwidth  \\\n",
       "0                              1   \n",
       "1                        1.29155   \n",
       "2                         1.6681   \n",
       "3                        2.15443   \n",
       "4                        2.78256   \n",
       "5                        3.59381   \n",
       "6                        4.64159   \n",
       "7                        5.99484   \n",
       "8                        7.74264   \n",
       "9                             10   \n",
       "\n",
       "                                             params  split0_test_AUC  \\\n",
       "0                 {'kdeclassifier__bandwidth': 1.0}         0.894678   \n",
       "1  {'kdeclassifier__bandwidth': 1.2915496650148839}         0.891654   \n",
       "2  {'kdeclassifier__bandwidth': 1.6681005372000588}         0.893810   \n",
       "3   {'kdeclassifier__bandwidth': 2.154434690031884}         0.901172   \n",
       "4  {'kdeclassifier__bandwidth': 2.7825594022071245}         0.907466   \n",
       "5  {'kdeclassifier__bandwidth': 3.5938136638046276}         0.912042   \n",
       "6   {'kdeclassifier__bandwidth': 4.641588833612778}         0.914257   \n",
       "7   {'kdeclassifier__bandwidth': 5.994842503189409}         0.915414   \n",
       "8   {'kdeclassifier__bandwidth': 7.742636826811269}         0.916126   \n",
       "9                {'kdeclassifier__bandwidth': 10.0}         0.916743   \n",
       "\n",
       "   split1_test_AUC  split2_test_AUC  split3_test_AUC  ...  \\\n",
       "0         0.830510         0.790376         0.774951  ...   \n",
       "1         0.831096         0.801189         0.795670  ...   \n",
       "2         0.833773         0.801525         0.805612  ...   \n",
       "3         0.840102         0.805615         0.811767  ...   \n",
       "4         0.847083         0.810753         0.818250  ...   \n",
       "5         0.849152         0.813108         0.821159  ...   \n",
       "6         0.848604         0.813908         0.821646  ...   \n",
       "7         0.847477         0.814584         0.822821  ...   \n",
       "8         0.846189         0.814964         0.823610  ...   \n",
       "9         0.845432         0.815206         0.824008  ...   \n",
       "\n",
       "   split3_test_Sensitivity  mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                 0.274633               0.223790              0.031737   \n",
       "1                 0.174004               0.145176              0.017563   \n",
       "2                 0.092243               0.075995              0.009419   \n",
       "3                 0.046122               0.034590              0.008956   \n",
       "4                 0.020964               0.016247              0.004775   \n",
       "5                 0.008386               0.007337              0.003779   \n",
       "6                 0.002096               0.002096              0.002568   \n",
       "7                 0.000000               0.000000              0.000000   \n",
       "8                 0.000000               0.000000              0.000000   \n",
       "9                 0.000000               0.000000              0.000000   \n",
       "\n",
       "   rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                      1                  0.366876                  0.407407   \n",
       "1                      2                  0.212439                  0.220126   \n",
       "2                      3                  0.103424                  0.109713   \n",
       "3                      4                  0.040531                  0.042628   \n",
       "4                      5                  0.020266                  0.020964   \n",
       "5                      6                  0.006289                  0.007687   \n",
       "6                      7                  0.001398                  0.001398   \n",
       "7                      8                  0.000000                  0.000000   \n",
       "8                      8                  0.000000                  0.000000   \n",
       "9                      8                  0.000000                  0.000000   \n",
       "\n",
       "   split2_train_Sensitivity  split3_train_Sensitivity  mean_train_Sensitivity  \\\n",
       "0                  0.440252                  0.441649                0.414046   \n",
       "1                  0.259958                  0.248777                0.235325   \n",
       "2                  0.132075                  0.124389                0.117400   \n",
       "3                  0.053809                  0.049616                0.046646   \n",
       "4                  0.024458                  0.017470                0.020790   \n",
       "5                  0.008386                  0.007687                0.007512   \n",
       "6                  0.000699                  0.001398                0.001223   \n",
       "7                  0.000000                  0.000000                0.000000   \n",
       "8                  0.000000                  0.000000                0.000000   \n",
       "9                  0.000000                  0.000000                0.000000   \n",
       "\n",
       "   std_train_Sensitivity  \n",
       "0               0.030487  \n",
       "1               0.019638  \n",
       "2               0.011387  \n",
       "3               0.005331  \n",
       "4               0.002489  \n",
       "5               0.000762  \n",
       "6               0.000303  \n",
       "7               0.000000  \n",
       "8               0.000000  \n",
       "9               0.000000  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_kd= gs.cv_results_\n",
    "data = pandas.DataFrame(results_kd)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123743</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.0}</td>\n",
       "      <td>0.822635</td>\n",
       "      <td>0.333549</td>\n",
       "      <td>0.223790</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943239</td>\n",
       "      <td>0.578753</td>\n",
       "      <td>0.414046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123992</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.2915496650148839}</td>\n",
       "      <td>0.829907</td>\n",
       "      <td>0.240523</td>\n",
       "      <td>0.145176</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.906338</td>\n",
       "      <td>0.376716</td>\n",
       "      <td>0.235325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131473</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.6681005372000588}</td>\n",
       "      <td>0.833685</td>\n",
       "      <td>0.138044</td>\n",
       "      <td>0.075995</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.878227</td>\n",
       "      <td>0.209060</td>\n",
       "      <td>0.117400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.114167</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.154434690031884}</td>\n",
       "      <td>0.839669</td>\n",
       "      <td>0.065942</td>\n",
       "      <td>0.034590</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.864297</td>\n",
       "      <td>0.088813</td>\n",
       "      <td>0.046646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126461</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 2.7825594022071245}</td>\n",
       "      <td>0.845893</td>\n",
       "      <td>0.031702</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859112</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>0.020790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.125399</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 3.5938136638046276}</td>\n",
       "      <td>0.848870</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.856430</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.007512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.121087</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 4.641588833612778}</td>\n",
       "      <td>0.849609</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855848</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.001223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.117178</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 5.994842503189409}</td>\n",
       "      <td>0.850079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.855598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.109367</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 7.742636826811269}</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.855355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.115231</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 10.0}</td>\n",
       "      <td>0.850353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.855183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                            params  \\\n",
       "0       0.123743                 {'kdeclassifier__bandwidth': 1.0}   \n",
       "1       0.123992  {'kdeclassifier__bandwidth': 1.2915496650148839}   \n",
       "2       0.131473  {'kdeclassifier__bandwidth': 1.6681005372000588}   \n",
       "3       0.114167   {'kdeclassifier__bandwidth': 2.154434690031884}   \n",
       "4       0.126461  {'kdeclassifier__bandwidth': 2.7825594022071245}   \n",
       "5       0.125399  {'kdeclassifier__bandwidth': 3.5938136638046276}   \n",
       "6       0.121087   {'kdeclassifier__bandwidth': 4.641588833612778}   \n",
       "7       0.117178   {'kdeclassifier__bandwidth': 5.994842503189409}   \n",
       "8       0.109367   {'kdeclassifier__bandwidth': 7.742636826811269}   \n",
       "9       0.115231                {'kdeclassifier__bandwidth': 10.0}   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0       0.822635           0.333549               0.223790             10   \n",
       "1       0.829907           0.240523               0.145176              9   \n",
       "2       0.833685           0.138044               0.075995              8   \n",
       "3       0.839669           0.065942               0.034590              7   \n",
       "4       0.845893           0.031702               0.016247              6   \n",
       "5       0.848870           0.014461               0.007337              5   \n",
       "6       0.849609           0.004157               0.002096              4   \n",
       "7       0.850079           0.000000               0.000000              3   \n",
       "8       0.850227           0.000000               0.000000              2   \n",
       "9       0.850353           0.000000               0.000000              1   \n",
       "\n",
       "   rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  1                      1        0.943239   \n",
       "1                  2                      2        0.906338   \n",
       "2                  3                      3        0.878227   \n",
       "3                  4                      4        0.864297   \n",
       "4                  5                      5        0.859112   \n",
       "5                  6                      6        0.856430   \n",
       "6                  7                      7        0.855848   \n",
       "7                  8                      8        0.855598   \n",
       "8                  8                      8        0.855355   \n",
       "9                  8                      8        0.855183   \n",
       "\n",
       "   mean_train_F_score  mean_train_Sensitivity  \n",
       "0            0.578753                0.414046  \n",
       "1            0.376716                0.235325  \n",
       "2            0.209060                0.117400  \n",
       "3            0.088813                0.046646  \n",
       "4            0.040661                0.020790  \n",
       "5            0.014896                0.007512  \n",
       "6            0.002443                0.001223  \n",
       "7            0.000000                0.000000  \n",
       "8            0.000000                0.000000  \n",
       "9            0.000000                0.000000  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_kd = make_table(data)\n",
    "table_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-df2c2bd0f67e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed: 20.3min finished\n",
      "C:\\Users\\Karol\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [20, 30, 40],\n",
    "    'mlpclassifier__max_iter': [250],\n",
    "    'mlpclassifier__activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'mlpclassifier__solver': ('sgd', 'adam'),\n",
    "    'mlpclassifier__alpha': (0.001, 0.01, 0.1),\n",
    "    'mlpclassifier__learning_rate': ['adaptive'], \n",
    "    'mlpclassifier__random_state':[0]\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), MLPClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='AUC', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_mlpclassifier__activation</th>\n",
       "      <th>param_mlpclassifier__alpha</th>\n",
       "      <th>param_mlpclassifier__hidden_layer_sizes</th>\n",
       "      <th>param_mlpclassifier__learning_rate</th>\n",
       "      <th>param_mlpclassifier__max_iter</th>\n",
       "      <th>param_mlpclassifier__random_state</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.775499</td>\n",
       "      <td>0.222299</td>\n",
       "      <td>0.047252</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>66</td>\n",
       "      <td>0.310273</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>0.414396</td>\n",
       "      <td>0.376660</td>\n",
       "      <td>0.045452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.694757</td>\n",
       "      <td>0.123489</td>\n",
       "      <td>0.047745</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>59</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.361286</td>\n",
       "      <td>0.430468</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.383124</td>\n",
       "      <td>0.044629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.782754</td>\n",
       "      <td>0.446546</td>\n",
       "      <td>0.064001</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>69</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.415793</td>\n",
       "      <td>0.375087</td>\n",
       "      <td>0.043715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856503</td>\n",
       "      <td>0.087326</td>\n",
       "      <td>0.061756</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>0.052788</td>\n",
       "      <td>54</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.422082</td>\n",
       "      <td>0.377184</td>\n",
       "      <td>0.047741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.251008</td>\n",
       "      <td>0.229054</td>\n",
       "      <td>0.061996</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>62</td>\n",
       "      <td>0.311670</td>\n",
       "      <td>0.360587</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.412998</td>\n",
       "      <td>0.376136</td>\n",
       "      <td>0.043642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.101501</td>\n",
       "      <td>0.385897</td>\n",
       "      <td>0.075764</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>0.049553</td>\n",
       "      <td>57</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.356394</td>\n",
       "      <td>0.426275</td>\n",
       "      <td>0.412299</td>\n",
       "      <td>0.375611</td>\n",
       "      <td>0.047234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.861001</td>\n",
       "      <td>0.176050</td>\n",
       "      <td>0.063767</td>\n",
       "      <td>0.010816</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>66</td>\n",
       "      <td>0.310273</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>0.413697</td>\n",
       "      <td>0.376485</td>\n",
       "      <td>0.045308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.975233</td>\n",
       "      <td>0.134533</td>\n",
       "      <td>0.048255</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.363201</td>\n",
       "      <td>0.046226</td>\n",
       "      <td>60</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.361286</td>\n",
       "      <td>0.430468</td>\n",
       "      <td>0.419986</td>\n",
       "      <td>0.383124</td>\n",
       "      <td>0.044629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.924519</td>\n",
       "      <td>0.413967</td>\n",
       "      <td>0.071993</td>\n",
       "      <td>0.015847</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>69</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.415793</td>\n",
       "      <td>0.375087</td>\n",
       "      <td>0.043715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.564246</td>\n",
       "      <td>0.133171</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.017327</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>0.052788</td>\n",
       "      <td>54</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.422082</td>\n",
       "      <td>0.377184</td>\n",
       "      <td>0.047741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.837503</td>\n",
       "      <td>0.084054</td>\n",
       "      <td>0.071249</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>62</td>\n",
       "      <td>0.311670</td>\n",
       "      <td>0.360587</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>0.412998</td>\n",
       "      <td>0.375961</td>\n",
       "      <td>0.043470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.969244</td>\n",
       "      <td>0.273480</td>\n",
       "      <td>0.069498</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423480</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>0.049553</td>\n",
       "      <td>57</td>\n",
       "      <td>0.307477</td>\n",
       "      <td>0.356394</td>\n",
       "      <td>0.426275</td>\n",
       "      <td>0.412299</td>\n",
       "      <td>0.375611</td>\n",
       "      <td>0.047234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.398499</td>\n",
       "      <td>0.184817</td>\n",
       "      <td>0.065744</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.357960</td>\n",
       "      <td>0.047515</td>\n",
       "      <td>68</td>\n",
       "      <td>0.308875</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.412299</td>\n",
       "      <td>0.375087</td>\n",
       "      <td>0.045065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.132252</td>\n",
       "      <td>1.405434</td>\n",
       "      <td>0.064004</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402516</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>0.045968</td>\n",
       "      <td>72</td>\n",
       "      <td>0.314465</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.422781</td>\n",
       "      <td>0.406709</td>\n",
       "      <td>0.375786</td>\n",
       "      <td>0.042428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.191003</td>\n",
       "      <td>0.613985</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.024681</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.355864</td>\n",
       "      <td>0.046605</td>\n",
       "      <td>71</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.357792</td>\n",
       "      <td>0.414396</td>\n",
       "      <td>0.414396</td>\n",
       "      <td>0.374389</td>\n",
       "      <td>0.043296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.965756</td>\n",
       "      <td>1.215305</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>0.046012</td>\n",
       "      <td>56</td>\n",
       "      <td>0.315863</td>\n",
       "      <td>0.352201</td>\n",
       "      <td>0.443047</td>\n",
       "      <td>0.410203</td>\n",
       "      <td>0.380328</td>\n",
       "      <td>0.049429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.888509</td>\n",
       "      <td>0.429072</td>\n",
       "      <td>0.085996</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>62</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.360587</td>\n",
       "      <td>0.417890</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.375262</td>\n",
       "      <td>0.043261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.239740</td>\n",
       "      <td>1.764236</td>\n",
       "      <td>0.081007</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425577</td>\n",
       "      <td>0.360057</td>\n",
       "      <td>0.047930</td>\n",
       "      <td>65</td>\n",
       "      <td>0.321454</td>\n",
       "      <td>0.354997</td>\n",
       "      <td>0.417191</td>\n",
       "      <td>0.421384</td>\n",
       "      <td>0.378756</td>\n",
       "      <td>0.042256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.984505</td>\n",
       "      <td>0.274071</td>\n",
       "      <td>0.056747</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>0.047167</td>\n",
       "      <td>46</td>\n",
       "      <td>0.340321</td>\n",
       "      <td>0.395528</td>\n",
       "      <td>0.460517</td>\n",
       "      <td>0.456324</td>\n",
       "      <td>0.413173</td>\n",
       "      <td>0.049301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.233255</td>\n",
       "      <td>0.537467</td>\n",
       "      <td>0.054246</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.491612</td>\n",
       "      <td>0.073686</td>\n",
       "      <td>13</td>\n",
       "      <td>0.501048</td>\n",
       "      <td>0.570929</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.579839</td>\n",
       "      <td>0.051885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.456001</td>\n",
       "      <td>0.431638</td>\n",
       "      <td>0.062255</td>\n",
       "      <td>0.011095</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438155</td>\n",
       "      <td>0.385213</td>\n",
       "      <td>0.049995</td>\n",
       "      <td>48</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>0.393431</td>\n",
       "      <td>0.453529</td>\n",
       "      <td>0.442348</td>\n",
       "      <td>0.404437</td>\n",
       "      <td>0.049355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.227499</td>\n",
       "      <td>0.372447</td>\n",
       "      <td>0.058501</td>\n",
       "      <td>0.004384</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.084813</td>\n",
       "      <td>6</td>\n",
       "      <td>0.502446</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.621244</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.588924</td>\n",
       "      <td>0.055211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20.316752</td>\n",
       "      <td>1.935356</td>\n",
       "      <td>0.086254</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.369490</td>\n",
       "      <td>0.049620</td>\n",
       "      <td>52</td>\n",
       "      <td>0.316562</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.432565</td>\n",
       "      <td>0.431866</td>\n",
       "      <td>0.389762</td>\n",
       "      <td>0.047697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.478493</td>\n",
       "      <td>1.871172</td>\n",
       "      <td>0.094001</td>\n",
       "      <td>0.034710</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.506809</td>\n",
       "      <td>0.069997</td>\n",
       "      <td>3</td>\n",
       "      <td>0.505940</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.642907</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.593117</td>\n",
       "      <td>0.055995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.342246</td>\n",
       "      <td>2.135672</td>\n",
       "      <td>0.057502</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.389930</td>\n",
       "      <td>0.046241</td>\n",
       "      <td>47</td>\n",
       "      <td>0.340321</td>\n",
       "      <td>0.393431</td>\n",
       "      <td>0.455625</td>\n",
       "      <td>0.456324</td>\n",
       "      <td>0.411426</td>\n",
       "      <td>0.048345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.477503</td>\n",
       "      <td>0.972069</td>\n",
       "      <td>0.070746</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.487417</td>\n",
       "      <td>0.069858</td>\n",
       "      <td>19</td>\n",
       "      <td>0.493361</td>\n",
       "      <td>0.563242</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.633823</td>\n",
       "      <td>0.573375</td>\n",
       "      <td>0.052538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17.921009</td>\n",
       "      <td>1.186882</td>\n",
       "      <td>0.057752</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.384689</td>\n",
       "      <td>0.049445</td>\n",
       "      <td>49</td>\n",
       "      <td>0.327743</td>\n",
       "      <td>0.392732</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.442348</td>\n",
       "      <td>0.403913</td>\n",
       "      <td>0.049490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.937748</td>\n",
       "      <td>0.400661</td>\n",
       "      <td>0.058747</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.496326</td>\n",
       "      <td>0.076708</td>\n",
       "      <td>10</td>\n",
       "      <td>0.491265</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.610063</td>\n",
       "      <td>0.630328</td>\n",
       "      <td>0.575996</td>\n",
       "      <td>0.053163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.978260</td>\n",
       "      <td>1.704479</td>\n",
       "      <td>0.069743</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>0.049412</td>\n",
       "      <td>53</td>\n",
       "      <td>0.316562</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.431866</td>\n",
       "      <td>0.429071</td>\n",
       "      <td>0.388714</td>\n",
       "      <td>0.046972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.667002</td>\n",
       "      <td>0.400816</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.502616</td>\n",
       "      <td>0.066421</td>\n",
       "      <td>4</td>\n",
       "      <td>0.498952</td>\n",
       "      <td>0.573725</td>\n",
       "      <td>0.625437</td>\n",
       "      <td>0.635919</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.823998</td>\n",
       "      <td>0.305129</td>\n",
       "      <td>0.047753</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>23</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.512928</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.519567</td>\n",
       "      <td>0.052781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>18.064747</td>\n",
       "      <td>0.987442</td>\n",
       "      <td>0.074006</td>\n",
       "      <td>0.025545</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.496325</td>\n",
       "      <td>0.050061</td>\n",
       "      <td>11</td>\n",
       "      <td>0.530398</td>\n",
       "      <td>0.589099</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>0.667365</td>\n",
       "      <td>0.614430</td>\n",
       "      <td>0.058501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20.441744</td>\n",
       "      <td>0.859471</td>\n",
       "      <td>0.058257</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.469598</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>35</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.484976</td>\n",
       "      <td>0.526205</td>\n",
       "      <td>0.545772</td>\n",
       "      <td>0.493012</td>\n",
       "      <td>0.050053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.879757</td>\n",
       "      <td>1.127441</td>\n",
       "      <td>0.063248</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526205</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>0.048503</td>\n",
       "      <td>31</td>\n",
       "      <td>0.560447</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.633997</td>\n",
       "      <td>0.047850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>24.770258</td>\n",
       "      <td>1.138280</td>\n",
       "      <td>0.070745</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.460164</td>\n",
       "      <td>0.041042</td>\n",
       "      <td>41</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>0.482879</td>\n",
       "      <td>0.524109</td>\n",
       "      <td>0.530398</td>\n",
       "      <td>0.487247</td>\n",
       "      <td>0.047335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>26.123751</td>\n",
       "      <td>2.587408</td>\n",
       "      <td>0.064745</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.491079</td>\n",
       "      <td>0.075942</td>\n",
       "      <td>15</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.613557</td>\n",
       "      <td>0.725367</td>\n",
       "      <td>0.709993</td>\n",
       "      <td>0.651468</td>\n",
       "      <td>0.069384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.667006</td>\n",
       "      <td>0.450582</td>\n",
       "      <td>0.045999</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.479555</td>\n",
       "      <td>0.039982</td>\n",
       "      <td>28</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.516771</td>\n",
       "      <td>0.054061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15.789512</td>\n",
       "      <td>1.152794</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570231</td>\n",
       "      <td>0.487938</td>\n",
       "      <td>0.066087</td>\n",
       "      <td>18</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.636618</td>\n",
       "      <td>0.653389</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.055496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19.630752</td>\n",
       "      <td>0.814912</td>\n",
       "      <td>0.056755</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.461735</td>\n",
       "      <td>0.046794</td>\n",
       "      <td>38</td>\n",
       "      <td>0.403913</td>\n",
       "      <td>0.482879</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.540182</td>\n",
       "      <td>0.487247</td>\n",
       "      <td>0.052379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>19.751008</td>\n",
       "      <td>0.129881</td>\n",
       "      <td>0.063997</td>\n",
       "      <td>0.014076</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532495</td>\n",
       "      <td>0.487942</td>\n",
       "      <td>0.044901</td>\n",
       "      <td>17</td>\n",
       "      <td>0.535290</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.640112</td>\n",
       "      <td>0.636618</td>\n",
       "      <td>0.599755</td>\n",
       "      <td>0.042737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20.885504</td>\n",
       "      <td>0.414476</td>\n",
       "      <td>0.064501</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.459115</td>\n",
       "      <td>0.039165</td>\n",
       "      <td>42</td>\n",
       "      <td>0.404612</td>\n",
       "      <td>0.480084</td>\n",
       "      <td>0.520615</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>0.482704</td>\n",
       "      <td>0.048411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.926750</td>\n",
       "      <td>0.421111</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.016843</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.498944</td>\n",
       "      <td>0.059279</td>\n",
       "      <td>8</td>\n",
       "      <td>0.524109</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.682739</td>\n",
       "      <td>0.675751</td>\n",
       "      <td>0.614605</td>\n",
       "      <td>0.067221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15.441752</td>\n",
       "      <td>0.355514</td>\n",
       "      <td>0.054748</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>44</td>\n",
       "      <td>0.427673</td>\n",
       "      <td>0.473795</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.494584</td>\n",
       "      <td>0.049793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>19.335506</td>\n",
       "      <td>0.384764</td>\n",
       "      <td>0.052495</td>\n",
       "      <td>0.004930</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551363</td>\n",
       "      <td>0.482175</td>\n",
       "      <td>0.057986</td>\n",
       "      <td>26</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.669462</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.616702</td>\n",
       "      <td>0.056713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20.795001</td>\n",
       "      <td>0.801262</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.051083</td>\n",
       "      <td>36</td>\n",
       "      <td>0.450035</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.546471</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.520615</td>\n",
       "      <td>0.045752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>21.490246</td>\n",
       "      <td>0.542226</td>\n",
       "      <td>0.061502</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.493706</td>\n",
       "      <td>0.055677</td>\n",
       "      <td>12</td>\n",
       "      <td>0.556953</td>\n",
       "      <td>0.617051</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.628232</td>\n",
       "      <td>0.046425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>21.902505</td>\n",
       "      <td>0.395145</td>\n",
       "      <td>0.078490</td>\n",
       "      <td>0.011068</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542977</td>\n",
       "      <td>0.478507</td>\n",
       "      <td>0.058126</td>\n",
       "      <td>29</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.516422</td>\n",
       "      <td>0.558351</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.529525</td>\n",
       "      <td>0.041810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>24.099248</td>\n",
       "      <td>0.507706</td>\n",
       "      <td>0.071501</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.514668</td>\n",
       "      <td>0.057195</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573026</td>\n",
       "      <td>0.633823</td>\n",
       "      <td>0.737945</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.660028</td>\n",
       "      <td>0.062395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>16.362752</td>\n",
       "      <td>0.324775</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463312</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>44</td>\n",
       "      <td>0.429071</td>\n",
       "      <td>0.473096</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.494584</td>\n",
       "      <td>0.049333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>18.254003</td>\n",
       "      <td>0.529828</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.006222</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551363</td>\n",
       "      <td>0.484795</td>\n",
       "      <td>0.057575</td>\n",
       "      <td>22</td>\n",
       "      <td>0.542278</td>\n",
       "      <td>0.587701</td>\n",
       "      <td>0.668763</td>\n",
       "      <td>0.672956</td>\n",
       "      <td>0.617925</td>\n",
       "      <td>0.055337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>18.654258</td>\n",
       "      <td>0.472881</td>\n",
       "      <td>0.072999</td>\n",
       "      <td>0.020114</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>0.051083</td>\n",
       "      <td>36</td>\n",
       "      <td>0.450035</td>\n",
       "      <td>0.513627</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.572327</td>\n",
       "      <td>0.520790</td>\n",
       "      <td>0.045852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>21.592253</td>\n",
       "      <td>0.304740</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.491085</td>\n",
       "      <td>0.055969</td>\n",
       "      <td>14</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.665269</td>\n",
       "      <td>0.624214</td>\n",
       "      <td>0.050187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>21.988753</td>\n",
       "      <td>0.644842</td>\n",
       "      <td>0.074999</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.477459</td>\n",
       "      <td>0.057250</td>\n",
       "      <td>30</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.557652</td>\n",
       "      <td>0.575821</td>\n",
       "      <td>0.529175</td>\n",
       "      <td>0.041746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>24.327004</td>\n",
       "      <td>0.705039</td>\n",
       "      <td>0.068753</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.512047</td>\n",
       "      <td>0.061969</td>\n",
       "      <td>2</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.619846</td>\n",
       "      <td>0.730259</td>\n",
       "      <td>0.693920</td>\n",
       "      <td>0.653040</td>\n",
       "      <td>0.063136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>16.787014</td>\n",
       "      <td>0.252831</td>\n",
       "      <td>0.055991</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467505</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>0.063748</td>\n",
       "      <td>43</td>\n",
       "      <td>0.429769</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.492837</td>\n",
       "      <td>0.047927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>15.897499</td>\n",
       "      <td>1.557125</td>\n",
       "      <td>0.049254</td>\n",
       "      <td>0.006903</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.498945</td>\n",
       "      <td>0.067236</td>\n",
       "      <td>7</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.589797</td>\n",
       "      <td>0.682041</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.620196</td>\n",
       "      <td>0.055452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>19.482251</td>\n",
       "      <td>0.452556</td>\n",
       "      <td>0.060503</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507338</td>\n",
       "      <td>0.460689</td>\n",
       "      <td>0.049087</td>\n",
       "      <td>39</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.501048</td>\n",
       "      <td>0.546471</td>\n",
       "      <td>0.570929</td>\n",
       "      <td>0.513103</td>\n",
       "      <td>0.052120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20.434508</td>\n",
       "      <td>1.322099</td>\n",
       "      <td>0.050248</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>30</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557652</td>\n",
       "      <td>0.484272</td>\n",
       "      <td>0.060271</td>\n",
       "      <td>24</td>\n",
       "      <td>0.519217</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.656883</td>\n",
       "      <td>0.602725</td>\n",
       "      <td>0.055194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>22.482255</td>\n",
       "      <td>0.316870</td>\n",
       "      <td>0.074746</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>0.058634</td>\n",
       "      <td>33</td>\n",
       "      <td>0.455625</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.557652</td>\n",
       "      <td>0.573026</td>\n",
       "      <td>0.523760</td>\n",
       "      <td>0.045945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16.944497</td>\n",
       "      <td>1.239742</td>\n",
       "      <td>0.058999</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.058070</td>\n",
       "      <td>5</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.596087</td>\n",
       "      <td>0.694619</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.625961</td>\n",
       "      <td>0.049982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        4.775499      0.222299         0.047252        0.001922   \n",
       "1        1.694757      0.123489         0.047745        0.005322   \n",
       "2        6.782754      0.446546         0.064001        0.005999   \n",
       "3        1.856503      0.087326         0.061756        0.002273   \n",
       "4        6.251008      0.229054         0.061996        0.012090   \n",
       "5        2.101501      0.385897         0.075764        0.014529   \n",
       "6        6.861001      0.176050         0.063767        0.010816   \n",
       "7        1.975233      0.134533         0.048255        0.004498   \n",
       "8        5.924519      0.413967         0.071993        0.015847   \n",
       "9        2.564246      0.133171         0.075255        0.017327   \n",
       "10       6.837503      0.084054         0.071249        0.003341   \n",
       "11       1.969244      0.273480         0.069498        0.004026   \n",
       "12       6.398499      0.184817         0.065744        0.008046   \n",
       "13       5.132252      1.405434         0.064004        0.019839   \n",
       "14       6.191003      0.613985         0.076500        0.024681   \n",
       "15       4.965756      1.215305         0.067753        0.011171   \n",
       "16       6.888509      0.429072         0.085996        0.014865   \n",
       "17       6.239740      1.764236         0.081007        0.010891   \n",
       "18      14.984505      0.274071         0.056747        0.003266   \n",
       "19      18.233255      0.537467         0.054246        0.005849   \n",
       "20      17.456001      0.431638         0.062255        0.011095   \n",
       "21      19.227499      0.372447         0.058501        0.004384   \n",
       "22      20.316752      1.935356         0.086254        0.024316   \n",
       "23      32.478493      1.871172         0.094001        0.034710   \n",
       "24      17.342246      2.135672         0.057502        0.018819   \n",
       "25      16.477503      0.972069         0.070746        0.030543   \n",
       "26      17.921009      1.186882         0.057752        0.001297   \n",
       "27      18.937748      0.400661         0.058747        0.005532   \n",
       "28      18.978260      1.704479         0.069743        0.005889   \n",
       "29      21.667002      0.400816         0.077498        0.016587   \n",
       "..            ...           ...              ...             ...   \n",
       "42      14.823998      0.305129         0.047753        0.003342   \n",
       "43      18.064747      0.987442         0.074006        0.025545   \n",
       "44      20.441744      0.859471         0.058257        0.010325   \n",
       "45      26.879757      1.127441         0.063248        0.017687   \n",
       "46      24.770258      1.138280         0.070745        0.006867   \n",
       "47      26.123751      2.587408         0.064745        0.007259   \n",
       "48      14.667006      0.450582         0.045999        0.003534   \n",
       "49      15.789512      1.152794         0.047742        0.003958   \n",
       "50      19.630752      0.814912         0.056755        0.009704   \n",
       "51      19.751008      0.129881         0.063997        0.014076   \n",
       "52      20.885504      0.414476         0.064501        0.002685   \n",
       "53      22.926750      0.421111         0.081250        0.016843   \n",
       "54      15.441752      0.355514         0.054748        0.000826   \n",
       "55      19.335506      0.384764         0.052495        0.004930   \n",
       "56      20.795001      0.801262         0.061502        0.002179   \n",
       "57      21.490246      0.542226         0.061502        0.005680   \n",
       "58      21.902505      0.395145         0.078490        0.011068   \n",
       "59      24.099248      0.507706         0.071501        0.011104   \n",
       "60      16.362752      0.324775         0.057250        0.005492   \n",
       "61      18.254003      0.529828         0.049753        0.006222   \n",
       "62      18.654258      0.472881         0.072999        0.020114   \n",
       "63      21.592253      0.304740         0.057250        0.007728   \n",
       "64      21.988753      0.644842         0.074999        0.009137   \n",
       "65      24.327004      0.705039         0.068753        0.009004   \n",
       "66      16.787014      0.252831         0.055991        0.003317   \n",
       "67      15.897499      1.557125         0.049254        0.006903   \n",
       "68      19.482251      0.452556         0.060503        0.003200   \n",
       "69      20.434508      1.322099         0.050248        0.007462   \n",
       "70      22.482255      0.316870         0.074746        0.015660   \n",
       "71      16.944497      1.239742         0.058999        0.024928   \n",
       "\n",
       "   param_mlpclassifier__activation param_mlpclassifier__alpha  \\\n",
       "0                         identity                      0.001   \n",
       "1                         identity                      0.001   \n",
       "2                         identity                      0.001   \n",
       "3                         identity                      0.001   \n",
       "4                         identity                      0.001   \n",
       "5                         identity                      0.001   \n",
       "6                         identity                       0.01   \n",
       "7                         identity                       0.01   \n",
       "8                         identity                       0.01   \n",
       "9                         identity                       0.01   \n",
       "10                        identity                       0.01   \n",
       "11                        identity                       0.01   \n",
       "12                        identity                        0.1   \n",
       "13                        identity                        0.1   \n",
       "14                        identity                        0.1   \n",
       "15                        identity                        0.1   \n",
       "16                        identity                        0.1   \n",
       "17                        identity                        0.1   \n",
       "18                        logistic                      0.001   \n",
       "19                        logistic                      0.001   \n",
       "20                        logistic                      0.001   \n",
       "21                        logistic                      0.001   \n",
       "22                        logistic                      0.001   \n",
       "23                        logistic                      0.001   \n",
       "24                        logistic                       0.01   \n",
       "25                        logistic                       0.01   \n",
       "26                        logistic                       0.01   \n",
       "27                        logistic                       0.01   \n",
       "28                        logistic                       0.01   \n",
       "29                        logistic                       0.01   \n",
       "..                             ...                        ...   \n",
       "42                            tanh                       0.01   \n",
       "43                            tanh                       0.01   \n",
       "44                            tanh                       0.01   \n",
       "45                            tanh                       0.01   \n",
       "46                            tanh                       0.01   \n",
       "47                            tanh                       0.01   \n",
       "48                            tanh                        0.1   \n",
       "49                            tanh                        0.1   \n",
       "50                            tanh                        0.1   \n",
       "51                            tanh                        0.1   \n",
       "52                            tanh                        0.1   \n",
       "53                            tanh                        0.1   \n",
       "54                            relu                      0.001   \n",
       "55                            relu                      0.001   \n",
       "56                            relu                      0.001   \n",
       "57                            relu                      0.001   \n",
       "58                            relu                      0.001   \n",
       "59                            relu                      0.001   \n",
       "60                            relu                       0.01   \n",
       "61                            relu                       0.01   \n",
       "62                            relu                       0.01   \n",
       "63                            relu                       0.01   \n",
       "64                            relu                       0.01   \n",
       "65                            relu                       0.01   \n",
       "66                            relu                        0.1   \n",
       "67                            relu                        0.1   \n",
       "68                            relu                        0.1   \n",
       "69                            relu                        0.1   \n",
       "70                            relu                        0.1   \n",
       "71                            relu                        0.1   \n",
       "\n",
       "   param_mlpclassifier__hidden_layer_sizes param_mlpclassifier__learning_rate  \\\n",
       "0                                       20                           adaptive   \n",
       "1                                       20                           adaptive   \n",
       "2                                       30                           adaptive   \n",
       "3                                       30                           adaptive   \n",
       "4                                       40                           adaptive   \n",
       "5                                       40                           adaptive   \n",
       "6                                       20                           adaptive   \n",
       "7                                       20                           adaptive   \n",
       "8                                       30                           adaptive   \n",
       "9                                       30                           adaptive   \n",
       "10                                      40                           adaptive   \n",
       "11                                      40                           adaptive   \n",
       "12                                      20                           adaptive   \n",
       "13                                      20                           adaptive   \n",
       "14                                      30                           adaptive   \n",
       "15                                      30                           adaptive   \n",
       "16                                      40                           adaptive   \n",
       "17                                      40                           adaptive   \n",
       "18                                      20                           adaptive   \n",
       "19                                      20                           adaptive   \n",
       "20                                      30                           adaptive   \n",
       "21                                      30                           adaptive   \n",
       "22                                      40                           adaptive   \n",
       "23                                      40                           adaptive   \n",
       "24                                      20                           adaptive   \n",
       "25                                      20                           adaptive   \n",
       "26                                      30                           adaptive   \n",
       "27                                      30                           adaptive   \n",
       "28                                      40                           adaptive   \n",
       "29                                      40                           adaptive   \n",
       "..                                     ...                                ...   \n",
       "42                                      20                           adaptive   \n",
       "43                                      20                           adaptive   \n",
       "44                                      30                           adaptive   \n",
       "45                                      30                           adaptive   \n",
       "46                                      40                           adaptive   \n",
       "47                                      40                           adaptive   \n",
       "48                                      20                           adaptive   \n",
       "49                                      20                           adaptive   \n",
       "50                                      30                           adaptive   \n",
       "51                                      30                           adaptive   \n",
       "52                                      40                           adaptive   \n",
       "53                                      40                           adaptive   \n",
       "54                                      20                           adaptive   \n",
       "55                                      20                           adaptive   \n",
       "56                                      30                           adaptive   \n",
       "57                                      30                           adaptive   \n",
       "58                                      40                           adaptive   \n",
       "59                                      40                           adaptive   \n",
       "60                                      20                           adaptive   \n",
       "61                                      20                           adaptive   \n",
       "62                                      30                           adaptive   \n",
       "63                                      30                           adaptive   \n",
       "64                                      40                           adaptive   \n",
       "65                                      40                           adaptive   \n",
       "66                                      20                           adaptive   \n",
       "67                                      20                           adaptive   \n",
       "68                                      30                           adaptive   \n",
       "69                                      30                           adaptive   \n",
       "70                                      40                           adaptive   \n",
       "71                                      40                           adaptive   \n",
       "\n",
       "   param_mlpclassifier__max_iter param_mlpclassifier__random_state  ...  \\\n",
       "0                            250                                 0  ...   \n",
       "1                            250                                 0  ...   \n",
       "2                            250                                 0  ...   \n",
       "3                            250                                 0  ...   \n",
       "4                            250                                 0  ...   \n",
       "5                            250                                 0  ...   \n",
       "6                            250                                 0  ...   \n",
       "7                            250                                 0  ...   \n",
       "8                            250                                 0  ...   \n",
       "9                            250                                 0  ...   \n",
       "10                           250                                 0  ...   \n",
       "11                           250                                 0  ...   \n",
       "12                           250                                 0  ...   \n",
       "13                           250                                 0  ...   \n",
       "14                           250                                 0  ...   \n",
       "15                           250                                 0  ...   \n",
       "16                           250                                 0  ...   \n",
       "17                           250                                 0  ...   \n",
       "18                           250                                 0  ...   \n",
       "19                           250                                 0  ...   \n",
       "20                           250                                 0  ...   \n",
       "21                           250                                 0  ...   \n",
       "22                           250                                 0  ...   \n",
       "23                           250                                 0  ...   \n",
       "24                           250                                 0  ...   \n",
       "25                           250                                 0  ...   \n",
       "26                           250                                 0  ...   \n",
       "27                           250                                 0  ...   \n",
       "28                           250                                 0  ...   \n",
       "29                           250                                 0  ...   \n",
       "..                           ...                               ...  ...   \n",
       "42                           250                                 0  ...   \n",
       "43                           250                                 0  ...   \n",
       "44                           250                                 0  ...   \n",
       "45                           250                                 0  ...   \n",
       "46                           250                                 0  ...   \n",
       "47                           250                                 0  ...   \n",
       "48                           250                                 0  ...   \n",
       "49                           250                                 0  ...   \n",
       "50                           250                                 0  ...   \n",
       "51                           250                                 0  ...   \n",
       "52                           250                                 0  ...   \n",
       "53                           250                                 0  ...   \n",
       "54                           250                                 0  ...   \n",
       "55                           250                                 0  ...   \n",
       "56                           250                                 0  ...   \n",
       "57                           250                                 0  ...   \n",
       "58                           250                                 0  ...   \n",
       "59                           250                                 0  ...   \n",
       "60                           250                                 0  ...   \n",
       "61                           250                                 0  ...   \n",
       "62                           250                                 0  ...   \n",
       "63                           250                                 0  ...   \n",
       "64                           250                                 0  ...   \n",
       "65                           250                                 0  ...   \n",
       "66                           250                                 0  ...   \n",
       "67                           250                                 0  ...   \n",
       "68                           250                                 0  ...   \n",
       "69                           250                                 0  ...   \n",
       "70                           250                                 0  ...   \n",
       "71                           250                                 0  ...   \n",
       "\n",
       "   split3_test_Sensitivity mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                 0.419287              0.359533              0.046843   \n",
       "1                 0.423480              0.363725              0.046419   \n",
       "2                 0.415094              0.356388              0.046596   \n",
       "3                 0.433962              0.366870              0.052788   \n",
       "4                 0.419287              0.360581              0.048537   \n",
       "5                 0.423480              0.364249              0.049553   \n",
       "6                 0.419287              0.359533              0.046843   \n",
       "7                 0.423480              0.363201              0.046226   \n",
       "8                 0.415094              0.356388              0.046596   \n",
       "9                 0.433962              0.366870              0.052788   \n",
       "10                0.419287              0.360581              0.048537   \n",
       "11                0.423480              0.364249              0.049553   \n",
       "12                0.419287              0.357960              0.047515   \n",
       "13                0.402516              0.355340              0.045968   \n",
       "14                0.415094              0.355864              0.046605   \n",
       "15                0.408805              0.365297              0.046012   \n",
       "16                0.419287              0.360581              0.048537   \n",
       "17                0.425577              0.360057              0.047930   \n",
       "18                0.446541              0.391503              0.047167   \n",
       "19                0.545073              0.491612              0.073686   \n",
       "20                0.438155              0.385213              0.049995   \n",
       "21                0.570231              0.498947              0.084813   \n",
       "22                0.429769              0.369490              0.049620   \n",
       "23                0.566038              0.506809              0.069997   \n",
       "24                0.446541              0.389930              0.046241   \n",
       "25                0.538784              0.487417              0.069858   \n",
       "26                0.436059              0.384689              0.049445   \n",
       "27                0.563941              0.496326              0.076708   \n",
       "28                0.429769              0.368966              0.049412   \n",
       "29                0.559748              0.502616              0.066421   \n",
       "..                     ...                   ...                   ...   \n",
       "42                0.513627              0.484273              0.038998   \n",
       "43                0.555556              0.496325              0.050061   \n",
       "44                0.513627              0.469598              0.038027   \n",
       "45                0.526205              0.474837              0.048503   \n",
       "46                0.511530              0.460164              0.041042   \n",
       "47                0.566038              0.491079              0.075942   \n",
       "48                0.509434              0.479555              0.039982   \n",
       "49                0.570231              0.487938              0.066087   \n",
       "50                0.513627              0.461735              0.046794   \n",
       "51                0.532495              0.487942              0.044901   \n",
       "52                0.507338              0.459115              0.039165   \n",
       "53                0.568134              0.498944              0.059279   \n",
       "54                0.463312              0.426622              0.065909   \n",
       "55                0.551363              0.482175              0.057986   \n",
       "56                0.509434              0.461737              0.051083   \n",
       "57                0.536688              0.493706              0.055677   \n",
       "58                0.542977              0.478507              0.058126   \n",
       "59                0.563941              0.514668              0.057195   \n",
       "60                0.463312              0.426622              0.065909   \n",
       "61                0.551363              0.484795              0.057575   \n",
       "62                0.509434              0.461737              0.051083   \n",
       "63                0.540881              0.491085              0.055969   \n",
       "64                0.540881              0.477459              0.057250   \n",
       "65                0.563941              0.512047              0.061969   \n",
       "66                0.467505              0.427670              0.063748   \n",
       "67                0.566038              0.498945              0.067236   \n",
       "68                0.507338              0.460689              0.049087   \n",
       "69                0.557652              0.484272              0.060271   \n",
       "70                0.536688              0.474314              0.058634   \n",
       "71                0.559748              0.501567              0.058070   \n",
       "\n",
       "    rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                      66                  0.310273                  0.359189   \n",
       "1                      59                  0.320755                  0.361286   \n",
       "2                      69                  0.310971                  0.358491   \n",
       "3                      54                  0.307477                  0.358491   \n",
       "4                      62                  0.311670                  0.360587   \n",
       "5                      57                  0.307477                  0.356394   \n",
       "6                      66                  0.310273                  0.359189   \n",
       "7                      60                  0.320755                  0.361286   \n",
       "8                      69                  0.310971                  0.358491   \n",
       "9                      54                  0.307477                  0.358491   \n",
       "10                     62                  0.311670                  0.360587   \n",
       "11                     57                  0.307477                  0.356394   \n",
       "12                     68                  0.308875                  0.358491   \n",
       "13                     72                  0.314465                  0.359189   \n",
       "14                     71                  0.310971                  0.357792   \n",
       "15                     56                  0.315863                  0.352201   \n",
       "16                     62                  0.310971                  0.360587   \n",
       "17                     65                  0.321454                  0.354997   \n",
       "18                     46                  0.340321                  0.395528   \n",
       "19                     13                  0.501048                  0.570929   \n",
       "20                     48                  0.328442                  0.393431   \n",
       "21                      6                  0.502446                  0.582809   \n",
       "22                     52                  0.316562                  0.378057   \n",
       "23                      3                  0.505940                  0.582110   \n",
       "24                     47                  0.340321                  0.393431   \n",
       "25                     19                  0.493361                  0.563242   \n",
       "26                     49                  0.327743                  0.392732   \n",
       "27                     10                  0.491265                  0.572327   \n",
       "28                     53                  0.316562                  0.377358   \n",
       "29                      4                  0.498952                  0.573725   \n",
       "..                    ...                       ...                       ...   \n",
       "42                     23                  0.436059                  0.512928   \n",
       "43                     11                  0.530398                  0.589099   \n",
       "44                     35                  0.415094                  0.484976   \n",
       "45                     31                  0.560447                  0.622642   \n",
       "46                     41                  0.411600                  0.482879   \n",
       "47                     15                  0.556953                  0.613557   \n",
       "48                     28                  0.429769                  0.513627   \n",
       "49                     18                  0.515024                  0.566737   \n",
       "50                     38                  0.403913                  0.482879   \n",
       "51                     17                  0.535290                  0.587002   \n",
       "52                     42                  0.404612                  0.480084   \n",
       "53                      8                  0.524109                  0.575821   \n",
       "54                     44                  0.427673                  0.473795   \n",
       "55                     26                  0.538784                  0.586303   \n",
       "56                     36                  0.450035                  0.513627   \n",
       "57                     12                  0.556953                  0.617051   \n",
       "58                     29                  0.467505                  0.516422   \n",
       "59                      1                  0.573026                  0.633823   \n",
       "60                     44                  0.429071                  0.473096   \n",
       "61                     22                  0.542278                  0.587701   \n",
       "62                     36                  0.450035                  0.513627   \n",
       "63                     14                  0.547170                  0.612159   \n",
       "64                     30                  0.467505                  0.515723   \n",
       "65                      2                  0.568134                  0.619846   \n",
       "66                     43                  0.429769                  0.470999   \n",
       "67                      7                  0.545073                  0.589797   \n",
       "68                     39                  0.433962                  0.501048   \n",
       "69                     24                  0.519217                  0.587002   \n",
       "70                     33                  0.455625                  0.508735   \n",
       "71                      5                  0.563941                  0.596087   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.422781                  0.414396   \n",
       "1                   0.430468                  0.419986   \n",
       "2                   0.415094                  0.415793   \n",
       "3                   0.420685                  0.422082   \n",
       "4                   0.419287                  0.412998   \n",
       "5                   0.426275                  0.412299   \n",
       "6                   0.422781                  0.413697   \n",
       "7                   0.430468                  0.419986   \n",
       "8                   0.415094                  0.415793   \n",
       "9                   0.420685                  0.422082   \n",
       "10                  0.418588                  0.412998   \n",
       "11                  0.426275                  0.412299   \n",
       "12                  0.420685                  0.412299   \n",
       "13                  0.422781                  0.406709   \n",
       "14                  0.414396                  0.414396   \n",
       "15                  0.443047                  0.410203   \n",
       "16                  0.417890                  0.411600   \n",
       "17                  0.417191                  0.421384   \n",
       "18                  0.460517                  0.456324   \n",
       "19                  0.605870                  0.641509   \n",
       "20                  0.453529                  0.442348   \n",
       "21                  0.621244                  0.649196   \n",
       "22                  0.432565                  0.431866   \n",
       "23                  0.642907                  0.641509   \n",
       "24                  0.455625                  0.456324   \n",
       "25                  0.603075                  0.633823   \n",
       "26                  0.452830                  0.442348   \n",
       "27                  0.610063                  0.630328   \n",
       "28                  0.431866                  0.429071   \n",
       "29                  0.625437                  0.635919   \n",
       "..                       ...                       ...   \n",
       "42                  0.559050                  0.570231   \n",
       "43                  0.670860                  0.667365   \n",
       "44                  0.526205                  0.545772   \n",
       "45                  0.679245                  0.673655   \n",
       "46                  0.524109                  0.530398   \n",
       "47                  0.725367                  0.709993   \n",
       "48                  0.556953                  0.566737   \n",
       "49                  0.636618                  0.653389   \n",
       "50                  0.522013                  0.540182   \n",
       "51                  0.640112                  0.636618   \n",
       "52                  0.520615                  0.525507   \n",
       "53                  0.682739                  0.675751   \n",
       "54                  0.514326                  0.562544   \n",
       "55                  0.669462                  0.672257   \n",
       "56                  0.546471                  0.572327   \n",
       "57                  0.672257                  0.666667   \n",
       "58                  0.558351                  0.575821   \n",
       "59                  0.737945                  0.695318   \n",
       "60                  0.513627                  0.562544   \n",
       "61                  0.668763                  0.672956   \n",
       "62                  0.547170                  0.572327   \n",
       "63                  0.672257                  0.665269   \n",
       "64                  0.557652                  0.575821   \n",
       "65                  0.730259                  0.693920   \n",
       "66                  0.511530                  0.559050   \n",
       "67                  0.682041                  0.663871   \n",
       "68                  0.546471                  0.570929   \n",
       "69                  0.647799                  0.656883   \n",
       "70                  0.557652                  0.573026   \n",
       "71                  0.694619                  0.649196   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.376660               0.045452  \n",
       "1                 0.383124               0.044629  \n",
       "2                 0.375087               0.043715  \n",
       "3                 0.377184               0.047741  \n",
       "4                 0.376136               0.043642  \n",
       "5                 0.375611               0.047234  \n",
       "6                 0.376485               0.045308  \n",
       "7                 0.383124               0.044629  \n",
       "8                 0.375087               0.043715  \n",
       "9                 0.377184               0.047741  \n",
       "10                0.375961               0.043470  \n",
       "11                0.375611               0.047234  \n",
       "12                0.375087               0.045065  \n",
       "13                0.375786               0.042428  \n",
       "14                0.374389               0.043296  \n",
       "15                0.380328               0.049429  \n",
       "16                0.375262               0.043261  \n",
       "17                0.378756               0.042256  \n",
       "18                0.413173               0.049301  \n",
       "19                0.579839               0.051885  \n",
       "20                0.404437               0.049355  \n",
       "21                0.588924               0.055211  \n",
       "22                0.389762               0.047697  \n",
       "23                0.593117               0.055995  \n",
       "24                0.411426               0.048345  \n",
       "25                0.573375               0.052538  \n",
       "26                0.403913               0.049490  \n",
       "27                0.575996               0.053163  \n",
       "28                0.388714               0.046972  \n",
       "29                0.583508               0.054200  \n",
       "..                     ...                    ...  \n",
       "42                0.519567               0.052781  \n",
       "43                0.614430               0.058501  \n",
       "44                0.493012               0.050053  \n",
       "45                0.633997               0.047850  \n",
       "46                0.487247               0.047335  \n",
       "47                0.651468               0.069384  \n",
       "48                0.516771               0.054061  \n",
       "49                0.592942               0.055496  \n",
       "50                0.487247               0.052379  \n",
       "51                0.599755               0.042737  \n",
       "52                0.482704               0.048411  \n",
       "53                0.614605               0.067221  \n",
       "54                0.494584               0.049793  \n",
       "55                0.616702               0.056713  \n",
       "56                0.520615               0.045752  \n",
       "57                0.628232               0.046425  \n",
       "58                0.529525               0.041810  \n",
       "59                0.660028               0.062395  \n",
       "60                0.494584               0.049333  \n",
       "61                0.617925               0.055337  \n",
       "62                0.520790               0.045852  \n",
       "63                0.624214               0.050187  \n",
       "64                0.529175               0.041746  \n",
       "65                0.653040               0.063136  \n",
       "66                0.492837               0.047927  \n",
       "67                0.620196               0.055452  \n",
       "68                0.513103               0.052120  \n",
       "69                0.602725               0.055194  \n",
       "70                0.523760               0.045945  \n",
       "71                0.625961               0.049982  \n",
       "\n",
       "[72 rows x 51 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mlp = gs.cv_results_\n",
    "data = pandas.DataFrame(results_mlp)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.775499</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.874702</td>\n",
       "      <td>0.475896</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>46</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.895454</td>\n",
       "      <td>0.499611</td>\n",
       "      <td>0.376660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.694757</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.870542</td>\n",
       "      <td>0.479616</td>\n",
       "      <td>0.363725</td>\n",
       "      <td>64</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>0.893105</td>\n",
       "      <td>0.505436</td>\n",
       "      <td>0.383124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.782754</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.873192</td>\n",
       "      <td>0.473380</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>53</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>0.894203</td>\n",
       "      <td>0.498355</td>\n",
       "      <td>0.375087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.856503</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.872280</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>58</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>0.892793</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.377184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.251008</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.875449</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>42</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0.894690</td>\n",
       "      <td>0.499144</td>\n",
       "      <td>0.376136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.101501</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.871387</td>\n",
       "      <td>0.480934</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>61</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>0.892603</td>\n",
       "      <td>0.499063</td>\n",
       "      <td>0.375611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.861001</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.874701</td>\n",
       "      <td>0.475896</td>\n",
       "      <td>0.359533</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.895454</td>\n",
       "      <td>0.499445</td>\n",
       "      <td>0.376485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.975233</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.870576</td>\n",
       "      <td>0.479117</td>\n",
       "      <td>0.363201</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>0.893121</td>\n",
       "      <td>0.505436</td>\n",
       "      <td>0.383124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.924519</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.873200</td>\n",
       "      <td>0.473380</td>\n",
       "      <td>0.356388</td>\n",
       "      <td>52</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>0.894205</td>\n",
       "      <td>0.498416</td>\n",
       "      <td>0.375087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.564246</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.872292</td>\n",
       "      <td>0.481655</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>0.499856</td>\n",
       "      <td>0.377184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.837503</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.875455</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>41</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>0.894691</td>\n",
       "      <td>0.498979</td>\n",
       "      <td>0.375961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.969244</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.480934</td>\n",
       "      <td>0.364249</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>0.892624</td>\n",
       "      <td>0.499063</td>\n",
       "      <td>0.375611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.398499</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.874708</td>\n",
       "      <td>0.474220</td>\n",
       "      <td>0.357960</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>0.895418</td>\n",
       "      <td>0.498286</td>\n",
       "      <td>0.375087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.132252</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.868748</td>\n",
       "      <td>0.473078</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>66</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>0.892663</td>\n",
       "      <td>0.499085</td>\n",
       "      <td>0.375786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.191003</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.355864</td>\n",
       "      <td>51</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>0.894218</td>\n",
       "      <td>0.497853</td>\n",
       "      <td>0.374389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.965756</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.869673</td>\n",
       "      <td>0.480692</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>65</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>0.892822</td>\n",
       "      <td>0.503448</td>\n",
       "      <td>0.380328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.888509</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.875250</td>\n",
       "      <td>0.477258</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>43</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>0.894625</td>\n",
       "      <td>0.498454</td>\n",
       "      <td>0.375262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.239740</td>\n",
       "      <td>{'mlpclassifier__activation': 'identity', 'mlp...</td>\n",
       "      <td>0.871448</td>\n",
       "      <td>0.477777</td>\n",
       "      <td>0.360057</td>\n",
       "      <td>59</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>0.892860</td>\n",
       "      <td>0.501004</td>\n",
       "      <td>0.378756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.984505</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.881215</td>\n",
       "      <td>0.502821</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>19</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.530166</td>\n",
       "      <td>0.413173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.233255</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.895757</td>\n",
       "      <td>0.559186</td>\n",
       "      <td>0.491612</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0.923915</td>\n",
       "      <td>0.644879</td>\n",
       "      <td>0.579839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.456001</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879712</td>\n",
       "      <td>0.497272</td>\n",
       "      <td>0.385213</td>\n",
       "      <td>28</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>0.897016</td>\n",
       "      <td>0.521912</td>\n",
       "      <td>0.404437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.227499</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.893407</td>\n",
       "      <td>0.560192</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.928074</td>\n",
       "      <td>0.654749</td>\n",
       "      <td>0.588924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20.316752</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879773</td>\n",
       "      <td>0.482566</td>\n",
       "      <td>0.369490</td>\n",
       "      <td>25</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>0.895797</td>\n",
       "      <td>0.510192</td>\n",
       "      <td>0.389762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.478493</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.893475</td>\n",
       "      <td>0.566355</td>\n",
       "      <td>0.506809</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.928758</td>\n",
       "      <td>0.657432</td>\n",
       "      <td>0.593117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.342246</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.881179</td>\n",
       "      <td>0.501246</td>\n",
       "      <td>0.389930</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>0.898034</td>\n",
       "      <td>0.528639</td>\n",
       "      <td>0.411426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.477503</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.896360</td>\n",
       "      <td>0.558226</td>\n",
       "      <td>0.487417</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>0.922572</td>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.573375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17.921009</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879716</td>\n",
       "      <td>0.496814</td>\n",
       "      <td>0.384689</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.896994</td>\n",
       "      <td>0.521466</td>\n",
       "      <td>0.403913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18.937748</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.561888</td>\n",
       "      <td>0.496326</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.926234</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.575996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18.978260</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.879766</td>\n",
       "      <td>0.482411</td>\n",
       "      <td>0.368966</td>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.895780</td>\n",
       "      <td>0.509267</td>\n",
       "      <td>0.388714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.667002</td>\n",
       "      <td>{'mlpclassifier__activation': 'logistic', 'mlp...</td>\n",
       "      <td>0.894628</td>\n",
       "      <td>0.565379</td>\n",
       "      <td>0.502616</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.926367</td>\n",
       "      <td>0.651365</td>\n",
       "      <td>0.583508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14.823998</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.878853</td>\n",
       "      <td>0.567894</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0.905560</td>\n",
       "      <td>0.604135</td>\n",
       "      <td>0.519567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>18.064747</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.868652</td>\n",
       "      <td>0.545042</td>\n",
       "      <td>0.496325</td>\n",
       "      <td>67</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>0.939513</td>\n",
       "      <td>0.684445</td>\n",
       "      <td>0.614430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20.441744</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.877959</td>\n",
       "      <td>0.557799</td>\n",
       "      <td>0.469598</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0.903121</td>\n",
       "      <td>0.588428</td>\n",
       "      <td>0.493012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>26.879757</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.863318</td>\n",
       "      <td>0.527817</td>\n",
       "      <td>0.474837</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>0.945649</td>\n",
       "      <td>0.706825</td>\n",
       "      <td>0.633997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>24.770258</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.880258</td>\n",
       "      <td>0.549196</td>\n",
       "      <td>0.460164</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>0.584665</td>\n",
       "      <td>0.487247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>26.123751</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.858753</td>\n",
       "      <td>0.525453</td>\n",
       "      <td>0.491079</td>\n",
       "      <td>71</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>0.950632</td>\n",
       "      <td>0.718143</td>\n",
       "      <td>0.651468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14.667006</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.879087</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.479555</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.905278</td>\n",
       "      <td>0.602569</td>\n",
       "      <td>0.516771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>15.789512</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.877404</td>\n",
       "      <td>0.545270</td>\n",
       "      <td>0.487938</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>0.933976</td>\n",
       "      <td>0.666176</td>\n",
       "      <td>0.592942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19.630752</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.877836</td>\n",
       "      <td>0.551756</td>\n",
       "      <td>0.461735</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>0.902617</td>\n",
       "      <td>0.584122</td>\n",
       "      <td>0.487247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>19.751008</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.874128</td>\n",
       "      <td>0.548880</td>\n",
       "      <td>0.487942</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>0.936242</td>\n",
       "      <td>0.677237</td>\n",
       "      <td>0.599755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20.885504</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.880582</td>\n",
       "      <td>0.549222</td>\n",
       "      <td>0.459115</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>0.905904</td>\n",
       "      <td>0.581875</td>\n",
       "      <td>0.482704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>22.926750</td>\n",
       "      <td>{'mlpclassifier__activation': 'tanh', 'mlpclas...</td>\n",
       "      <td>0.872354</td>\n",
       "      <td>0.545427</td>\n",
       "      <td>0.498944</td>\n",
       "      <td>56</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>0.938497</td>\n",
       "      <td>0.686262</td>\n",
       "      <td>0.614605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>15.441752</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887666</td>\n",
       "      <td>0.524281</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>0.914389</td>\n",
       "      <td>0.593204</td>\n",
       "      <td>0.494584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>19.335506</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.537242</td>\n",
       "      <td>0.482175</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>0.939566</td>\n",
       "      <td>0.684732</td>\n",
       "      <td>0.616702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20.795001</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.885909</td>\n",
       "      <td>0.551189</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>0.914588</td>\n",
       "      <td>0.609394</td>\n",
       "      <td>0.520615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>21.490246</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.872986</td>\n",
       "      <td>0.543745</td>\n",
       "      <td>0.493706</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>0.946896</td>\n",
       "      <td>0.698741</td>\n",
       "      <td>0.628232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>21.902505</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887667</td>\n",
       "      <td>0.562246</td>\n",
       "      <td>0.478507</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>0.918090</td>\n",
       "      <td>0.619567</td>\n",
       "      <td>0.529525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>24.099248</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.871116</td>\n",
       "      <td>0.544025</td>\n",
       "      <td>0.514668</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951540</td>\n",
       "      <td>0.719065</td>\n",
       "      <td>0.660028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>16.362752</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887681</td>\n",
       "      <td>0.524281</td>\n",
       "      <td>0.426622</td>\n",
       "      <td>13</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>0.914379</td>\n",
       "      <td>0.593170</td>\n",
       "      <td>0.494584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>18.254003</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.874974</td>\n",
       "      <td>0.540667</td>\n",
       "      <td>0.484795</td>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>0.939396</td>\n",
       "      <td>0.685319</td>\n",
       "      <td>0.617925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>18.654258</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.885941</td>\n",
       "      <td>0.551149</td>\n",
       "      <td>0.461737</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>0.914562</td>\n",
       "      <td>0.609597</td>\n",
       "      <td>0.520790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>21.592253</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.873579</td>\n",
       "      <td>0.540996</td>\n",
       "      <td>0.491085</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>0.946488</td>\n",
       "      <td>0.696007</td>\n",
       "      <td>0.624214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>21.988753</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887703</td>\n",
       "      <td>0.561451</td>\n",
       "      <td>0.477459</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>0.918047</td>\n",
       "      <td>0.619287</td>\n",
       "      <td>0.529175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>24.327004</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.873101</td>\n",
       "      <td>0.542828</td>\n",
       "      <td>0.512047</td>\n",
       "      <td>54</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951005</td>\n",
       "      <td>0.713958</td>\n",
       "      <td>0.653040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>16.787014</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887938</td>\n",
       "      <td>0.526418</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>0.592316</td>\n",
       "      <td>0.492837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>15.897499</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.877452</td>\n",
       "      <td>0.545943</td>\n",
       "      <td>0.498945</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.936629</td>\n",
       "      <td>0.679239</td>\n",
       "      <td>0.620196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>19.482251</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.885888</td>\n",
       "      <td>0.550743</td>\n",
       "      <td>0.460689</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>39</td>\n",
       "      <td>0.913827</td>\n",
       "      <td>0.604496</td>\n",
       "      <td>0.513103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20.434508</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.876358</td>\n",
       "      <td>0.541550</td>\n",
       "      <td>0.484272</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>0.941911</td>\n",
       "      <td>0.679762</td>\n",
       "      <td>0.602725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>22.482255</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.560416</td>\n",
       "      <td>0.474314</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>0.917312</td>\n",
       "      <td>0.614747</td>\n",
       "      <td>0.523760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16.944497</td>\n",
       "      <td>{'mlpclassifier__activation': 'relu', 'mlpclas...</td>\n",
       "      <td>0.874643</td>\n",
       "      <td>0.549809</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>0.944009</td>\n",
       "      <td>0.694192</td>\n",
       "      <td>0.625961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        4.775499  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "1        1.694757  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "2        6.782754  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "3        1.856503  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "4        6.251008  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "5        2.101501  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "6        6.861001  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "7        1.975233  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "8        5.924519  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "9        2.564246  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "10       6.837503  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "11       1.969244  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "12       6.398499  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "13       5.132252  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "14       6.191003  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "15       4.965756  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "16       6.888509  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "17       6.239740  {'mlpclassifier__activation': 'identity', 'mlp...   \n",
       "18      14.984505  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "19      18.233255  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "20      17.456001  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "21      19.227499  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "22      20.316752  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "23      32.478493  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "24      17.342246  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "25      16.477503  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "26      17.921009  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "27      18.937748  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "28      18.978260  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "29      21.667002  {'mlpclassifier__activation': 'logistic', 'mlp...   \n",
       "..            ...                                                ...   \n",
       "42      14.823998  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "43      18.064747  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "44      20.441744  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "45      26.879757  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "46      24.770258  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "47      26.123751  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "48      14.667006  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "49      15.789512  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "50      19.630752  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "51      19.751008  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "52      20.885504  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "53      22.926750  {'mlpclassifier__activation': 'tanh', 'mlpclas...   \n",
       "54      15.441752  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "55      19.335506  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "56      20.795001  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "57      21.490246  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "58      21.902505  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "59      24.099248  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "60      16.362752  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "61      18.254003  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "62      18.654258  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "63      21.592253  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "64      21.988753  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "65      24.327004  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "66      16.787014  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "67      15.897499  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "68      19.482251  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "69      20.434508  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "70      22.482255  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "71      16.944497  {'mlpclassifier__activation': 'relu', 'mlpclas...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.874702           0.475896               0.359533             46   \n",
       "1        0.870542           0.479616               0.363725             64   \n",
       "2        0.873192           0.473380               0.356388             53   \n",
       "3        0.872280           0.481655               0.366870             58   \n",
       "4        0.875449           0.477281               0.360581             42   \n",
       "5        0.871387           0.480934               0.364249             61   \n",
       "6        0.874701           0.475896               0.359533             47   \n",
       "7        0.870576           0.479117               0.363201             63   \n",
       "8        0.873200           0.473380               0.356388             52   \n",
       "9        0.872292           0.481655               0.366870             57   \n",
       "10       0.875455           0.477281               0.360581             41   \n",
       "11       0.871421           0.480934               0.364249             60   \n",
       "12       0.874708           0.474220               0.357960             45   \n",
       "13       0.868748           0.473078               0.355340             66   \n",
       "14       0.873333           0.472989               0.355864             51   \n",
       "15       0.869673           0.480692               0.365297             65   \n",
       "16       0.875250           0.477258               0.360581             43   \n",
       "17       0.871448           0.477777               0.360057             59   \n",
       "18       0.881215           0.502821               0.391503             19   \n",
       "19       0.895757           0.559186               0.491612              2   \n",
       "20       0.879712           0.497272               0.385213             28   \n",
       "21       0.893407           0.560192               0.498947              8   \n",
       "22       0.879773           0.482566               0.369490             25   \n",
       "23       0.893475           0.566355               0.506809              7   \n",
       "24       0.881179           0.501246               0.389930             20   \n",
       "25       0.896360           0.558226               0.487417              1   \n",
       "26       0.879716           0.496814               0.384689             27   \n",
       "27       0.893962           0.561888               0.496326              5   \n",
       "28       0.879766           0.482411               0.368966             26   \n",
       "29       0.894628           0.565379               0.502616              3   \n",
       "..            ...                ...                    ...            ...   \n",
       "42       0.878853           0.567894               0.484273             32   \n",
       "43       0.868652           0.545042               0.496325             67   \n",
       "44       0.877959           0.557799               0.469598             34   \n",
       "45       0.863318           0.527817               0.474837             70   \n",
       "46       0.880258           0.549196               0.460164             23   \n",
       "47       0.858753           0.525453               0.491079             71   \n",
       "48       0.879087           0.563848               0.479555             31   \n",
       "49       0.877404           0.545270               0.487938             38   \n",
       "50       0.877836           0.551756               0.461735             36   \n",
       "51       0.874128           0.548880               0.487942             49   \n",
       "52       0.880582           0.549222               0.459115             22   \n",
       "53       0.872354           0.545427               0.498944             56   \n",
       "54       0.887666           0.524281               0.426622             15   \n",
       "55       0.875827           0.537242               0.482175             40   \n",
       "56       0.885909           0.551189               0.461737             17   \n",
       "57       0.872986           0.543745               0.493706             55   \n",
       "58       0.887667           0.562246               0.478507             14   \n",
       "59       0.871116           0.544025               0.514668             62   \n",
       "60       0.887681           0.524281               0.426622             13   \n",
       "61       0.874974           0.540667               0.484795             44   \n",
       "62       0.885941           0.551149               0.461737             16   \n",
       "63       0.873579           0.540996               0.491085             50   \n",
       "64       0.887703           0.561451               0.477459             12   \n",
       "65       0.873101           0.542828               0.512047             54   \n",
       "66       0.887938           0.526418               0.427670             10   \n",
       "67       0.877452           0.545943               0.498945             37   \n",
       "68       0.885888           0.550743               0.460689             18   \n",
       "69       0.876358           0.541550               0.484272             39   \n",
       "70       0.887870           0.560416               0.474314             11   \n",
       "71       0.874643           0.549809               0.501567             48   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                  65                     66        0.895454   \n",
       "1                  59                     59        0.893105   \n",
       "2                  69                     69        0.894203   \n",
       "3                  54                     54        0.892793   \n",
       "4                  62                     62        0.894690   \n",
       "5                  56                     57        0.892603   \n",
       "6                  65                     66        0.895454   \n",
       "7                  60                     60        0.893121   \n",
       "8                  69                     69        0.894205   \n",
       "9                  54                     54        0.892800   \n",
       "10                 62                     62        0.894691   \n",
       "11                 56                     57        0.892624   \n",
       "12                 68                     68        0.895418   \n",
       "13                 71                     72        0.892663   \n",
       "14                 72                     71        0.894218   \n",
       "15                 58                     56        0.892822   \n",
       "16                 64                     62        0.894625   \n",
       "17                 61                     65        0.892860   \n",
       "18                 46                     46        0.898089   \n",
       "19                 14                     13        0.923915   \n",
       "20                 48                     48        0.897016   \n",
       "21                 13                      6        0.928074   \n",
       "22                 52                     52        0.895797   \n",
       "23                  6                      3        0.928758   \n",
       "24                 47                     47        0.898034   \n",
       "25                 16                     19        0.922572   \n",
       "26                 49                     49        0.896994   \n",
       "27                 10                     10        0.926234   \n",
       "28                 53                     53        0.895780   \n",
       "29                  7                      4        0.926367   \n",
       "..                ...                    ...             ...   \n",
       "42                  4                     23        0.905560   \n",
       "43                 30                     11        0.939513   \n",
       "44                 17                     35        0.903121   \n",
       "45                 40                     31        0.945649   \n",
       "46                 25                     41        0.906226   \n",
       "47                 42                     15        0.950632   \n",
       "48                  8                     28        0.905278   \n",
       "49                 29                     18        0.933976   \n",
       "50                 18                     38        0.902617   \n",
       "51                 26                     17        0.936242   \n",
       "52                 24                     42        0.905904   \n",
       "53                 28                      8        0.938497   \n",
       "54                 43                     44        0.914389   \n",
       "55                 38                     26        0.939566   \n",
       "56                 19                     36        0.914588   \n",
       "57                 33                     12        0.946896   \n",
       "58                  9                     29        0.918090   \n",
       "59                 32                      1        0.951540   \n",
       "60                 43                     44        0.914379   \n",
       "61                 37                     22        0.939396   \n",
       "62                 20                     36        0.914562   \n",
       "63                 36                     14        0.946488   \n",
       "64                 11                     30        0.918047   \n",
       "65                 34                      2        0.951005   \n",
       "66                 41                     43        0.914281   \n",
       "67                 27                      7        0.936629   \n",
       "68                 21                     39        0.913827   \n",
       "69                 35                     24        0.941911   \n",
       "70                 12                     33        0.917312   \n",
       "71                 22                      5        0.944009   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.499611                0.376660  \n",
       "1             0.505436                0.383124  \n",
       "2             0.498355                0.375087  \n",
       "3             0.499800                0.377184  \n",
       "4             0.499144                0.376136  \n",
       "5             0.499063                0.375611  \n",
       "6             0.499445                0.376485  \n",
       "7             0.505436                0.383124  \n",
       "8             0.498416                0.375087  \n",
       "9             0.499856                0.377184  \n",
       "10            0.498979                0.375961  \n",
       "11            0.499063                0.375611  \n",
       "12            0.498286                0.375087  \n",
       "13            0.499085                0.375786  \n",
       "14            0.497853                0.374389  \n",
       "15            0.503448                0.380328  \n",
       "16            0.498454                0.375262  \n",
       "17            0.501004                0.378756  \n",
       "18            0.530166                0.413173  \n",
       "19            0.644879                0.579839  \n",
       "20            0.521912                0.404437  \n",
       "21            0.654749                0.588924  \n",
       "22            0.510192                0.389762  \n",
       "23            0.657432                0.593117  \n",
       "24            0.528639                0.411426  \n",
       "25            0.641730                0.573375  \n",
       "26            0.521466                0.403913  \n",
       "27            0.647431                0.575996  \n",
       "28            0.509267                0.388714  \n",
       "29            0.651365                0.583508  \n",
       "..                 ...                     ...  \n",
       "42            0.604135                0.519567  \n",
       "43            0.684445                0.614430  \n",
       "44            0.588428                0.493012  \n",
       "45            0.706825                0.633997  \n",
       "46            0.584665                0.487247  \n",
       "47            0.718143                0.651468  \n",
       "48            0.602569                0.516771  \n",
       "49            0.666176                0.592942  \n",
       "50            0.584122                0.487247  \n",
       "51            0.677237                0.599755  \n",
       "52            0.581875                0.482704  \n",
       "53            0.686262                0.614605  \n",
       "54            0.593204                0.494584  \n",
       "55            0.684732                0.616702  \n",
       "56            0.609394                0.520615  \n",
       "57            0.698741                0.628232  \n",
       "58            0.619567                0.529525  \n",
       "59            0.719065                0.660028  \n",
       "60            0.593170                0.494584  \n",
       "61            0.685319                0.617925  \n",
       "62            0.609597                0.520790  \n",
       "63            0.696007                0.624214  \n",
       "64            0.619287                0.529175  \n",
       "65            0.713958                0.653040  \n",
       "66            0.592316                0.492837  \n",
       "67            0.679239                0.620196  \n",
       "68            0.604496                0.513103  \n",
       "69            0.679762                0.602725  \n",
       "70            0.614747                0.523760  \n",
       "71            0.694192                0.625961  \n",
       "\n",
       "[72 rows x 11 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_mlp = make_table(data)\n",
    "table_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlpclassifier__activation': 'logistic',\n",
       " 'mlpclassifier__alpha': 0.01,\n",
       " 'mlpclassifier__hidden_layer_sizes': 20,\n",
       " 'mlpclassifier__learning_rate': 'adaptive',\n",
       " 'mlpclassifier__max_iter': 250,\n",
       " 'mlpclassifier__random_state': 0,\n",
       " 'mlpclassifier__solver': 'adam'}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_index_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'complementnb__alpha': [0, 0.4, 0.5, 0.6, 1],\n",
    "    'complementnb__fit_prior': (True, False),\n",
    "    'complementnb__norm': (True, False)\n",
    "}\n",
    "\n",
    "pp = make_pipeline(MinMaxScaler(), ComplementNB())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_complementnb__alpha</th>\n",
       "      <th>param_complementnb__fit_prior</th>\n",
       "      <th>param_complementnb__norm</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034258</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.813963</td>\n",
       "      <td>0.803381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>0.179328</td>\n",
       "      <td>11</td>\n",
       "      <td>0.836478</td>\n",
       "      <td>0.450734</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.669811</td>\n",
       "      <td>0.139773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036255</td>\n",
       "      <td>0.011507</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.816455</td>\n",
       "      <td>0.819132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839273</td>\n",
       "      <td>0.877009</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862858</td>\n",
       "      <td>0.014325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.039065</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.813963</td>\n",
       "      <td>0.803381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>0.179328</td>\n",
       "      <td>11</td>\n",
       "      <td>0.836478</td>\n",
       "      <td>0.450734</td>\n",
       "      <td>0.673655</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.669811</td>\n",
       "      <td>0.139773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054212</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.026475</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.816455</td>\n",
       "      <td>0.819132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839273</td>\n",
       "      <td>0.877009</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862858</td>\n",
       "      <td>0.014325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.039064</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.813820</td>\n",
       "      <td>0.802612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.447240</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.715584</td>\n",
       "      <td>0.665618</td>\n",
       "      <td>0.138401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.046873</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.816521</td>\n",
       "      <td>0.818484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.060763</td>\n",
       "      <td>0.018033</td>\n",
       "      <td>0.045240</td>\n",
       "      <td>0.007587</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.813820</td>\n",
       "      <td>0.802612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.828092</td>\n",
       "      <td>0.447240</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.715584</td>\n",
       "      <td>0.665618</td>\n",
       "      <td>0.138401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.056997</td>\n",
       "      <td>0.017936</td>\n",
       "      <td>0.061253</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.816521</td>\n",
       "      <td>0.818484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.054504</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.813757</td>\n",
       "      <td>0.802388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.714885</td>\n",
       "      <td>0.664396</td>\n",
       "      <td>0.139032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.044748</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.816530</td>\n",
       "      <td>0.818332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.874913</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862509</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.044995</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.813757</td>\n",
       "      <td>0.802388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>0.180396</td>\n",
       "      <td>13</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.671558</td>\n",
       "      <td>0.714885</td>\n",
       "      <td>0.664396</td>\n",
       "      <td>0.139032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055503</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.047493</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.816530</td>\n",
       "      <td>0.818332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.874913</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862509</td>\n",
       "      <td>0.013536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.813695</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>0.180195</td>\n",
       "      <td>17</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>0.713487</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.138898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055927</td>\n",
       "      <td>0.010170</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.6</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.816552</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.050787</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.813695</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851153</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>0.180195</td>\n",
       "      <td>17</td>\n",
       "      <td>0.826695</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.670860</td>\n",
       "      <td>0.713487</td>\n",
       "      <td>0.663871</td>\n",
       "      <td>0.138898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059201</td>\n",
       "      <td>0.012571</td>\n",
       "      <td>0.042966</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.816552</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>0.107610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.870720</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.864430</td>\n",
       "      <td>0.862683</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050785</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>0.017027</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.813452</td>\n",
       "      <td>0.801399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.181475</td>\n",
       "      <td>19</td>\n",
       "      <td>0.820405</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.667365</td>\n",
       "      <td>0.712089</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>0.140217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.064924</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>0.051369</td>\n",
       "      <td>0.019297</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.816570</td>\n",
       "      <td>0.817513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>0.107884</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870021</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.863732</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.013577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.813452</td>\n",
       "      <td>0.801399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846960</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.181475</td>\n",
       "      <td>19</td>\n",
       "      <td>0.820405</td>\n",
       "      <td>0.436059</td>\n",
       "      <td>0.667365</td>\n",
       "      <td>0.712089</td>\n",
       "      <td>0.658980</td>\n",
       "      <td>0.140217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.046872</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.039067</td>\n",
       "      <td>0.007805</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.816570</td>\n",
       "      <td>0.817513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>0.107884</td>\n",
       "      <td>9</td>\n",
       "      <td>0.870021</td>\n",
       "      <td>0.839972</td>\n",
       "      <td>0.875611</td>\n",
       "      <td>0.863732</td>\n",
       "      <td>0.862334</td>\n",
       "      <td>0.013577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.034258      0.001295         0.047244        0.006984   \n",
       "1        0.036255      0.011507         0.034548        0.003359   \n",
       "2        0.035156      0.006772         0.039065        0.013554   \n",
       "3        0.054212      0.013631         0.026475        0.005136   \n",
       "4        0.035156      0.006764         0.039064        0.007809   \n",
       "5        0.046873      0.011056         0.039053        0.007804   \n",
       "6        0.060763      0.018033         0.045240        0.007587   \n",
       "7        0.056997      0.017936         0.061253        0.008869   \n",
       "8        0.054504      0.012627         0.048248        0.004258   \n",
       "9        0.063504      0.017629         0.044748        0.002487   \n",
       "10       0.055000      0.010635         0.044995        0.005047   \n",
       "11       0.055503      0.007700         0.047493        0.009066   \n",
       "12       0.069002      0.022504         0.047500        0.006580   \n",
       "13       0.055927      0.010170         0.035155        0.006768   \n",
       "14       0.050787      0.006776         0.046875        0.000017   \n",
       "15       0.059201      0.012571         0.042966        0.006758   \n",
       "16       0.050785      0.006763         0.050776        0.017027   \n",
       "17       0.064924      0.013970         0.051369        0.019297   \n",
       "18       0.042969      0.006731         0.042959        0.012933   \n",
       "19       0.046872      0.000015         0.039067        0.007805   \n",
       "\n",
       "   param_complementnb__alpha param_complementnb__fit_prior  \\\n",
       "0                          0                          True   \n",
       "1                          0                          True   \n",
       "2                          0                         False   \n",
       "3                          0                         False   \n",
       "4                        0.4                          True   \n",
       "5                        0.4                          True   \n",
       "6                        0.4                         False   \n",
       "7                        0.4                         False   \n",
       "8                        0.5                          True   \n",
       "9                        0.5                          True   \n",
       "10                       0.5                         False   \n",
       "11                       0.5                         False   \n",
       "12                       0.6                          True   \n",
       "13                       0.6                          True   \n",
       "14                       0.6                         False   \n",
       "15                       0.6                         False   \n",
       "16                         1                          True   \n",
       "17                         1                          True   \n",
       "18                         1                         False   \n",
       "19                         1                         False   \n",
       "\n",
       "   param_complementnb__norm  \\\n",
       "0                      True   \n",
       "1                     False   \n",
       "2                      True   \n",
       "3                     False   \n",
       "4                      True   \n",
       "5                     False   \n",
       "6                      True   \n",
       "7                     False   \n",
       "8                      True   \n",
       "9                     False   \n",
       "10                     True   \n",
       "11                    False   \n",
       "12                     True   \n",
       "13                    False   \n",
       "14                     True   \n",
       "15                    False   \n",
       "16                     True   \n",
       "17                    False   \n",
       "18                     True   \n",
       "19                    False   \n",
       "\n",
       "                                               params  split0_test_AUC  \\\n",
       "0   {'complementnb__alpha': 0, 'complementnb__fit_...         0.813963   \n",
       "1   {'complementnb__alpha': 0, 'complementnb__fit_...         0.816455   \n",
       "2   {'complementnb__alpha': 0, 'complementnb__fit_...         0.813963   \n",
       "3   {'complementnb__alpha': 0, 'complementnb__fit_...         0.816455   \n",
       "4   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.813820   \n",
       "5   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.816521   \n",
       "6   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.813820   \n",
       "7   {'complementnb__alpha': 0.4, 'complementnb__fi...         0.816521   \n",
       "8   {'complementnb__alpha': 0.5, 'complementnb__fi...         0.813757   \n",
       "9   {'complementnb__alpha': 0.5, 'complementnb__fi...         0.816530   \n",
       "10  {'complementnb__alpha': 0.5, 'complementnb__fi...         0.813757   \n",
       "11  {'complementnb__alpha': 0.5, 'complementnb__fi...         0.816530   \n",
       "12  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.813695   \n",
       "13  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.816552   \n",
       "14  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.813695   \n",
       "15  {'complementnb__alpha': 0.6, 'complementnb__fi...         0.816552   \n",
       "16  {'complementnb__alpha': 1, 'complementnb__fit_...         0.813452   \n",
       "17  {'complementnb__alpha': 1, 'complementnb__fit_...         0.816570   \n",
       "18  {'complementnb__alpha': 1, 'complementnb__fit_...         0.813452   \n",
       "19  {'complementnb__alpha': 1, 'complementnb__fit_...         0.816570   \n",
       "\n",
       "    split1_test_AUC  ...  split3_test_Sensitivity  mean_test_Sensitivity  \\\n",
       "0          0.803381  ...                 0.853249               0.642534   \n",
       "1          0.819132  ...                 0.955975               0.835413   \n",
       "2          0.803381  ...                 0.853249               0.642534   \n",
       "3          0.819132  ...                 0.955975               0.835413   \n",
       "4          0.802612  ...                 0.851153               0.639914   \n",
       "5          0.818484  ...                 0.955975               0.835413   \n",
       "6          0.802612  ...                 0.851153               0.639914   \n",
       "7          0.818484  ...                 0.955975               0.835413   \n",
       "8          0.802388  ...                 0.851153               0.639914   \n",
       "9          0.818332  ...                 0.955975               0.835413   \n",
       "10         0.802388  ...                 0.851153               0.639914   \n",
       "11         0.818332  ...                 0.955975               0.835413   \n",
       "12         0.802181  ...                 0.851153               0.638865   \n",
       "13         0.818178  ...                 0.955975               0.835413   \n",
       "14         0.802181  ...                 0.851153               0.638865   \n",
       "15         0.818178  ...                 0.955975               0.835413   \n",
       "16         0.801399  ...                 0.846960               0.635196   \n",
       "17         0.817513  ...                 0.955975               0.834889   \n",
       "18         0.801399  ...                 0.846960               0.635196   \n",
       "19         0.817513  ...                 0.955975               0.834889   \n",
       "\n",
       "    std_test_Sensitivity  rank_test_Sensitivity  split0_train_Sensitivity  \\\n",
       "0               0.179328                     11                  0.836478   \n",
       "1               0.107610                      1                  0.870720   \n",
       "2               0.179328                     11                  0.836478   \n",
       "3               0.107610                      1                  0.870720   \n",
       "4               0.180396                     13                  0.828092   \n",
       "5               0.107610                      1                  0.870720   \n",
       "6               0.180396                     13                  0.828092   \n",
       "7               0.107610                      1                  0.870720   \n",
       "8               0.180396                     13                  0.826695   \n",
       "9               0.107610                      1                  0.870720   \n",
       "10              0.180396                     13                  0.826695   \n",
       "11              0.107610                      1                  0.870720   \n",
       "12              0.180195                     17                  0.826695   \n",
       "13              0.107610                      1                  0.870720   \n",
       "14              0.180195                     17                  0.826695   \n",
       "15              0.107610                      1                  0.870720   \n",
       "16              0.181475                     19                  0.820405   \n",
       "17              0.107884                      9                  0.870021   \n",
       "18              0.181475                     19                  0.820405   \n",
       "19              0.107884                      9                  0.870021   \n",
       "\n",
       "    split1_train_Sensitivity  split2_train_Sensitivity  \\\n",
       "0                   0.450734                  0.673655   \n",
       "1                   0.839273                  0.877009   \n",
       "2                   0.450734                  0.673655   \n",
       "3                   0.839273                  0.877009   \n",
       "4                   0.447240                  0.671558   \n",
       "5                   0.839972                  0.875611   \n",
       "6                   0.447240                  0.671558   \n",
       "7                   0.839972                  0.875611   \n",
       "8                   0.444444                  0.671558   \n",
       "9                   0.839972                  0.874913   \n",
       "10                  0.444444                  0.671558   \n",
       "11                  0.839972                  0.874913   \n",
       "12                  0.444444                  0.670860   \n",
       "13                  0.839972                  0.875611   \n",
       "14                  0.444444                  0.670860   \n",
       "15                  0.839972                  0.875611   \n",
       "16                  0.436059                  0.667365   \n",
       "17                  0.839972                  0.875611   \n",
       "18                  0.436059                  0.667365   \n",
       "19                  0.839972                  0.875611   \n",
       "\n",
       "    split3_train_Sensitivity  mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                   0.718379                0.669811               0.139773  \n",
       "1                   0.864430                0.862858               0.014325  \n",
       "2                   0.718379                0.669811               0.139773  \n",
       "3                   0.864430                0.862858               0.014325  \n",
       "4                   0.715584                0.665618               0.138401  \n",
       "5                   0.864430                0.862683               0.013698  \n",
       "6                   0.715584                0.665618               0.138401  \n",
       "7                   0.864430                0.862683               0.013698  \n",
       "8                   0.714885                0.664396               0.139032  \n",
       "9                   0.864430                0.862509               0.013536  \n",
       "10                  0.714885                0.664396               0.139032  \n",
       "11                  0.864430                0.862509               0.013536  \n",
       "12                  0.713487                0.663871               0.138898  \n",
       "13                  0.864430                0.862683               0.013698  \n",
       "14                  0.713487                0.663871               0.138898  \n",
       "15                  0.864430                0.862683               0.013698  \n",
       "16                  0.712089                0.658980               0.140217  \n",
       "17                  0.863732                0.862334               0.013577  \n",
       "18                  0.712089                0.658980               0.140217  \n",
       "19                  0.863732                0.862334               0.013577  \n",
       "\n",
       "[20 rows x 47 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cnb = gs.cv_results_\n",
    "data = pandas.DataFrame(results_cnb)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034258</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.792167</td>\n",
       "      <td>0.441939</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.807121</td>\n",
       "      <td>0.463429</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036255</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.797842</td>\n",
       "      <td>0.420761</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812993</td>\n",
       "      <td>0.418521</td>\n",
       "      <td>0.862858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.792167</td>\n",
       "      <td>0.441939</td>\n",
       "      <td>0.642534</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.807121</td>\n",
       "      <td>0.463429</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054212</td>\n",
       "      <td>{'complementnb__alpha': 0, 'complementnb__fit_...</td>\n",
       "      <td>0.797842</td>\n",
       "      <td>0.420761</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812993</td>\n",
       "      <td>0.418521</td>\n",
       "      <td>0.862858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035156</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.441950</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806888</td>\n",
       "      <td>0.464422</td>\n",
       "      <td>0.665618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.046873</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.420606</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812962</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.060763</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.791874</td>\n",
       "      <td>0.441950</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806888</td>\n",
       "      <td>0.464422</td>\n",
       "      <td>0.665618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.056997</td>\n",
       "      <td>{'complementnb__alpha': 0.4, 'complementnb__fi...</td>\n",
       "      <td>0.797702</td>\n",
       "      <td>0.420606</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812962</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.054504</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.791784</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806834</td>\n",
       "      <td>0.464271</td>\n",
       "      <td>0.664396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.797661</td>\n",
       "      <td>0.420680</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812953</td>\n",
       "      <td>0.418923</td>\n",
       "      <td>0.862509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.055000</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.791784</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806834</td>\n",
       "      <td>0.464271</td>\n",
       "      <td>0.664396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.055503</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.797661</td>\n",
       "      <td>0.420680</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812953</td>\n",
       "      <td>0.418923</td>\n",
       "      <td>0.862509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.069002</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.791695</td>\n",
       "      <td>0.442157</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.806776</td>\n",
       "      <td>0.464853</td>\n",
       "      <td>0.663871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.055927</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.797630</td>\n",
       "      <td>0.420610</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812943</td>\n",
       "      <td>0.418993</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.050787</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.791695</td>\n",
       "      <td>0.442157</td>\n",
       "      <td>0.638865</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.806776</td>\n",
       "      <td>0.464853</td>\n",
       "      <td>0.663871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059201</td>\n",
       "      <td>{'complementnb__alpha': 0.6, 'complementnb__fi...</td>\n",
       "      <td>0.797630</td>\n",
       "      <td>0.420610</td>\n",
       "      <td>0.835413</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812943</td>\n",
       "      <td>0.418993</td>\n",
       "      <td>0.862683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050785</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.791362</td>\n",
       "      <td>0.441815</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.806544</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>0.658980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.064924</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.797465</td>\n",
       "      <td>0.420549</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812911</td>\n",
       "      <td>0.419207</td>\n",
       "      <td>0.862334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.042969</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.791362</td>\n",
       "      <td>0.441815</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.806544</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>0.658980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.046872</td>\n",
       "      <td>{'complementnb__alpha': 1, 'complementnb__fit_...</td>\n",
       "      <td>0.797465</td>\n",
       "      <td>0.420549</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0.812911</td>\n",
       "      <td>0.419207</td>\n",
       "      <td>0.862334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        0.034258  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "1        0.036255  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "2        0.035156  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "3        0.054212  {'complementnb__alpha': 0, 'complementnb__fit_...   \n",
       "4        0.035156  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "5        0.046873  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "6        0.060763  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "7        0.056997  {'complementnb__alpha': 0.4, 'complementnb__fi...   \n",
       "8        0.054504  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "9        0.063504  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "10       0.055000  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "11       0.055503  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "12       0.069002  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "13       0.055927  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "14       0.050787  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "15       0.059201  {'complementnb__alpha': 0.6, 'complementnb__fi...   \n",
       "16       0.050785  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "17       0.064924  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "18       0.042969  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "19       0.046872  {'complementnb__alpha': 1, 'complementnb__fit_...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.792167           0.441939               0.642534             11   \n",
       "1        0.797842           0.420761               0.835413              1   \n",
       "2        0.792167           0.441939               0.642534             11   \n",
       "3        0.797842           0.420761               0.835413              1   \n",
       "4        0.791874           0.441950               0.639914             13   \n",
       "5        0.797702           0.420606               0.835413              3   \n",
       "6        0.791874           0.441950               0.639914             13   \n",
       "7        0.797702           0.420606               0.835413              3   \n",
       "8        0.791784           0.442446               0.639914             15   \n",
       "9        0.797661           0.420680               0.835413              5   \n",
       "10       0.791784           0.442446               0.639914             15   \n",
       "11       0.797661           0.420680               0.835413              5   \n",
       "12       0.791695           0.442157               0.638865             17   \n",
       "13       0.797630           0.420610               0.835413              7   \n",
       "14       0.791695           0.442157               0.638865             17   \n",
       "15       0.797630           0.420610               0.835413              7   \n",
       "16       0.791362           0.441815               0.635196             19   \n",
       "17       0.797465           0.420549               0.834889              9   \n",
       "18       0.791362           0.441815               0.635196             19   \n",
       "19       0.797465           0.420549               0.834889              9   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   7                     11        0.807121   \n",
       "1                  11                      1        0.812993   \n",
       "2                   7                     11        0.807121   \n",
       "3                  11                      1        0.812993   \n",
       "4                   5                     13        0.806888   \n",
       "5                  17                      1        0.812962   \n",
       "6                   5                     13        0.806888   \n",
       "7                  17                      1        0.812962   \n",
       "8                   1                     13        0.806834   \n",
       "9                  13                      1        0.812953   \n",
       "10                  1                     13        0.806834   \n",
       "11                 13                      1        0.812953   \n",
       "12                  3                     17        0.806776   \n",
       "13                 15                      1        0.812943   \n",
       "14                  3                     17        0.806776   \n",
       "15                 15                      1        0.812943   \n",
       "16                  9                     19        0.806544   \n",
       "17                 19                      9        0.812911   \n",
       "18                  9                     19        0.806544   \n",
       "19                 19                      9        0.812911   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.463429                0.669811  \n",
       "1             0.418521                0.862858  \n",
       "2             0.463429                0.669811  \n",
       "3             0.418521                0.862858  \n",
       "4             0.464422                0.665618  \n",
       "5             0.418882                0.862683  \n",
       "6             0.464422                0.665618  \n",
       "7             0.418882                0.862683  \n",
       "8             0.464271                0.664396  \n",
       "9             0.418923                0.862509  \n",
       "10            0.464271                0.664396  \n",
       "11            0.418923                0.862509  \n",
       "12            0.464853                0.663871  \n",
       "13            0.418993                0.862683  \n",
       "14            0.464853                0.663871  \n",
       "15            0.418993                0.862683  \n",
       "16            0.464452                0.658980  \n",
       "17            0.419207                0.862334  \n",
       "18            0.464452                0.658980  \n",
       "19            0.419207                0.862334  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_cnb = make_table(data)\n",
    "table_cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complementnb__alpha': 0.5,\n",
       " 'complementnb__fit_prior': True,\n",
       " 'complementnb__norm': True}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 11 candidates, totalling 44 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  44 | elapsed:    4.7s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'gaussiannb__var_smoothing': [0.0000000001, 0.00000001, 0.000001, 0.0001, 0.1, 10, 100, 1000, 100000, 1000000, 100000000]\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), GaussianNB())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gaussiannb__var_smoothing</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>split1_test_AUC</th>\n",
       "      <th>split2_test_AUC</th>\n",
       "      <th>split3_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079499</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>1e-10</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-10}</td>\n",
       "      <td>0.848756</td>\n",
       "      <td>0.830094</td>\n",
       "      <td>0.800651</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062663</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-08}</td>\n",
       "      <td>0.848756</td>\n",
       "      <td>0.830094</td>\n",
       "      <td>0.800651</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093762</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.076338</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-06}</td>\n",
       "      <td>0.848756</td>\n",
       "      <td>0.830095</td>\n",
       "      <td>0.800651</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121751</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.098747</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.0001}</td>\n",
       "      <td>0.848759</td>\n",
       "      <td>0.830101</td>\n",
       "      <td>0.800648</td>\n",
       "      <td>0.806298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823899</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.132485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709294</td>\n",
       "      <td>0.649196</td>\n",
       "      <td>0.689029</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.026621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124741</td>\n",
       "      <td>0.015825</td>\n",
       "      <td>0.075324</td>\n",
       "      <td>0.024469</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.1}</td>\n",
       "      <td>0.849927</td>\n",
       "      <td>0.834127</td>\n",
       "      <td>0.797387</td>\n",
       "      <td>0.803957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740042</td>\n",
       "      <td>0.574929</td>\n",
       "      <td>0.120297</td>\n",
       "      <td>5</td>\n",
       "      <td>0.614256</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.612509</td>\n",
       "      <td>0.027976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.121096</td>\n",
       "      <td>0.027891</td>\n",
       "      <td>0.066414</td>\n",
       "      <td>0.017030</td>\n",
       "      <td>10</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 10}</td>\n",
       "      <td>0.902410</td>\n",
       "      <td>0.840693</td>\n",
       "      <td>0.807217</td>\n",
       "      <td>0.816855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>6</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.000579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097652</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.074227</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>100</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100}</td>\n",
       "      <td>0.917182</td>\n",
       "      <td>0.844509</td>\n",
       "      <td>0.814459</td>\n",
       "      <td>0.824082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089851</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.074209</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000}</td>\n",
       "      <td>0.918948</td>\n",
       "      <td>0.845084</td>\n",
       "      <td>0.815406</td>\n",
       "      <td>0.825040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129291</td>\n",
       "      <td>0.013739</td>\n",
       "      <td>0.088745</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>100000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000}</td>\n",
       "      <td>0.919126</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>0.825148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.028086</td>\n",
       "      <td>1000000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000000}</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144359</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.102747</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>100000000</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000000}</td>\n",
       "      <td>0.919129</td>\n",
       "      <td>0.845140</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.079499      0.004940         0.060749        0.004810   \n",
       "1        0.062663      0.000249         0.050786        0.006753   \n",
       "2        0.093762      0.011059         0.076338        0.014632   \n",
       "3        0.121751      0.032900         0.098747        0.017100   \n",
       "4        0.124741      0.015825         0.075324        0.024469   \n",
       "5        0.121096      0.027891         0.066414        0.017030   \n",
       "6        0.097652      0.024027         0.074227        0.027898   \n",
       "7        0.089851      0.017025         0.074209        0.012958   \n",
       "8        0.129291      0.013739         0.088745        0.008550   \n",
       "9        0.092042      0.016431         0.080500        0.028086   \n",
       "10       0.144359      0.018933         0.102747        0.012047   \n",
       "\n",
       "   param_gaussiannb__var_smoothing                                    params  \\\n",
       "0                            1e-10      {'gaussiannb__var_smoothing': 1e-10}   \n",
       "1                            1e-08      {'gaussiannb__var_smoothing': 1e-08}   \n",
       "2                            1e-06      {'gaussiannb__var_smoothing': 1e-06}   \n",
       "3                           0.0001     {'gaussiannb__var_smoothing': 0.0001}   \n",
       "4                              0.1        {'gaussiannb__var_smoothing': 0.1}   \n",
       "5                               10         {'gaussiannb__var_smoothing': 10}   \n",
       "6                              100        {'gaussiannb__var_smoothing': 100}   \n",
       "7                             1000       {'gaussiannb__var_smoothing': 1000}   \n",
       "8                           100000     {'gaussiannb__var_smoothing': 100000}   \n",
       "9                          1000000    {'gaussiannb__var_smoothing': 1000000}   \n",
       "10                       100000000  {'gaussiannb__var_smoothing': 100000000}   \n",
       "\n",
       "    split0_test_AUC  split1_test_AUC  split2_test_AUC  split3_test_AUC  ...  \\\n",
       "0          0.848756         0.830094         0.800651         0.806303  ...   \n",
       "1          0.848756         0.830094         0.800651         0.806303  ...   \n",
       "2          0.848756         0.830095         0.800651         0.806302  ...   \n",
       "3          0.848759         0.830101         0.800648         0.806298  ...   \n",
       "4          0.849927         0.834127         0.797387         0.803957  ...   \n",
       "5          0.902410         0.840693         0.807217         0.816855  ...   \n",
       "6          0.917182         0.844509         0.814459         0.824082  ...   \n",
       "7          0.918948         0.845084         0.815406         0.825040  ...   \n",
       "8          0.919126         0.845140         0.815510         0.825148  ...   \n",
       "9          0.919129         0.845140         0.815510         0.825150  ...   \n",
       "10         0.919129         0.845140         0.815510         0.825150  ...   \n",
       "\n",
       "    split3_test_Sensitivity  mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                  0.823899               0.649351              0.132485   \n",
       "1                  0.823899               0.649351              0.132485   \n",
       "2                  0.823899               0.649351              0.132485   \n",
       "3                  0.823899               0.649351              0.132485   \n",
       "4                  0.740042               0.574929              0.120297   \n",
       "5                  0.008386               0.008909              0.003743   \n",
       "6                  0.000000               0.000000              0.000000   \n",
       "7                  0.000000               0.000000              0.000000   \n",
       "8                  0.000000               0.000000              0.000000   \n",
       "9                  0.000000               0.000000              0.000000   \n",
       "10                 0.000000               0.000000              0.000000   \n",
       "\n",
       "    rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                       1                  0.709294                  0.649196   \n",
       "1                       1                  0.709294                  0.649196   \n",
       "2                       1                  0.709294                  0.649196   \n",
       "3                       1                  0.709294                  0.649196   \n",
       "4                       5                  0.614256                  0.568134   \n",
       "5                       6                  0.009783                  0.010482   \n",
       "6                       7                  0.000000                  0.000000   \n",
       "7                       7                  0.000000                  0.000000   \n",
       "8                       7                  0.000000                  0.000000   \n",
       "9                       7                  0.000000                  0.000000   \n",
       "10                      7                  0.000000                  0.000000   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.689029                  0.718379   \n",
       "1                   0.689029                  0.718379   \n",
       "2                   0.689029                  0.718379   \n",
       "3                   0.689029                  0.718379   \n",
       "4                   0.622642                  0.645003   \n",
       "5                   0.009085                  0.010482   \n",
       "6                   0.000000                  0.000000   \n",
       "7                   0.000000                  0.000000   \n",
       "8                   0.000000                  0.000000   \n",
       "9                   0.000000                  0.000000   \n",
       "10                  0.000000                  0.000000   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.691474               0.026621  \n",
       "1                 0.691474               0.026621  \n",
       "2                 0.691474               0.026621  \n",
       "3                 0.691474               0.026621  \n",
       "4                 0.612509               0.027976  \n",
       "5                 0.009958               0.000579  \n",
       "6                 0.000000               0.000000  \n",
       "7                 0.000000               0.000000  \n",
       "8                 0.000000               0.000000  \n",
       "9                 0.000000               0.000000  \n",
       "10                0.000000               0.000000  \n",
       "\n",
       "[11 rows x 45 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gnb = gs.cv_results_\n",
    "data = pandas.DataFrame(results_gnb)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079499</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-10}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062663</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-08}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093762</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1e-06}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495376</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516555</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.121751</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.0001}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495452</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516617</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124741</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.1}</td>\n",
       "      <td>0.821353</td>\n",
       "      <td>0.484189</td>\n",
       "      <td>0.574929</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.834098</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>0.612509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.121096</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 10}</td>\n",
       "      <td>0.841798</td>\n",
       "      <td>0.017491</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.009958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.097652</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100}</td>\n",
       "      <td>0.850063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089851</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000}</td>\n",
       "      <td>0.851124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129291</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000}</td>\n",
       "      <td>0.851236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.092042</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 1000000}</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.144359</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 100000000}</td>\n",
       "      <td>0.851237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                    params  mean_test_AUC  \\\n",
       "0        0.079499      {'gaussiannb__var_smoothing': 1e-10}       0.821454   \n",
       "1        0.062663      {'gaussiannb__var_smoothing': 1e-08}       0.821454   \n",
       "2        0.093762      {'gaussiannb__var_smoothing': 1e-06}       0.821454   \n",
       "3        0.121751     {'gaussiannb__var_smoothing': 0.0001}       0.821454   \n",
       "4        0.124741        {'gaussiannb__var_smoothing': 0.1}       0.821353   \n",
       "5        0.121096         {'gaussiannb__var_smoothing': 10}       0.841798   \n",
       "6        0.097652        {'gaussiannb__var_smoothing': 100}       0.850063   \n",
       "7        0.089851       {'gaussiannb__var_smoothing': 1000}       0.851124   \n",
       "8        0.129291     {'gaussiannb__var_smoothing': 100000}       0.851236   \n",
       "9        0.092042    {'gaussiannb__var_smoothing': 1000000}       0.851237   \n",
       "10       0.144359  {'gaussiannb__var_smoothing': 100000000}       0.851237   \n",
       "\n",
       "    mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0            0.495376               0.649351              9   \n",
       "1            0.495376               0.649351              9   \n",
       "2            0.495376               0.649351              8   \n",
       "3            0.495452               0.649351              7   \n",
       "4            0.484189               0.574929             11   \n",
       "5            0.017491               0.008909              6   \n",
       "6            0.000000               0.000000              5   \n",
       "7            0.000000               0.000000              4   \n",
       "8            0.000000               0.000000              3   \n",
       "9            0.000000               0.000000              1   \n",
       "10           0.000000               0.000000              1   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   2                      1        0.835938   \n",
       "1                   2                      1        0.835938   \n",
       "2                   2                      1        0.835938   \n",
       "3                   1                      1        0.835938   \n",
       "4                   5                      5        0.834098   \n",
       "5                   6                      6        0.846939   \n",
       "6                   7                      7        0.854538   \n",
       "7                   7                      7        0.855518   \n",
       "8                   7                      7        0.855632   \n",
       "9                   7                      7        0.855633   \n",
       "10                  7                      7        0.855633   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.516555                0.691474  \n",
       "1             0.516555                0.691474  \n",
       "2             0.516555                0.691474  \n",
       "3             0.516617                0.691474  \n",
       "4             0.516355                0.612509  \n",
       "5             0.019630                0.009958  \n",
       "6             0.000000                0.000000  \n",
       "7             0.000000                0.000000  \n",
       "8             0.000000                0.000000  \n",
       "9             0.000000                0.000000  \n",
       "10            0.000000                0.000000  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_gnb = make_table(data)\n",
    "table_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gaussiannb__var_smoothing': 0.0001}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'F-score': 'f1', 'Sensitivity': make_scorer(recall_score)} \n",
    "parameters = {\n",
    "    'adaboostclassifier__n_estimators': (3, 10, 15, 20, 50),\n",
    "    'adaboostclassifier__learning_rate': (0.1, 0.15, 0.2, 0.25, 0.5),\n",
    "    'adaboostclassifier__algorithm': ('SAMME', 'SAMME.R'),\n",
    "    'adaboostclassifier__random_state': (0, 1)\n",
    "}\n",
    "\n",
    "pp = make_pipeline(StandardScaler(), AdaBoostClassifier())\n",
    "\n",
    "gs = GridSearchCV(pp, parameters, cv=skf, scoring=scoring, refit='F-score', return_train_score=True, n_jobs=-1, verbose=10) \n",
    "gs.fit(X, Y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_adaboostclassifier__algorithm</th>\n",
       "      <th>param_adaboostclassifier__learning_rate</th>\n",
       "      <th>param_adaboostclassifier__n_estimators</th>\n",
       "      <th>param_adaboostclassifier__random_state</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_AUC</th>\n",
       "      <th>...</th>\n",
       "      <th>split3_test_Sensitivity</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>std_test_Sensitivity</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>split0_train_Sensitivity</th>\n",
       "      <th>split1_train_Sensitivity</th>\n",
       "      <th>split2_train_Sensitivity</th>\n",
       "      <th>split3_train_Sensitivity</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "      <th>std_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156501</td>\n",
       "      <td>0.015208</td>\n",
       "      <td>0.049008</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>0.139308</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.788959</td>\n",
       "      <td>0.037106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196256</td>\n",
       "      <td>0.022158</td>\n",
       "      <td>0.068751</td>\n",
       "      <td>0.017856</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>0.139308</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.788959</td>\n",
       "      <td>0.037106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.368767</td>\n",
       "      <td>0.066866</td>\n",
       "      <td>0.070736</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.354273</td>\n",
       "      <td>0.030717</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.336007</td>\n",
       "      <td>0.020204</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.114104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.362008</td>\n",
       "      <td>0.020871</td>\n",
       "      <td>0.087259</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.695318</td>\n",
       "      <td>0.114104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.446495</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>0.124510</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.771488</td>\n",
       "      <td>0.534067</td>\n",
       "      <td>0.310510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.489993</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>0.128265</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.771488</td>\n",
       "      <td>0.534067</td>\n",
       "      <td>0.310510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.325745</td>\n",
       "      <td>0.089315</td>\n",
       "      <td>0.250248</td>\n",
       "      <td>0.020525</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>0.259764</td>\n",
       "      <td>73</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.569532</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>0.093706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.029241</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.224010</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>0.259764</td>\n",
       "      <td>73</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.632425</td>\n",
       "      <td>0.569532</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>0.093706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.122994</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>0.150175</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.776730</td>\n",
       "      <td>0.026731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150757</td>\n",
       "      <td>0.019215</td>\n",
       "      <td>0.075494</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>0.150175</td>\n",
       "      <td>3</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.776730</td>\n",
       "      <td>0.026731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.248010</td>\n",
       "      <td>0.012231</td>\n",
       "      <td>0.070739</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.247001</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.071256</td>\n",
       "      <td>0.009980</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>0.087993</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>0.286703</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.538609</td>\n",
       "      <td>0.311503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.343245</td>\n",
       "      <td>0.022064</td>\n",
       "      <td>0.087011</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>0.286703</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.538609</td>\n",
       "      <td>0.311503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.457270</td>\n",
       "      <td>0.020698</td>\n",
       "      <td>0.116230</td>\n",
       "      <td>0.010020</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>0.273563</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.305858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.433242</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>0.106762</td>\n",
       "      <td>0.012076</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>0.273563</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.305858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.370501</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.285247</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>0.289439</td>\n",
       "      <td>91</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.567435</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.467680</td>\n",
       "      <td>0.099647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.107773</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.027040</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.15</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.974272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>0.289439</td>\n",
       "      <td>91</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.567435</td>\n",
       "      <td>0.566737</td>\n",
       "      <td>0.467680</td>\n",
       "      <td>0.099647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.135008</td>\n",
       "      <td>0.015243</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>0.188299</td>\n",
       "      <td>15</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.041803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.132990</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>0.050749</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>0.188299</td>\n",
       "      <td>15</td>\n",
       "      <td>0.730957</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.734451</td>\n",
       "      <td>0.041803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.252509</td>\n",
       "      <td>0.010426</td>\n",
       "      <td>0.072990</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>0.268381</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.304013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.282499</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.069998</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>0.268381</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.676450</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.525157</td>\n",
       "      <td>0.304013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.353754</td>\n",
       "      <td>0.015236</td>\n",
       "      <td>0.085510</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>0.281047</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.513976</td>\n",
       "      <td>0.301963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.347507</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.086492</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>0.281047</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.645003</td>\n",
       "      <td>0.513976</td>\n",
       "      <td>0.301963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.437012</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.106247</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>0.278356</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.722572</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.510133</td>\n",
       "      <td>0.296173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.461752</td>\n",
       "      <td>0.049831</td>\n",
       "      <td>0.106005</td>\n",
       "      <td>0.011595</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.973654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>0.278356</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634521</td>\n",
       "      <td>0.722572</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.510133</td>\n",
       "      <td>0.296173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.971259</td>\n",
       "      <td>0.033903</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.975218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>0.277387</td>\n",
       "      <td>53</td>\n",
       "      <td>0.482180</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.594689</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.543501</td>\n",
       "      <td>0.046589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.987249</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>0.215497</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.975218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651992</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>0.277387</td>\n",
       "      <td>53</td>\n",
       "      <td>0.482180</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.594689</td>\n",
       "      <td>0.582110</td>\n",
       "      <td>0.543501</td>\n",
       "      <td>0.046589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.168505</td>\n",
       "      <td>0.010366</td>\n",
       "      <td>0.068998</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.180498</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>0.057015</td>\n",
       "      <td>0.010241</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.969984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.114405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626136</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.750524</td>\n",
       "      <td>0.071928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.344006</td>\n",
       "      <td>0.021806</td>\n",
       "      <td>0.106246</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.973914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>0.289039</td>\n",
       "      <td>89</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.404612</td>\n",
       "      <td>0.387142</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.354752</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.973914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>0.289039</td>\n",
       "      <td>89</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.404612</td>\n",
       "      <td>0.387142</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.378764</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.114990</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>0.287805</td>\n",
       "      <td>93</td>\n",
       "      <td>0.466806</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.398323</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.077371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.387517</td>\n",
       "      <td>0.025833</td>\n",
       "      <td>0.109238</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>0.287805</td>\n",
       "      <td>93</td>\n",
       "      <td>0.466806</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.562544</td>\n",
       "      <td>0.398323</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.077371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.489513</td>\n",
       "      <td>0.021975</td>\n",
       "      <td>0.137992</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>0.260063</td>\n",
       "      <td>79</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.571628</td>\n",
       "      <td>0.552061</td>\n",
       "      <td>0.527254</td>\n",
       "      <td>0.038458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.503252</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618449</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>0.260063</td>\n",
       "      <td>79</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.571628</td>\n",
       "      <td>0.552061</td>\n",
       "      <td>0.527254</td>\n",
       "      <td>0.038458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.389000</td>\n",
       "      <td>0.094072</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.028454</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.977763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>0.273470</td>\n",
       "      <td>51</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.578616</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.590496</td>\n",
       "      <td>0.583159</td>\n",
       "      <td>0.030710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.157256</td>\n",
       "      <td>0.028560</td>\n",
       "      <td>0.432507</td>\n",
       "      <td>0.107543</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.977763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>0.273470</td>\n",
       "      <td>51</td>\n",
       "      <td>0.538784</td>\n",
       "      <td>0.578616</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.590496</td>\n",
       "      <td>0.583159</td>\n",
       "      <td>0.030710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.166496</td>\n",
       "      <td>0.040743</td>\n",
       "      <td>0.059748</td>\n",
       "      <td>0.013162</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.970175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>0.122974</td>\n",
       "      <td>17</td>\n",
       "      <td>0.624039</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.708421</td>\n",
       "      <td>0.059607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.123497</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.970175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>0.122974</td>\n",
       "      <td>17</td>\n",
       "      <td>0.624039</td>\n",
       "      <td>0.688330</td>\n",
       "      <td>0.785465</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.708421</td>\n",
       "      <td>0.059607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.266008</td>\n",
       "      <td>0.010069</td>\n",
       "      <td>0.088001</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.974354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>0.277497</td>\n",
       "      <td>95</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.640811</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.490915</td>\n",
       "      <td>0.124339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.298004</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.102259</td>\n",
       "      <td>0.011716</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.974354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563941</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>0.277497</td>\n",
       "      <td>95</td>\n",
       "      <td>0.378057</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.640811</td>\n",
       "      <td>0.586303</td>\n",
       "      <td>0.490915</td>\n",
       "      <td>0.124339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.420750</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.125497</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>0.266503</td>\n",
       "      <td>67</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.565863</td>\n",
       "      <td>0.051355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.377755</td>\n",
       "      <td>0.018836</td>\n",
       "      <td>0.109502</td>\n",
       "      <td>0.010736</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.975790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>0.266503</td>\n",
       "      <td>67</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.515024</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.603075</td>\n",
       "      <td>0.565863</td>\n",
       "      <td>0.051355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.510499</td>\n",
       "      <td>0.013903</td>\n",
       "      <td>0.150750</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>69</td>\n",
       "      <td>0.505241</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587701</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.549441</td>\n",
       "      <td>0.036343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.618006</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649895</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>0.262703</td>\n",
       "      <td>69</td>\n",
       "      <td>0.505241</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587701</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.549441</td>\n",
       "      <td>0.036343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.620758</td>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.297504</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.270662</td>\n",
       "      <td>61</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.575122</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.598882</td>\n",
       "      <td>0.583857</td>\n",
       "      <td>0.026750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.243498</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.329497</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.25</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.270662</td>\n",
       "      <td>61</td>\n",
       "      <td>0.545073</td>\n",
       "      <td>0.575122</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.598882</td>\n",
       "      <td>0.583857</td>\n",
       "      <td>0.026750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.141492</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.057007</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.972684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>0.240801</td>\n",
       "      <td>99</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.343117</td>\n",
       "      <td>0.349406</td>\n",
       "      <td>0.384696</td>\n",
       "      <td>0.059384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.120506</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.055743</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.972684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536688</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>0.240801</td>\n",
       "      <td>99</td>\n",
       "      <td>0.487072</td>\n",
       "      <td>0.359189</td>\n",
       "      <td>0.343117</td>\n",
       "      <td>0.349406</td>\n",
       "      <td>0.384696</td>\n",
       "      <td>0.059384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.283014</td>\n",
       "      <td>0.014751</td>\n",
       "      <td>0.087988</td>\n",
       "      <td>0.008804</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>0.270415</td>\n",
       "      <td>63</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.539483</td>\n",
       "      <td>0.046320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.350001</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.092751</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.976835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>0.270415</td>\n",
       "      <td>63</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.522013</td>\n",
       "      <td>0.587002</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.539483</td>\n",
       "      <td>0.046320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.401513</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>0.270619</td>\n",
       "      <td>59</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.617750</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.578791</td>\n",
       "      <td>0.040029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.398753</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.117493</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>0.270619</td>\n",
       "      <td>59</td>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.577219</td>\n",
       "      <td>0.617750</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.578791</td>\n",
       "      <td>0.040029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.540764</td>\n",
       "      <td>0.020495</td>\n",
       "      <td>0.143003</td>\n",
       "      <td>0.009674</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>0.271576</td>\n",
       "      <td>49</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.020745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.530995</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.143266</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647799</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>0.271576</td>\n",
       "      <td>49</td>\n",
       "      <td>0.568134</td>\n",
       "      <td>0.583508</td>\n",
       "      <td>0.624738</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.020745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.230754</td>\n",
       "      <td>0.026318</td>\n",
       "      <td>0.328504</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>0.273348</td>\n",
       "      <td>71</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.619846</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.591020</td>\n",
       "      <td>0.023508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.351996</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>0.464001</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.978817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645702</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>0.273348</td>\n",
       "      <td>71</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.579315</td>\n",
       "      <td>0.619846</td>\n",
       "      <td>0.605870</td>\n",
       "      <td>0.591020</td>\n",
       "      <td>0.023508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.156501      0.015208         0.049008        0.004058   \n",
       "1        0.196256      0.022158         0.068751        0.017856   \n",
       "2        0.368767      0.066866         0.070736        0.008678   \n",
       "3        0.354273      0.030717         0.066744        0.001778   \n",
       "4        0.336007      0.020204         0.085752        0.006415   \n",
       "5        0.362008      0.020871         0.087259        0.007717   \n",
       "6        0.446495      0.019841         0.124510        0.017552   \n",
       "7        0.489993      0.024610         0.128265        0.005767   \n",
       "8        1.325745      0.089315         0.250248        0.020525   \n",
       "9        1.029241      0.020517         0.224010        0.022558   \n",
       "10       0.122994      0.011021         0.054249        0.012654   \n",
       "11       0.150757      0.019215         0.075494        0.022909   \n",
       "12       0.248010      0.012231         0.070739        0.006830   \n",
       "13       0.247001      0.014278         0.071256        0.009980   \n",
       "14       0.370006      0.026975         0.087993        0.014729   \n",
       "15       0.343245      0.022064         0.087011        0.008653   \n",
       "16       0.457270      0.020698         0.116230        0.010020   \n",
       "17       0.433242      0.029575         0.106762        0.012076   \n",
       "18       1.370501      0.014257         0.285247        0.009751   \n",
       "19       1.107773      0.042858         0.219000        0.027040   \n",
       "20       0.135008      0.015243         0.050498        0.007152   \n",
       "21       0.132990      0.014701         0.050749        0.005253   \n",
       "22       0.252509      0.010426         0.072990        0.007780   \n",
       "23       0.282499      0.019343         0.069998        0.003242   \n",
       "24       0.353754      0.015236         0.085510        0.008315   \n",
       "25       0.347507      0.014652         0.086492        0.012050   \n",
       "26       0.437012      0.027505         0.106247        0.014572   \n",
       "27       0.461752      0.049831         0.106005        0.011595   \n",
       "28       0.971259      0.033903         0.218750        0.023670   \n",
       "29       0.987249      0.045759         0.215497        0.025590   \n",
       "..            ...           ...              ...             ...   \n",
       "70       0.168505      0.010366         0.068998        0.013514   \n",
       "71       0.180498      0.035166         0.057015        0.010241   \n",
       "72       0.344006      0.021806         0.106246        0.010735   \n",
       "73       0.354752      0.016529         0.091743        0.010641   \n",
       "74       0.378764      0.009237         0.114990        0.012137   \n",
       "75       0.387517      0.025833         0.109238        0.008927   \n",
       "76       0.489513      0.021975         0.137992        0.011679   \n",
       "77       0.503252      0.026729         0.134000        0.009907   \n",
       "78       1.389000      0.094072         0.453000        0.028454   \n",
       "79       1.157256      0.028560         0.432507        0.107543   \n",
       "80       0.166496      0.040743         0.059748        0.013162   \n",
       "81       0.123497      0.006098         0.053498        0.006028   \n",
       "82       0.266008      0.010069         0.088001        0.013468   \n",
       "83       0.298004      0.015407         0.102259        0.011716   \n",
       "84       0.420750      0.004962         0.125497        0.010739   \n",
       "85       0.377755      0.018836         0.109502        0.010736   \n",
       "86       0.510499      0.013903         0.150750        0.011525   \n",
       "87       0.618006      0.013979         0.189500        0.026257   \n",
       "88       1.620758      0.040519         0.297504        0.015976   \n",
       "89       1.243498      0.023100         0.329497        0.008074   \n",
       "90       0.141492      0.013532         0.057007        0.008685   \n",
       "91       0.120506      0.006185         0.055743        0.003696   \n",
       "92       0.283014      0.014751         0.087988        0.008804   \n",
       "93       0.350001      0.024221         0.092751        0.004818   \n",
       "94       0.401513      0.012292         0.112505        0.008952   \n",
       "95       0.398753      0.012428         0.117493        0.009650   \n",
       "96       0.540764      0.020495         0.143003        0.009674   \n",
       "97       0.530995      0.016802         0.143266        0.012136   \n",
       "98       1.230754      0.026318         0.328504        0.041100   \n",
       "99       1.351996      0.044238         0.464001        0.015536   \n",
       "\n",
       "   param_adaboostclassifier__algorithm  \\\n",
       "0                                SAMME   \n",
       "1                                SAMME   \n",
       "2                                SAMME   \n",
       "3                                SAMME   \n",
       "4                                SAMME   \n",
       "5                                SAMME   \n",
       "6                                SAMME   \n",
       "7                                SAMME   \n",
       "8                                SAMME   \n",
       "9                                SAMME   \n",
       "10                               SAMME   \n",
       "11                               SAMME   \n",
       "12                               SAMME   \n",
       "13                               SAMME   \n",
       "14                               SAMME   \n",
       "15                               SAMME   \n",
       "16                               SAMME   \n",
       "17                               SAMME   \n",
       "18                               SAMME   \n",
       "19                               SAMME   \n",
       "20                               SAMME   \n",
       "21                               SAMME   \n",
       "22                               SAMME   \n",
       "23                               SAMME   \n",
       "24                               SAMME   \n",
       "25                               SAMME   \n",
       "26                               SAMME   \n",
       "27                               SAMME   \n",
       "28                               SAMME   \n",
       "29                               SAMME   \n",
       "..                                 ...   \n",
       "70                             SAMME.R   \n",
       "71                             SAMME.R   \n",
       "72                             SAMME.R   \n",
       "73                             SAMME.R   \n",
       "74                             SAMME.R   \n",
       "75                             SAMME.R   \n",
       "76                             SAMME.R   \n",
       "77                             SAMME.R   \n",
       "78                             SAMME.R   \n",
       "79                             SAMME.R   \n",
       "80                             SAMME.R   \n",
       "81                             SAMME.R   \n",
       "82                             SAMME.R   \n",
       "83                             SAMME.R   \n",
       "84                             SAMME.R   \n",
       "85                             SAMME.R   \n",
       "86                             SAMME.R   \n",
       "87                             SAMME.R   \n",
       "88                             SAMME.R   \n",
       "89                             SAMME.R   \n",
       "90                             SAMME.R   \n",
       "91                             SAMME.R   \n",
       "92                             SAMME.R   \n",
       "93                             SAMME.R   \n",
       "94                             SAMME.R   \n",
       "95                             SAMME.R   \n",
       "96                             SAMME.R   \n",
       "97                             SAMME.R   \n",
       "98                             SAMME.R   \n",
       "99                             SAMME.R   \n",
       "\n",
       "   param_adaboostclassifier__learning_rate  \\\n",
       "0                                      0.1   \n",
       "1                                      0.1   \n",
       "2                                      0.1   \n",
       "3                                      0.1   \n",
       "4                                      0.1   \n",
       "5                                      0.1   \n",
       "6                                      0.1   \n",
       "7                                      0.1   \n",
       "8                                      0.1   \n",
       "9                                      0.1   \n",
       "10                                    0.15   \n",
       "11                                    0.15   \n",
       "12                                    0.15   \n",
       "13                                    0.15   \n",
       "14                                    0.15   \n",
       "15                                    0.15   \n",
       "16                                    0.15   \n",
       "17                                    0.15   \n",
       "18                                    0.15   \n",
       "19                                    0.15   \n",
       "20                                     0.2   \n",
       "21                                     0.2   \n",
       "22                                     0.2   \n",
       "23                                     0.2   \n",
       "24                                     0.2   \n",
       "25                                     0.2   \n",
       "26                                     0.2   \n",
       "27                                     0.2   \n",
       "28                                     0.2   \n",
       "29                                     0.2   \n",
       "..                                     ...   \n",
       "70                                     0.2   \n",
       "71                                     0.2   \n",
       "72                                     0.2   \n",
       "73                                     0.2   \n",
       "74                                     0.2   \n",
       "75                                     0.2   \n",
       "76                                     0.2   \n",
       "77                                     0.2   \n",
       "78                                     0.2   \n",
       "79                                     0.2   \n",
       "80                                    0.25   \n",
       "81                                    0.25   \n",
       "82                                    0.25   \n",
       "83                                    0.25   \n",
       "84                                    0.25   \n",
       "85                                    0.25   \n",
       "86                                    0.25   \n",
       "87                                    0.25   \n",
       "88                                    0.25   \n",
       "89                                    0.25   \n",
       "90                                     0.5   \n",
       "91                                     0.5   \n",
       "92                                     0.5   \n",
       "93                                     0.5   \n",
       "94                                     0.5   \n",
       "95                                     0.5   \n",
       "96                                     0.5   \n",
       "97                                     0.5   \n",
       "98                                     0.5   \n",
       "99                                     0.5   \n",
       "\n",
       "   param_adaboostclassifier__n_estimators  \\\n",
       "0                                       3   \n",
       "1                                       3   \n",
       "2                                      10   \n",
       "3                                      10   \n",
       "4                                      15   \n",
       "5                                      15   \n",
       "6                                      20   \n",
       "7                                      20   \n",
       "8                                      50   \n",
       "9                                      50   \n",
       "10                                      3   \n",
       "11                                      3   \n",
       "12                                     10   \n",
       "13                                     10   \n",
       "14                                     15   \n",
       "15                                     15   \n",
       "16                                     20   \n",
       "17                                     20   \n",
       "18                                     50   \n",
       "19                                     50   \n",
       "20                                      3   \n",
       "21                                      3   \n",
       "22                                     10   \n",
       "23                                     10   \n",
       "24                                     15   \n",
       "25                                     15   \n",
       "26                                     20   \n",
       "27                                     20   \n",
       "28                                     50   \n",
       "29                                     50   \n",
       "..                                    ...   \n",
       "70                                      3   \n",
       "71                                      3   \n",
       "72                                     10   \n",
       "73                                     10   \n",
       "74                                     15   \n",
       "75                                     15   \n",
       "76                                     20   \n",
       "77                                     20   \n",
       "78                                     50   \n",
       "79                                     50   \n",
       "80                                      3   \n",
       "81                                      3   \n",
       "82                                     10   \n",
       "83                                     10   \n",
       "84                                     15   \n",
       "85                                     15   \n",
       "86                                     20   \n",
       "87                                     20   \n",
       "88                                     50   \n",
       "89                                     50   \n",
       "90                                      3   \n",
       "91                                      3   \n",
       "92                                     10   \n",
       "93                                     10   \n",
       "94                                     15   \n",
       "95                                     15   \n",
       "96                                     20   \n",
       "97                                     20   \n",
       "98                                     50   \n",
       "99                                     50   \n",
       "\n",
       "   param_adaboostclassifier__random_state  \\\n",
       "0                                       0   \n",
       "1                                       1   \n",
       "2                                       0   \n",
       "3                                       1   \n",
       "4                                       0   \n",
       "5                                       1   \n",
       "6                                       0   \n",
       "7                                       1   \n",
       "8                                       0   \n",
       "9                                       1   \n",
       "10                                      0   \n",
       "11                                      1   \n",
       "12                                      0   \n",
       "13                                      1   \n",
       "14                                      0   \n",
       "15                                      1   \n",
       "16                                      0   \n",
       "17                                      1   \n",
       "18                                      0   \n",
       "19                                      1   \n",
       "20                                      0   \n",
       "21                                      1   \n",
       "22                                      0   \n",
       "23                                      1   \n",
       "24                                      0   \n",
       "25                                      1   \n",
       "26                                      0   \n",
       "27                                      1   \n",
       "28                                      0   \n",
       "29                                      1   \n",
       "..                                    ...   \n",
       "70                                      0   \n",
       "71                                      1   \n",
       "72                                      0   \n",
       "73                                      1   \n",
       "74                                      0   \n",
       "75                                      1   \n",
       "76                                      0   \n",
       "77                                      1   \n",
       "78                                      0   \n",
       "79                                      1   \n",
       "80                                      0   \n",
       "81                                      1   \n",
       "82                                      0   \n",
       "83                                      1   \n",
       "84                                      0   \n",
       "85                                      1   \n",
       "86                                      0   \n",
       "87                                      1   \n",
       "88                                      0   \n",
       "89                                      1   \n",
       "90                                      0   \n",
       "91                                      1   \n",
       "92                                      0   \n",
       "93                                      1   \n",
       "94                                      0   \n",
       "95                                      1   \n",
       "96                                      0   \n",
       "97                                      1   \n",
       "98                                      0   \n",
       "99                                      1   \n",
       "\n",
       "                                               params  split0_test_AUC  ...  \\\n",
       "0   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "1   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "2   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "3   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "4   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "5   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "6   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "7   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973825  ...   \n",
       "8   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974442  ...   \n",
       "9   {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974442  ...   \n",
       "10  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "11  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "12  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "13  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "14  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "15  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "16  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "17  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.969984  ...   \n",
       "18  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974272  ...   \n",
       "19  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.974272  ...   \n",
       "20  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "21  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.963083  ...   \n",
       "22  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "23  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "24  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "25  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "26  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "27  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.973654  ...   \n",
       "28  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.975218  ...   \n",
       "29  {'adaboostclassifier__algorithm': 'SAMME', 'ad...         0.975218  ...   \n",
       "..                                                ...              ...  ...   \n",
       "70  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.969984  ...   \n",
       "71  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.969984  ...   \n",
       "72  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.973914  ...   \n",
       "73  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.973914  ...   \n",
       "74  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975163  ...   \n",
       "75  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975163  ...   \n",
       "76  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976878  ...   \n",
       "77  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976878  ...   \n",
       "78  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.977763  ...   \n",
       "79  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.977763  ...   \n",
       "80  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.970175  ...   \n",
       "81  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.970175  ...   \n",
       "82  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.974354  ...   \n",
       "83  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.974354  ...   \n",
       "84  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975790  ...   \n",
       "85  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.975790  ...   \n",
       "86  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976842  ...   \n",
       "87  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976842  ...   \n",
       "88  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978649  ...   \n",
       "89  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978649  ...   \n",
       "90  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.972684  ...   \n",
       "91  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.972684  ...   \n",
       "92  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976835  ...   \n",
       "93  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.976835  ...   \n",
       "94  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978102  ...   \n",
       "95  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978102  ...   \n",
       "96  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978090  ...   \n",
       "97  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978090  ...   \n",
       "98  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978817  ...   \n",
       "99  {'adaboostclassifier__algorithm': 'SAMME.R', '...         0.978817  ...   \n",
       "\n",
       "    split3_test_Sensitivity  mean_test_Sensitivity  std_test_Sensitivity  \\\n",
       "0                  0.618449               0.786182              0.139308   \n",
       "1                  0.618449               0.786182              0.139308   \n",
       "2                  0.618449               0.744252              0.114405   \n",
       "3                  0.618449               0.744252              0.114405   \n",
       "4                  0.618449               0.671390              0.038123   \n",
       "5                  0.618449               0.671390              0.038123   \n",
       "6                  0.618449               0.465390              0.275031   \n",
       "7                  0.618449               0.465390              0.275031   \n",
       "8                  0.618449               0.418204              0.259764   \n",
       "9                  0.618449               0.418204              0.259764   \n",
       "10                 0.618449               0.768890              0.150175   \n",
       "11                 0.618449               0.768890              0.150175   \n",
       "12                 0.618449               0.744252              0.114405   \n",
       "13                 0.618449               0.744252              0.114405   \n",
       "14                 0.618449               0.493688              0.286703   \n",
       "15                 0.618449               0.493688              0.286703   \n",
       "16                 0.618449               0.473244              0.273563   \n",
       "17                 0.618449               0.473244              0.273563   \n",
       "18                 0.618449               0.349534              0.289439   \n",
       "19                 0.618449               0.349534              0.289439   \n",
       "20                 0.563941               0.726968              0.188299   \n",
       "21                 0.563941               0.726968              0.188299   \n",
       "22                 0.563941               0.451766              0.268381   \n",
       "23                 0.563941               0.451766              0.268381   \n",
       "24                 0.563941               0.476912              0.281047   \n",
       "25                 0.563941               0.476912              0.281047   \n",
       "26                 0.651992               0.481629              0.278356   \n",
       "27                 0.651992               0.481629              0.278356   \n",
       "28                 0.651992               0.434447              0.277387   \n",
       "29                 0.651992               0.434447              0.277387   \n",
       "..                      ...                    ...                   ...   \n",
       "70                 0.618449               0.744252              0.114405   \n",
       "71                 0.618449               0.744252              0.114405   \n",
       "72                 0.618449               0.350059              0.289039   \n",
       "73                 0.618449               0.350059              0.289039   \n",
       "74                 0.618449               0.347962              0.287805   \n",
       "75                 0.618449               0.347962              0.287805   \n",
       "76                 0.618449               0.414534              0.260063   \n",
       "77                 0.618449               0.414534              0.260063   \n",
       "78                 0.647799               0.435497              0.273470   \n",
       "79                 0.647799               0.435497              0.273470   \n",
       "80                 0.563941               0.704942              0.122974   \n",
       "81                 0.563941               0.704942              0.122974   \n",
       "82                 0.563941               0.335910              0.277497   \n",
       "83                 0.563941               0.335910              0.277497   \n",
       "84                 0.649895               0.422395              0.266503   \n",
       "85                 0.649895               0.422395              0.266503   \n",
       "86                 0.649895               0.420824              0.262703   \n",
       "87                 0.649895               0.420824              0.262703   \n",
       "88                 0.647799               0.431829              0.270662   \n",
       "89                 0.647799               0.431829              0.270662   \n",
       "90                 0.536688               0.301324              0.240801   \n",
       "91                 0.536688               0.301324              0.240801   \n",
       "92                 0.643606               0.430256              0.270415   \n",
       "93                 0.643606               0.430256              0.270415   \n",
       "94                 0.645702               0.432877              0.270619   \n",
       "95                 0.645702               0.432877              0.270619   \n",
       "96                 0.647799               0.442836              0.271576   \n",
       "97                 0.647799               0.442836              0.271576   \n",
       "98                 0.645702               0.419772              0.273348   \n",
       "99                 0.645702               0.419772              0.273348   \n",
       "\n",
       "    rank_test_Sensitivity  split0_train_Sensitivity  split1_train_Sensitivity  \\\n",
       "0                       1                  0.730957                  0.794549   \n",
       "1                       1                  0.730957                  0.794549   \n",
       "2                       9                  0.626136                  0.794549   \n",
       "3                       9                  0.626136                  0.794549   \n",
       "4                      21                  0.511530                  0.688330   \n",
       "5                      21                  0.511530                  0.688330   \n",
       "6                      37                  0.000000                  0.688330   \n",
       "7                      37                  0.000000                  0.688330   \n",
       "8                      73                  0.378057                  0.522013   \n",
       "9                      73                  0.378057                  0.522013   \n",
       "10                      3                  0.730957                  0.794549   \n",
       "11                      3                  0.730957                  0.794549   \n",
       "12                      9                  0.626136                  0.794549   \n",
       "13                      9                  0.626136                  0.794549   \n",
       "14                     27                  0.000000                  0.688330   \n",
       "15                     27                  0.000000                  0.688330   \n",
       "16                     35                  0.000000                  0.634521   \n",
       "17                     35                  0.000000                  0.634521   \n",
       "18                     91                  0.378057                  0.358491   \n",
       "19                     91                  0.378057                  0.358491   \n",
       "20                     15                  0.730957                  0.794549   \n",
       "21                     15                  0.730957                  0.794549   \n",
       "22                     39                  0.000000                  0.688330   \n",
       "23                     39                  0.000000                  0.688330   \n",
       "24                     33                  0.000000                  0.634521   \n",
       "25                     33                  0.000000                  0.634521   \n",
       "26                     31                  0.000000                  0.634521   \n",
       "27                     31                  0.000000                  0.634521   \n",
       "28                     53                  0.482180                  0.515024   \n",
       "29                     53                  0.482180                  0.515024   \n",
       "..                    ...                       ...                       ...   \n",
       "70                      9                  0.626136                  0.794549   \n",
       "71                      9                  0.626136                  0.794549   \n",
       "72                     89                  0.378057                  0.358491   \n",
       "73                     89                  0.378057                  0.358491   \n",
       "74                     93                  0.466806                  0.358491   \n",
       "75                     93                  0.466806                  0.358491   \n",
       "76                     79                  0.470999                  0.514326   \n",
       "77                     79                  0.470999                  0.514326   \n",
       "78                     51                  0.538784                  0.578616   \n",
       "79                     51                  0.538784                  0.578616   \n",
       "80                     17                  0.624039                  0.688330   \n",
       "81                     17                  0.624039                  0.688330   \n",
       "82                     95                  0.378057                  0.358491   \n",
       "83                     95                  0.378057                  0.358491   \n",
       "84                     67                  0.515723                  0.515024   \n",
       "85                     67                  0.515723                  0.515024   \n",
       "86                     69                  0.505241                  0.522013   \n",
       "87                     69                  0.505241                  0.522013   \n",
       "88                     61                  0.545073                  0.575122   \n",
       "89                     61                  0.545073                  0.575122   \n",
       "90                     99                  0.487072                  0.359189   \n",
       "91                     99                  0.487072                  0.359189   \n",
       "92                     63                  0.471698                  0.522013   \n",
       "93                     63                  0.471698                  0.522013   \n",
       "94                     59                  0.514326                  0.577219   \n",
       "95                     59                  0.514326                  0.577219   \n",
       "96                     49                  0.568134                  0.583508   \n",
       "97                     49                  0.568134                  0.583508   \n",
       "98                     71                  0.559050                  0.579315   \n",
       "99                     71                  0.559050                  0.579315   \n",
       "\n",
       "    split2_train_Sensitivity  split3_train_Sensitivity  \\\n",
       "0                   0.834382                  0.795947   \n",
       "1                   0.834382                  0.795947   \n",
       "2                   0.785465                  0.795947   \n",
       "3                   0.785465                  0.795947   \n",
       "4                   0.785465                  0.795947   \n",
       "5                   0.785465                  0.795947   \n",
       "6                   0.676450                  0.771488   \n",
       "7                   0.676450                  0.771488   \n",
       "8                   0.632425                  0.569532   \n",
       "9                   0.632425                  0.569532   \n",
       "10                  0.785465                  0.795947   \n",
       "11                  0.785465                  0.795947   \n",
       "12                  0.785465                  0.795947   \n",
       "13                  0.785465                  0.795947   \n",
       "14                  0.734451                  0.731656   \n",
       "15                  0.734451                  0.731656   \n",
       "16                  0.734451                  0.731656   \n",
       "17                  0.734451                  0.731656   \n",
       "18                  0.567435                  0.566737   \n",
       "19                  0.567435                  0.566737   \n",
       "20                  0.676450                  0.735849   \n",
       "21                  0.676450                  0.735849   \n",
       "22                  0.676450                  0.735849   \n",
       "23                  0.676450                  0.735849   \n",
       "24                  0.776380                  0.645003   \n",
       "25                  0.776380                  0.645003   \n",
       "26                  0.722572                  0.683438   \n",
       "27                  0.722572                  0.683438   \n",
       "28                  0.594689                  0.582110   \n",
       "29                  0.594689                  0.582110   \n",
       "..                       ...                       ...   \n",
       "70                  0.785465                  0.795947   \n",
       "71                  0.785465                  0.795947   \n",
       "72                  0.407407                  0.404612   \n",
       "73                  0.407407                  0.404612   \n",
       "74                  0.562544                  0.398323   \n",
       "75                  0.562544                  0.398323   \n",
       "76                  0.571628                  0.552061   \n",
       "77                  0.571628                  0.552061   \n",
       "78                  0.624738                  0.590496   \n",
       "79                  0.624738                  0.590496   \n",
       "80                  0.785465                  0.735849   \n",
       "81                  0.785465                  0.735849   \n",
       "82                  0.640811                  0.586303   \n",
       "83                  0.640811                  0.586303   \n",
       "84                  0.629630                  0.603075   \n",
       "85                  0.629630                  0.603075   \n",
       "86                  0.587701                  0.582809   \n",
       "87                  0.587701                  0.582809   \n",
       "88                  0.616352                  0.598882   \n",
       "89                  0.616352                  0.598882   \n",
       "90                  0.343117                  0.349406   \n",
       "91                  0.343117                  0.349406   \n",
       "92                  0.587002                  0.577219   \n",
       "93                  0.587002                  0.577219   \n",
       "94                  0.617750                  0.605870   \n",
       "95                  0.617750                  0.605870   \n",
       "96                  0.624738                  0.595388   \n",
       "97                  0.624738                  0.595388   \n",
       "98                  0.619846                  0.605870   \n",
       "99                  0.619846                  0.605870   \n",
       "\n",
       "    mean_train_Sensitivity  std_train_Sensitivity  \n",
       "0                 0.788959               0.037106  \n",
       "1                 0.788959               0.037106  \n",
       "2                 0.750524               0.071928  \n",
       "3                 0.750524               0.071928  \n",
       "4                 0.695318               0.114104  \n",
       "5                 0.695318               0.114104  \n",
       "6                 0.534067               0.310510  \n",
       "7                 0.534067               0.310510  \n",
       "8                 0.525507               0.093706  \n",
       "9                 0.525507               0.093706  \n",
       "10                0.776730               0.026731  \n",
       "11                0.776730               0.026731  \n",
       "12                0.750524               0.071928  \n",
       "13                0.750524               0.071928  \n",
       "14                0.538609               0.311503  \n",
       "15                0.538609               0.311503  \n",
       "16                0.525157               0.305858  \n",
       "17                0.525157               0.305858  \n",
       "18                0.467680               0.099647  \n",
       "19                0.467680               0.099647  \n",
       "20                0.734451               0.041803  \n",
       "21                0.734451               0.041803  \n",
       "22                0.525157               0.304013  \n",
       "23                0.525157               0.304013  \n",
       "24                0.513976               0.301963  \n",
       "25                0.513976               0.301963  \n",
       "26                0.510133               0.296173  \n",
       "27                0.510133               0.296173  \n",
       "28                0.543501               0.046589  \n",
       "29                0.543501               0.046589  \n",
       "..                     ...                    ...  \n",
       "70                0.750524               0.071928  \n",
       "71                0.750524               0.071928  \n",
       "72                0.387142               0.020120  \n",
       "73                0.387142               0.020120  \n",
       "74                0.446541               0.077371  \n",
       "75                0.446541               0.077371  \n",
       "76                0.527254               0.038458  \n",
       "77                0.527254               0.038458  \n",
       "78                0.583159               0.030710  \n",
       "79                0.583159               0.030710  \n",
       "80                0.708421               0.059607  \n",
       "81                0.708421               0.059607  \n",
       "82                0.490915               0.124339  \n",
       "83                0.490915               0.124339  \n",
       "84                0.565863               0.051355  \n",
       "85                0.565863               0.051355  \n",
       "86                0.549441               0.036343  \n",
       "87                0.549441               0.036343  \n",
       "88                0.583857               0.026750  \n",
       "89                0.583857               0.026750  \n",
       "90                0.384696               0.059384  \n",
       "91                0.384696               0.059384  \n",
       "92                0.539483               0.046320  \n",
       "93                0.539483               0.046320  \n",
       "94                0.578791               0.040029  \n",
       "95                0.578791               0.040029  \n",
       "96                0.592942               0.020745  \n",
       "97                0.592942               0.020745  \n",
       "98                0.591020               0.023508  \n",
       "99                0.591020               0.023508  \n",
       "\n",
       "[100 rows x 48 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_abc= gs.cv_results_\n",
    "data = pandas.DataFrame(results_abc)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboostclassifier__algorithm': 'SAMME',\n",
       " 'adaboostclassifier__learning_rate': 0.1,\n",
       " 'adaboostclassifier__n_estimators': 3,\n",
       " 'adaboostclassifier__random_state': 0}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.156501</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.658165</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.666451</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.196256</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.658165</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.666451</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.368767</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.853937</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.855969</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.354273</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.853937</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.855969</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.336007</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856433</td>\n",
       "      <td>0.639291</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.858755</td>\n",
       "      <td>0.653211</td>\n",
       "      <td>0.695318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.362008</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856433</td>\n",
       "      <td>0.639291</td>\n",
       "      <td>0.671390</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.858755</td>\n",
       "      <td>0.653211</td>\n",
       "      <td>0.695318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.446495</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856693</td>\n",
       "      <td>0.443687</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.882185</td>\n",
       "      <td>0.508378</td>\n",
       "      <td>0.534067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.489993</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.856693</td>\n",
       "      <td>0.443687</td>\n",
       "      <td>0.465390</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.882185</td>\n",
       "      <td>0.508378</td>\n",
       "      <td>0.534067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.325745</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.875720</td>\n",
       "      <td>0.423286</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>33</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>0.909647</td>\n",
       "      <td>0.594287</td>\n",
       "      <td>0.525507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.029241</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.875720</td>\n",
       "      <td>0.423286</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>33</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>0.909647</td>\n",
       "      <td>0.594287</td>\n",
       "      <td>0.525507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.122994</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843520</td>\n",
       "      <td>0.657874</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.666585</td>\n",
       "      <td>0.776730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150757</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843520</td>\n",
       "      <td>0.657874</td>\n",
       "      <td>0.768890</td>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.847966</td>\n",
       "      <td>0.666585</td>\n",
       "      <td>0.776730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.248010</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854374</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.856960</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.247001</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854374</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.856960</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.858815</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>65</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.891960</td>\n",
       "      <td>0.512983</td>\n",
       "      <td>0.538609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.343245</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.858815</td>\n",
       "      <td>0.451345</td>\n",
       "      <td>0.493688</td>\n",
       "      <td>65</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.891960</td>\n",
       "      <td>0.512983</td>\n",
       "      <td>0.538609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.457270</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860518</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>0.904527</td>\n",
       "      <td>0.509098</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.433242</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860518</td>\n",
       "      <td>0.451267</td>\n",
       "      <td>0.473244</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>0.904527</td>\n",
       "      <td>0.509098</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.370501</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880423</td>\n",
       "      <td>0.345517</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>21</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>0.913291</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.467680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.107773</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880423</td>\n",
       "      <td>0.345517</td>\n",
       "      <td>0.349534</td>\n",
       "      <td>21</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>0.913291</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.467680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.135008</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.846258</td>\n",
       "      <td>0.647820</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>0.851212</td>\n",
       "      <td>0.661795</td>\n",
       "      <td>0.734451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.132990</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.846258</td>\n",
       "      <td>0.647820</td>\n",
       "      <td>0.726968</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>0.851212</td>\n",
       "      <td>0.661795</td>\n",
       "      <td>0.734451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.252509</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854708</td>\n",
       "      <td>0.441291</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>75</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>0.880532</td>\n",
       "      <td>0.506505</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.282499</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.854708</td>\n",
       "      <td>0.441291</td>\n",
       "      <td>0.451766</td>\n",
       "      <td>75</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>0.880532</td>\n",
       "      <td>0.506505</td>\n",
       "      <td>0.525157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.353754</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860456</td>\n",
       "      <td>0.449162</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>0.903854</td>\n",
       "      <td>0.505644</td>\n",
       "      <td>0.513976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.347507</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.860456</td>\n",
       "      <td>0.449162</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>0.903854</td>\n",
       "      <td>0.505644</td>\n",
       "      <td>0.513976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.437012</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.869285</td>\n",
       "      <td>0.453064</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.908527</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.510133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.461752</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.869285</td>\n",
       "      <td>0.453064</td>\n",
       "      <td>0.481629</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.908527</td>\n",
       "      <td>0.506344</td>\n",
       "      <td>0.510133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.971259</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880774</td>\n",
       "      <td>0.424548</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.607536</td>\n",
       "      <td>0.543501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.987249</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.880774</td>\n",
       "      <td>0.424548</td>\n",
       "      <td>0.434447</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.607536</td>\n",
       "      <td>0.543501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.168505</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.849465</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.851323</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.180498</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.849465</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.851323</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.750524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.344006</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.862814</td>\n",
       "      <td>0.344632</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>53</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>0.905575</td>\n",
       "      <td>0.489506</td>\n",
       "      <td>0.387142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.354752</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.862814</td>\n",
       "      <td>0.344632</td>\n",
       "      <td>0.350059</td>\n",
       "      <td>53</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>0.905575</td>\n",
       "      <td>0.489506</td>\n",
       "      <td>0.387142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.378764</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.875492</td>\n",
       "      <td>0.345363</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>35</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.446541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.387517</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.875492</td>\n",
       "      <td>0.345363</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>35</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.446541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.489513</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.419785</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>0.918729</td>\n",
       "      <td>0.598195</td>\n",
       "      <td>0.527254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.503252</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878974</td>\n",
       "      <td>0.419785</td>\n",
       "      <td>0.414534</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>0.918729</td>\n",
       "      <td>0.598195</td>\n",
       "      <td>0.527254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.389000</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.885628</td>\n",
       "      <td>0.422660</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>51</td>\n",
       "      <td>0.925995</td>\n",
       "      <td>0.636283</td>\n",
       "      <td>0.583159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.157256</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.885628</td>\n",
       "      <td>0.422660</td>\n",
       "      <td>0.435497</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>51</td>\n",
       "      <td>0.925995</td>\n",
       "      <td>0.636283</td>\n",
       "      <td>0.583159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.166496</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.852996</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0.854229</td>\n",
       "      <td>0.660239</td>\n",
       "      <td>0.708421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.123497</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.852996</td>\n",
       "      <td>0.654837</td>\n",
       "      <td>0.704942</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0.854229</td>\n",
       "      <td>0.660239</td>\n",
       "      <td>0.708421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.266008</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.865025</td>\n",
       "      <td>0.341528</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>49</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>0.564754</td>\n",
       "      <td>0.490915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.298004</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.865025</td>\n",
       "      <td>0.341528</td>\n",
       "      <td>0.335910</td>\n",
       "      <td>49</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>0.564754</td>\n",
       "      <td>0.490915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.420750</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878162</td>\n",
       "      <td>0.423262</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>0.917192</td>\n",
       "      <td>0.622853</td>\n",
       "      <td>0.565863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.377755</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878162</td>\n",
       "      <td>0.423262</td>\n",
       "      <td>0.422395</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>0.917192</td>\n",
       "      <td>0.622853</td>\n",
       "      <td>0.565863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.510499</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.880590</td>\n",
       "      <td>0.423816</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>19</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.616510</td>\n",
       "      <td>0.549441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.618006</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.880590</td>\n",
       "      <td>0.423816</td>\n",
       "      <td>0.420824</td>\n",
       "      <td>19</td>\n",
       "      <td>57</td>\n",
       "      <td>69</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.616510</td>\n",
       "      <td>0.549441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.620758</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.422198</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>61</td>\n",
       "      <td>0.927662</td>\n",
       "      <td>0.639455</td>\n",
       "      <td>0.583857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.243498</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.884374</td>\n",
       "      <td>0.422198</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>61</td>\n",
       "      <td>0.927662</td>\n",
       "      <td>0.639455</td>\n",
       "      <td>0.583857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.141492</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.853406</td>\n",
       "      <td>0.332148</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>81</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0.899946</td>\n",
       "      <td>0.486668</td>\n",
       "      <td>0.384696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.120506</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.853406</td>\n",
       "      <td>0.332148</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>81</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0.899946</td>\n",
       "      <td>0.486668</td>\n",
       "      <td>0.384696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.283014</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878403</td>\n",
       "      <td>0.424421</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.539483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.350001</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.878403</td>\n",
       "      <td>0.424421</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.539483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.401513</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.886102</td>\n",
       "      <td>0.426331</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>0.924347</td>\n",
       "      <td>0.633775</td>\n",
       "      <td>0.578791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.398753</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.886102</td>\n",
       "      <td>0.426331</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>0.924347</td>\n",
       "      <td>0.633775</td>\n",
       "      <td>0.578791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.540764</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.882417</td>\n",
       "      <td>0.429917</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.926289</td>\n",
       "      <td>0.641037</td>\n",
       "      <td>0.592942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.530995</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.882417</td>\n",
       "      <td>0.429917</td>\n",
       "      <td>0.442836</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.926289</td>\n",
       "      <td>0.641037</td>\n",
       "      <td>0.592942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.230754</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.868296</td>\n",
       "      <td>0.406926</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>45</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>0.930508</td>\n",
       "      <td>0.645410</td>\n",
       "      <td>0.591020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.351996</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME.R', '...</td>\n",
       "      <td>0.868296</td>\n",
       "      <td>0.406926</td>\n",
       "      <td>0.419772</td>\n",
       "      <td>45</td>\n",
       "      <td>83</td>\n",
       "      <td>71</td>\n",
       "      <td>0.930508</td>\n",
       "      <td>0.645410</td>\n",
       "      <td>0.591020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                             params  \\\n",
       "0        0.156501  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "1        0.196256  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "2        0.368767  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "3        0.354273  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "4        0.336007  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "5        0.362008  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "6        0.446495  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "7        0.489993  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "8        1.325745  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "9        1.029241  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "10       0.122994  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "11       0.150757  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "12       0.248010  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "13       0.247001  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "14       0.370006  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "15       0.343245  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "16       0.457270  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "17       0.433242  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "18       1.370501  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "19       1.107773  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "20       0.135008  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "21       0.132990  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "22       0.252509  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "23       0.282499  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "24       0.353754  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "25       0.347507  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "26       0.437012  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "27       0.461752  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "28       0.971259  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "29       0.987249  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "..            ...                                                ...   \n",
       "70       0.168505  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "71       0.180498  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "72       0.344006  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "73       0.354752  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "74       0.378764  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "75       0.387517  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "76       0.489513  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "77       0.503252  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "78       1.389000  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "79       1.157256  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "80       0.166496  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "81       0.123497  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "82       0.266008  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "83       0.298004  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "84       0.420750  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "85       0.377755  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "86       0.510499  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "87       0.618006  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "88       1.620758  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "89       1.243498  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "90       0.141492  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "91       0.120506  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "92       0.283014  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "93       0.350001  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "94       0.401513  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "95       0.398753  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "96       0.540764  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "97       0.530995  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "98       1.230754  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "99       1.351996  {'adaboostclassifier__algorithm': 'SAMME.R', '...   \n",
       "\n",
       "    mean_test_AUC  mean_test_F_score  mean_test_Sensitivity  rank_test_AUC  \\\n",
       "0        0.843328           0.658165               0.786182             99   \n",
       "1        0.843328           0.658165               0.786182             99   \n",
       "2        0.853937           0.655670               0.744252             79   \n",
       "3        0.853937           0.655670               0.744252             79   \n",
       "4        0.856433           0.639291               0.671390             71   \n",
       "5        0.856433           0.639291               0.671390             71   \n",
       "6        0.856693           0.443687               0.465390             69   \n",
       "7        0.856693           0.443687               0.465390             69   \n",
       "8        0.875720           0.423286               0.418204             33   \n",
       "9        0.875720           0.423286               0.418204             33   \n",
       "10       0.843520           0.657874               0.768890             97   \n",
       "11       0.843520           0.657874               0.768890             97   \n",
       "12       0.854374           0.655670               0.744252             77   \n",
       "13       0.854374           0.655670               0.744252             77   \n",
       "14       0.858815           0.451345               0.493688             65   \n",
       "15       0.858815           0.451345               0.493688             65   \n",
       "16       0.860518           0.451267               0.473244             57   \n",
       "17       0.860518           0.451267               0.473244             57   \n",
       "18       0.880423           0.345517               0.349534             21   \n",
       "19       0.880423           0.345517               0.349534             21   \n",
       "20       0.846258           0.647820               0.726968             93   \n",
       "21       0.846258           0.647820               0.726968             93   \n",
       "22       0.854708           0.441291               0.451766             75   \n",
       "23       0.854708           0.441291               0.451766             75   \n",
       "24       0.860456           0.449162               0.476912             59   \n",
       "25       0.860456           0.449162               0.476912             59   \n",
       "26       0.869285           0.453064               0.481629             43   \n",
       "27       0.869285           0.453064               0.481629             43   \n",
       "28       0.880774           0.424548               0.434447             17   \n",
       "29       0.880774           0.424548               0.434447             17   \n",
       "..            ...                ...                    ...            ...   \n",
       "70       0.849465           0.655670               0.744252             87   \n",
       "71       0.849465           0.655670               0.744252             87   \n",
       "72       0.862814           0.344632               0.350059             53   \n",
       "73       0.862814           0.344632               0.350059             53   \n",
       "74       0.875492           0.345363               0.347962             35   \n",
       "75       0.875492           0.345363               0.347962             35   \n",
       "76       0.878974           0.419785               0.414534             23   \n",
       "77       0.878974           0.419785               0.414534             23   \n",
       "78       0.885628           0.422660               0.435497              5   \n",
       "79       0.885628           0.422660               0.435497              5   \n",
       "80       0.852996           0.654837               0.704942             83   \n",
       "81       0.852996           0.654837               0.704942             83   \n",
       "82       0.865025           0.341528               0.335910             49   \n",
       "83       0.865025           0.341528               0.335910             49   \n",
       "84       0.878162           0.423262               0.422395             27   \n",
       "85       0.878162           0.423262               0.422395             27   \n",
       "86       0.880590           0.423816               0.420824             19   \n",
       "87       0.880590           0.423816               0.420824             19   \n",
       "88       0.884374           0.422198               0.431829              7   \n",
       "89       0.884374           0.422198               0.431829              7   \n",
       "90       0.853406           0.332148               0.301324             81   \n",
       "91       0.853406           0.332148               0.301324             81   \n",
       "92       0.878403           0.424421               0.430256             25   \n",
       "93       0.878403           0.424421               0.430256             25   \n",
       "94       0.886102           0.426331               0.432877              1   \n",
       "95       0.886102           0.426331               0.432877              1   \n",
       "96       0.882417           0.429917               0.442836             11   \n",
       "97       0.882417           0.429917               0.442836             11   \n",
       "98       0.868296           0.406926               0.419772             45   \n",
       "99       0.868296           0.406926               0.419772             45   \n",
       "\n",
       "    rank_test_F_score  rank_test_Sensitivity  mean_train_AUC  \\\n",
       "0                   1                      1        0.847761   \n",
       "1                   1                      1        0.847761   \n",
       "2                   9                      9        0.855969   \n",
       "3                   9                      9        0.855969   \n",
       "4                  21                     21        0.858755   \n",
       "5                  21                     21        0.858755   \n",
       "6                  37                     37        0.882185   \n",
       "7                  37                     37        0.882185   \n",
       "8                  61                     73        0.909647   \n",
       "9                  61                     73        0.909647   \n",
       "10                  3                      3        0.847966   \n",
       "11                  3                      3        0.847966   \n",
       "12                  9                      9        0.856960   \n",
       "13                  9                      9        0.856960   \n",
       "14                 29                     27        0.891960   \n",
       "15                 29                     27        0.891960   \n",
       "16                 31                     35        0.904527   \n",
       "17                 31                     35        0.904527   \n",
       "18                 87                     91        0.913291   \n",
       "19                 87                     91        0.913291   \n",
       "20                 19                     15        0.851212   \n",
       "21                 19                     15        0.851212   \n",
       "22                 43                     39        0.880532   \n",
       "23                 43                     39        0.880532   \n",
       "24                 35                     33        0.903854   \n",
       "25                 35                     33        0.903854   \n",
       "26                 25                     31        0.908527   \n",
       "27                 25                     31        0.908527   \n",
       "28                 53                     53        0.915354   \n",
       "29                 53                     53        0.915354   \n",
       "..                ...                    ...             ...   \n",
       "70                  9                      9        0.851323   \n",
       "71                  9                      9        0.851323   \n",
       "72                 93                     89        0.905575   \n",
       "73                 93                     89        0.905575   \n",
       "74                 89                     93        0.912598   \n",
       "75                 89                     93        0.912598   \n",
       "76                 79                     79        0.918729   \n",
       "77                 79                     79        0.918729   \n",
       "78                 69                     51        0.925995   \n",
       "79                 69                     51        0.925995   \n",
       "80                 15                     17        0.854229   \n",
       "81                 15                     17        0.854229   \n",
       "82                 95                     95        0.909961   \n",
       "83                 95                     95        0.909961   \n",
       "84                 63                     67        0.917192   \n",
       "85                 63                     67        0.917192   \n",
       "86                 57                     69        0.922842   \n",
       "87                 57                     69        0.922842   \n",
       "88                 75                     61        0.927662   \n",
       "89                 75                     61        0.927662   \n",
       "90                 99                     99        0.899946   \n",
       "91                 99                     99        0.899946   \n",
       "92                 55                     63        0.920777   \n",
       "93                 55                     63        0.920777   \n",
       "94                 51                     59        0.924347   \n",
       "95                 51                     59        0.924347   \n",
       "96                 49                     49        0.926289   \n",
       "97                 49                     49        0.926289   \n",
       "98                 83                     71        0.930508   \n",
       "99                 83                     71        0.930508   \n",
       "\n",
       "    mean_train_F_score  mean_train_Sensitivity  \n",
       "0             0.666451                0.788959  \n",
       "1             0.666451                0.788959  \n",
       "2             0.664336                0.750524  \n",
       "3             0.664336                0.750524  \n",
       "4             0.653211                0.695318  \n",
       "5             0.653211                0.695318  \n",
       "6             0.508378                0.534067  \n",
       "7             0.508378                0.534067  \n",
       "8             0.594287                0.525507  \n",
       "9             0.594287                0.525507  \n",
       "10            0.666585                0.776730  \n",
       "11            0.666585                0.776730  \n",
       "12            0.664336                0.750524  \n",
       "13            0.664336                0.750524  \n",
       "14            0.512983                0.538609  \n",
       "15            0.512983                0.538609  \n",
       "16            0.509098                0.525157  \n",
       "17            0.509098                0.525157  \n",
       "18            0.553363                0.467680  \n",
       "19            0.553363                0.467680  \n",
       "20            0.661795                0.734451  \n",
       "21            0.661795                0.734451  \n",
       "22            0.506505                0.525157  \n",
       "23            0.506505                0.525157  \n",
       "24            0.505644                0.513976  \n",
       "25            0.505644                0.513976  \n",
       "26            0.506344                0.510133  \n",
       "27            0.506344                0.510133  \n",
       "28            0.607536                0.543501  \n",
       "29            0.607536                0.543501  \n",
       "..                 ...                     ...  \n",
       "70            0.664336                0.750524  \n",
       "71            0.664336                0.750524  \n",
       "72            0.489506                0.387142  \n",
       "73            0.489506                0.387142  \n",
       "74            0.534591                0.446541  \n",
       "75            0.534591                0.446541  \n",
       "76            0.598195                0.527254  \n",
       "77            0.598195                0.527254  \n",
       "78            0.636283                0.583159  \n",
       "79            0.636283                0.583159  \n",
       "80            0.660239                0.708421  \n",
       "81            0.660239                0.708421  \n",
       "82            0.564754                0.490915  \n",
       "83            0.564754                0.490915  \n",
       "84            0.622853                0.565863  \n",
       "85            0.622853                0.565863  \n",
       "86            0.616510                0.549441  \n",
       "87            0.616510                0.549441  \n",
       "88            0.639455                0.583857  \n",
       "89            0.639455                0.583857  \n",
       "90            0.486668                0.384696  \n",
       "91            0.486668                0.384696  \n",
       "92            0.606900                0.539483  \n",
       "93            0.606900                0.539483  \n",
       "94            0.633775                0.578791  \n",
       "95            0.633775                0.578791  \n",
       "96            0.641037                0.592942  \n",
       "97            0.641037                0.592942  \n",
       "98            0.645410                0.591020  \n",
       "99            0.645410                0.591020  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_abc = make_table(data)\n",
    "table_abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_AUC</th>\n",
       "      <th>mean_test_F_score</th>\n",
       "      <th>mean_test_Sensitivity</th>\n",
       "      <th>rank_test_AUC</th>\n",
       "      <th>rank_test_F_score</th>\n",
       "      <th>rank_test_Sensitivity</th>\n",
       "      <th>mean_train_AUC</th>\n",
       "      <th>mean_train_F_score</th>\n",
       "      <th>mean_train_Sensitivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.156501</td>\n",
       "      <td>{'adaboostclassifier__algorithm': 'SAMME', 'ad...</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.658165</td>\n",
       "      <td>0.786182</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.666451</td>\n",
       "      <td>0.788959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.836223</td>\n",
       "      <td>{'gradientboostingclassifier__learning_rate': ...</td>\n",
       "      <td>0.888507</td>\n",
       "      <td>0.564609</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.952666</td>\n",
       "      <td>0.715934</td>\n",
       "      <td>0.662648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.143254</td>\n",
       "      <td>{'quadraticdiscriminantanalysis__reg_param': 0...</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>0.505637</td>\n",
       "      <td>0.581745</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.542564</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.121751</td>\n",
       "      <td>{'gaussiannb__var_smoothing': 0.0001}</td>\n",
       "      <td>0.821454</td>\n",
       "      <td>0.495452</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.516617</td>\n",
       "      <td>0.691474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.054504</td>\n",
       "      <td>{'complementnb__alpha': 0.5, 'complementnb__fi...</td>\n",
       "      <td>0.791784</td>\n",
       "      <td>0.442446</td>\n",
       "      <td>0.639914</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.806834</td>\n",
       "      <td>0.464271</td>\n",
       "      <td>0.664396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.129594</td>\n",
       "      <td>{'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...</td>\n",
       "      <td>0.688934</td>\n",
       "      <td>0.426242</td>\n",
       "      <td>0.431863</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653024</td>\n",
       "      <td>0.430286</td>\n",
       "      <td>0.428022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123743</td>\n",
       "      <td>{'kdeclassifier__bandwidth': 1.0}</td>\n",
       "      <td>0.822635</td>\n",
       "      <td>0.333549</td>\n",
       "      <td>0.223790</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943239</td>\n",
       "      <td>0.578753</td>\n",
       "      <td>0.414046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time                                             params  \\\n",
       "7       0.156501  {'adaboostclassifier__algorithm': 'SAMME', 'ad...   \n",
       "2       1.836223  {'gradientboostingclassifier__learning_rate': ...   \n",
       "3       0.143254  {'quadraticdiscriminantanalysis__reg_param': 0...   \n",
       "6       0.121751              {'gaussiannb__var_smoothing': 0.0001}   \n",
       "5       0.054504  {'complementnb__alpha': 0.5, 'complementnb__fi...   \n",
       "1      16.129594  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...   \n",
       "4       0.123743                  {'kdeclassifier__bandwidth': 1.0}   \n",
       "\n",
       "   mean_test_AUC  mean_test_F_score  mean_test_Sensitivity rank_test_AUC  \\\n",
       "7       0.843328           0.658165               0.786182            99   \n",
       "2       0.888507           0.564609               0.538248             2   \n",
       "3       0.822152           0.505637               0.581745             7   \n",
       "6       0.821454           0.495452               0.649351             7   \n",
       "5       0.791784           0.442446               0.639914            15   \n",
       "1       0.688934           0.426242               0.431863            11   \n",
       "4       0.822635           0.333549               0.223790            10   \n",
       "\n",
       "  rank_test_F_score rank_test_Sensitivity  mean_train_AUC  mean_train_F_score  \\\n",
       "7                 1                     1        0.847761            0.666451   \n",
       "2                 1                     2        0.952666            0.715934   \n",
       "3                 1                     5        0.846864            0.542564   \n",
       "6                 1                     1        0.835938            0.516617   \n",
       "5                 1                    13        0.806834            0.464271   \n",
       "1                 7                     2        0.653024            0.430286   \n",
       "4                 1                     1        0.943239            0.578753   \n",
       "\n",
       "   mean_train_Sensitivity  \n",
       "7                0.788959  \n",
       "2                0.662648  \n",
       "3                0.620720  \n",
       "6                0.691474  \n",
       "5                0.664396  \n",
       "1                0.428022  \n",
       "4                0.414046  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.DataFrame(columns = [\"mean_fit_time\", \"params\", \"mean_test_AUC\", \"mean_test_F_score\", \"mean_test_Sensitivity\",\n",
    "                                 \"rank_test_AUC\", \"rank_test_F_score\", \"rank_test_Sensitivity\", \"mean_train_AUC\", \n",
    "                                 \"mean_train_F_score\", \"mean_train_Sensitivity\"])\n",
    "\n",
    "df.loc[1] = table_svc.iloc[6]\n",
    "df.loc[2] = table_gbt.iloc[21]\n",
    "df.loc[3] = table_qda.iloc[5]\n",
    "df.loc[4] = table_kd.iloc[0]\n",
    "df.loc[5] = table_cnb.iloc[8]\n",
    "df.loc[6] = table_gnb.iloc[3]\n",
    "df.loc[7] = table_abc.iloc[0]\n",
    "\n",
    "df = df.sort_values(by=['mean_test_F_score'], ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
